{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepMIL_Attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "yZDPUF3U7iWl",
        "DFh_3BzFT3ul",
        "UnFmxjFHShCK",
        "2JV3Bm_noDS_",
        "BQ9dWTyq6WaE",
        "yKFO898IMM_U",
        "7IgrCaAXyEEy",
        "-dkGxYypyQlz",
        "Wamm2SsM19Gm",
        "9Dgedv245upx",
        "MhJcVKLr5-7T",
        "b-GTOw7_6GYv",
        "sW_SynJZ6L3v",
        "H9RvnFdV6RBT",
        "w5J-Bv9F6cB7",
        "l-aXwq9u-6Nw",
        "yJ_-ZR_mJhAX",
        "l_KQOZFRJYwq",
        "T3rKMC_DJYwx",
        "SeI6Lc1KJYw8",
        "ZmYrob49JYxH",
        "pCSunYhOJYxO",
        "cQOSapkPRDY0",
        "dlix_5_mRI6H",
        "l0kE0WVvRI6I",
        "JMIPvcfLRI6s",
        "sgDE-s9ARI67",
        "og2etYrGRI7A",
        "_J0b12RoRI7F",
        "t2AG35HARI7N",
        "_ezElQsxRI7U",
        "7oGWuPTZRN9k"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhraaj17/DeepLearningHW/blob/master/DeepMIL_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZDPUF3U7iWl",
        "colab_type": "text"
      },
      "source": [
        "##Import Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kzkNH-d6dK5",
        "colab_type": "text"
      },
      "source": [
        "Chnage numpy version to be able to load dataset from .npy files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq91etNdKeIq",
        "colab_type": "code",
        "outputId": "17597ca8-4ac6-4ee3-d364-34213b41de24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install numpy==1.16.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiMZ9Nkt_Je",
        "colab_type": "code",
        "outputId": "d43fde6e-f851-40c8-e611-09f428efbab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7anO_H7rsV",
        "colab_type": "text"
      },
      "source": [
        "####Mount Google Drive \n",
        "Copy dataset files from Drive to current session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGSLCQ4LJ0aQ",
        "colab_type": "code",
        "outputId": "e39b7f65-3c63-403c-8bec-f7cfad652bac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRm_sp90J7_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My\\ Drive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAEJAOUk8W-f",
        "colab_type": "text"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2x1VkdtKJ6h",
        "colab_type": "code",
        "outputId": "8bbe546f-e730-4957-9352-0d9dd59072ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!ls $path/Train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_bag_0.npy\t\t     train_bag_20000.npy\n",
            "train_bag_100000_110000.npy  train_bag_210000_220000.npy\n",
            "train_bag_110000_120000.npy  train_bag_220000_230000.npy\n",
            "train_bag_120000_130000.npy  train_bag_230000_240000.npy\n",
            "train_bag_130000_140000.npy  train_bag_240000_250000.npy\n",
            "train_bag_140000_150000.npy  train_bag_250000_257573.npy\n",
            "train_bag_150000_160000.npy  train_bag_30000_40000.npy\n",
            "train_bag_160000_170000.npy  train_bag_40000_50000.npy\n",
            "train_bag_170000_180000.npy  train_bag_50000_60000.npy\n",
            "train_bag_180000_190000.npy  train_bag_60000_70000.npy\n",
            "train_bag_190000_200000.npy  train_bag_70000_80000.npy\n",
            "train_bag_200000_210000.npy  train_bag_80000_90000.npy\n",
            "train_bag_20000_30000.npy    train_bag_90000_100000.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8gkEItsKPwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp $path/Train/train_bag* ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsGbtwNc8C6Y",
        "colab_type": "text"
      },
      "source": [
        "Merge splits of dataset into one.\n",
        "\n",
        "**emb** is short for *pre-extracted BERT embeddings.*\n",
        "**lab** is short for *Bag level Lables*\n",
        "\n",
        "For each of **Train**, **Test** and **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0S6QcKbLyjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = './'\n",
        "emb = list()\n",
        "lab = list()\n",
        "\n",
        "for file in os.listdir(directory):\n",
        "     filename = os.fsdecode(file)\n",
        "     if filename.endswith(\".npy\"):\n",
        "        data = np.load(os.path.join(directory, filename))\n",
        "        emb.append(data[0])\n",
        "        lab.append(data[1])\n",
        "\n",
        "\n",
        "emb_train = np.concatenate(emb, axis=0)\n",
        "lab_train = np.concatenate(lab, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfuoZ5iBMDCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del emb\n",
        "del lab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynjb6MK48b0W",
        "colab_type": "text"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLdiDlv3MuXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp $path/Test/test_bag* ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unxp1gMfM5im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load('test_bag_0_50000.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numzAvhfM-qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_test = data[0]\n",
        "lab_test = data[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVV4k0wtNIOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgS2v-iD8j6z",
        "colab_type": "text"
      },
      "source": [
        "####Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1c6F2F2ZEXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp $path/Val/val_bag* ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XG0V5CNZOlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load('val_bag_0_50000.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDJJ9C1_ZVpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_val = data[0]\n",
        "lab_val = data[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhFJgxA89lif",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia8WAV1oTFGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFh_3BzFT3ul",
        "colab_type": "text"
      },
      "source": [
        "##Metrics\n",
        "\n",
        "Used for evaluating models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNzmVwXTT8My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpm-y463T-3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(y_target, y_prediction):\n",
        "    acc = metrics.accuracy_score(y_target, y_prediction)\n",
        "    f1_micro = metrics.f1_score(y_target, y_prediction, average=\"micro\")\n",
        "    f1_macro = metrics.f1_score(y_target, y_prediction, average=\"macro\")\n",
        "    f1_weighted = metrics.f1_score(y_target, y_prediction, average=\"weighted\")\n",
        "\n",
        "    print((\n",
        "        f\"Accuracy: {acc}; \\n\"\n",
        "        f\"F1-Score (Micro): {f1_micro}; \\n\"\n",
        "        f\"F1-Score (Macro): {f1_macro}; \\n\"\n",
        "        f\"F1-Score (Weighted): {f1_weighted}\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnFmxjFHShCK",
        "colab_type": "text"
      },
      "source": [
        "##MIL CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUw0hQG6SjSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L = 500\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Conv1d(768, 20, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv1d(10, 50, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(25 * 1, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 3),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = H.view(-1, 25 * 1)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "\n",
        "        A = self.attention(H)  # NxK\n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        Y_hat = torch.argmax(Y_prob, dim=1).float()\n",
        "\n",
        "        return Y_prob, Y_hat, A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65v2si87SpFj",
        "colab_type": "code",
        "outputId": "5e87da3e-6672-4bd4-a570-687f72524a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pES0rONfSx8I",
        "colab_type": "code",
        "outputId": "353c1c7f-eb40-4841-f038-e7acb7a1d85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Attention()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2lQ30IcS5cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF_fJHmyS66o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AArAIv6uTA3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpTb-r9CTQjE",
        "colab_type": "text"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMgGwp3ZTLvm",
        "colab_type": "code",
        "outputId": "584551f3-a635-45f8-fc0b-cb23dbe08392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [10000/257353], Loss: 1.4607, Acc: 0.00\n",
            "Epoch [1/5], Step [20000/257353], Loss: 1.3297, Acc: 0.00\n",
            "Epoch [1/5], Step [30000/257353], Loss: 0.5809, Acc: 1.00\n",
            "Epoch [1/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [50000/257353], Loss: 0.5574, Acc: 1.00\n",
            "Epoch [1/5], Step [60000/257353], Loss: 0.5565, Acc: 1.00\n",
            "Epoch [1/5], Step [70000/257353], Loss: 0.8517, Acc: 1.00\n",
            "Epoch [1/5], Step [80000/257353], Loss: 0.6970, Acc: 1.00\n",
            "Epoch [1/5], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [100000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [1/5], Step [110000/257353], Loss: 0.5562, Acc: 1.00\n",
            "Epoch [1/5], Step [120000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [1/5], Step [130000/257353], Loss: 1.5257, Acc: 0.00\n",
            "Epoch [1/5], Step [140000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [1/5], Step [150000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [1/5], Step [160000/257353], Loss: 0.6904, Acc: 1.00\n",
            "Epoch [1/5], Step [170000/257353], Loss: 1.5412, Acc: 0.00\n",
            "Epoch [1/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [200000/257353], Loss: 0.5542, Acc: 1.00\n",
            "Epoch [1/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [220000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [1/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [2/5], Step [10000/257353], Loss: 1.4701, Acc: 0.00\n",
            "Epoch [2/5], Step [20000/257353], Loss: 0.6054, Acc: 1.00\n",
            "Epoch [2/5], Step [30000/257353], Loss: 0.6417, Acc: 1.00\n",
            "Epoch [2/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [50000/257353], Loss: 0.5813, Acc: 1.00\n",
            "Epoch [2/5], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [2/5], Step [70000/257353], Loss: 0.8512, Acc: 1.00\n",
            "Epoch [2/5], Step [80000/257353], Loss: 0.6150, Acc: 1.00\n",
            "Epoch [2/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [100000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [2/5], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [120000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [2/5], Step [130000/257353], Loss: 1.3446, Acc: 0.00\n",
            "Epoch [2/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [160000/257353], Loss: 0.7470, Acc: 1.00\n",
            "Epoch [2/5], Step [170000/257353], Loss: 1.5402, Acc: 0.00\n",
            "Epoch [2/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [200000/257353], Loss: 0.6160, Acc: 1.00\n",
            "Epoch [2/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [220000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [2/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [250000/257353], Loss: 0.5616, Acc: 1.00\n",
            "Epoch [3/5], Step [10000/257353], Loss: 1.4654, Acc: 0.00\n",
            "Epoch [3/5], Step [20000/257353], Loss: 0.7650, Acc: 1.00\n",
            "Epoch [3/5], Step [30000/257353], Loss: 1.3382, Acc: 0.00\n",
            "Epoch [3/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [50000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [3/5], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [70000/257353], Loss: 0.5637, Acc: 1.00\n",
            "Epoch [3/5], Step [80000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [3/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [100000/257353], Loss: 0.5621, Acc: 1.00\n",
            "Epoch [3/5], Step [110000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [3/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [130000/257353], Loss: 1.4096, Acc: 0.00\n",
            "Epoch [3/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [160000/257353], Loss: 1.4058, Acc: 0.00\n",
            "Epoch [3/5], Step [170000/257353], Loss: 1.5313, Acc: 0.00\n",
            "Epoch [3/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [200000/257353], Loss: 0.6695, Acc: 1.00\n",
            "Epoch [3/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [220000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [3/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/5], Step [10000/257353], Loss: 1.4583, Acc: 0.00\n",
            "Epoch [4/5], Step [20000/257353], Loss: 1.0090, Acc: 0.00\n",
            "Epoch [4/5], Step [30000/257353], Loss: 1.0385, Acc: 0.00\n",
            "Epoch [4/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/5], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/5], Step [70000/257353], Loss: 1.4024, Acc: 0.00\n",
            "Epoch [4/5], Step [80000/257353], Loss: 0.5565, Acc: 1.00\n",
            "Epoch [4/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [100000/257353], Loss: 0.5662, Acc: 1.00\n",
            "Epoch [4/5], Step [110000/257353], Loss: 0.5581, Acc: 1.00\n",
            "Epoch [4/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/5], Step [130000/257353], Loss: 1.5304, Acc: 0.00\n",
            "Epoch [4/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [160000/257353], Loss: 0.7620, Acc: 1.00\n",
            "Epoch [4/5], Step [170000/257353], Loss: 1.5201, Acc: 0.00\n",
            "Epoch [4/5], Step [180000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [4/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [200000/257353], Loss: 0.5728, Acc: 1.00\n",
            "Epoch [4/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [5/5], Step [10000/257353], Loss: 1.5401, Acc: 0.00\n",
            "Epoch [5/5], Step [20000/257353], Loss: 0.7125, Acc: 1.00\n",
            "Epoch [5/5], Step [30000/257353], Loss: 1.1129, Acc: 0.00\n",
            "Epoch [5/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [50000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [5/5], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [5/5], Step [70000/257353], Loss: 1.2679, Acc: 0.00\n",
            "Epoch [5/5], Step [80000/257353], Loss: 0.7186, Acc: 1.00\n",
            "Epoch [5/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [100000/257353], Loss: 0.5572, Acc: 1.00\n",
            "Epoch [5/5], Step [110000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [5/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [130000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [5/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [160000/257353], Loss: 1.2856, Acc: 0.00\n",
            "Epoch [5/5], Step [170000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [5/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [200000/257353], Loss: 0.7607, Acc: 1.00\n",
            "Epoch [5/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [250000/257353], Loss: 0.5553, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kANFiEyeTTa8",
        "colab_type": "text"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20urHEwcTVqc",
        "colab_type": "code",
        "outputId": "b1f48be7-6fa6-4e9b-f160-b58211ebb84e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data) # add\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda() # update\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "  \n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "71.336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov3N-GizThik",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEDhHggzTjps",
        "colab_type": "code",
        "outputId": "90bd67e6-c4a7-4abd-b3ce-4147a46b1531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data) # add\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda() # update\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.9997e-01, 2.9058e-05, 4.8652e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4858, 0.5142]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[1.7476e-04, 9.9976e-01, 6.3243e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1709, 0.1648, 0.1706, 0.1708, 0.1684, 0.1545]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[7.5517e-02, 9.2448e-01, 2.8346e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3338, 0.3338, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[5.7650e-08, 6.8026e-02, 9.3197e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0976, 0.1010, 0.1000, 0.0974, 0.1009, 0.1009, 0.1009, 0.0993, 0.1010,\n",
            "         0.1009]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[5.1298e-01, 4.8702e-01, 5.3601e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2490, 0.2520, 0.2482, 0.2508]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[3.2020e-06, 9.9999e-01, 2.9558e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4926, 0.5074]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[2.3678e-09, 1.2664e-01, 8.7336e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2006, 0.2009, 0.1971, 0.2004, 0.2009]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[1.3055e-02, 9.8694e-01, 3.5620e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1401, 0.1434, 0.1433, 0.1433, 0.1434, 0.1433, 0.1432]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.2257e-05, 2.2818e-02, 9.7717e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0822, 0.0843, 0.0853, 0.0851, 0.0848, 0.0854, 0.0824, 0.0811, 0.0849,\n",
            "         0.0854, 0.0798, 0.0793]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[8.9357e-01, 1.0643e-01, 2.6079e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1591, 0.1686, 0.1656, 0.1693, 0.1679, 0.1695]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[6.5026e-13, 6.7104e-10, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3453, 0.3082, 0.3465]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[0.4473, 0.5518, 0.0009]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1106, 0.1135, 0.1132, 0.1135, 0.1127, 0.1013, 0.1129, 0.1099, 0.1125]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[9.9983e-01, 1.7136e-04, 1.0137e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1425, 0.1436, 0.1440, 0.1443, 0.1442, 0.1373, 0.1441]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[1.8488e-02, 9.8151e-01, 7.3380e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1412, 0.1406, 0.1442, 0.1414, 0.1442, 0.1442, 0.1442]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[4.5667e-08, 2.6458e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1414, 0.1435, 0.1434, 0.1425, 0.1426, 0.1434, 0.1432]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[0.4527, 0.2953, 0.2520]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2003, 0.2002, 0.2003, 0.1998, 0.1994]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.9588e-01, 4.1198e-03, 7.0587e-22]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3335, 0.3330, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[0.0550, 0.7381, 0.2068]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0757, 0.0784, 0.0735, 0.0770, 0.0739, 0.0740, 0.0783, 0.0785, 0.0784,\n",
            "         0.0778, 0.0784, 0.0783, 0.0779]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[0.0039, 0.0027, 0.9934]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[1.3798e-02, 9.8620e-01, 1.0809e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0787, 0.0789, 0.0737, 0.0789, 0.0769, 0.0789, 0.0786, 0.0754, 0.0752,\n",
            "         0.0703, 0.0789, 0.0789, 0.0769]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[9.9773e-01, 2.2651e-03, 1.0665e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3345, 0.3308, 0.3347]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.9375e-01, 6.2499e-03, 8.7814e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4928, 0.5072]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 3.8428e-07, 6.8901e-30]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3324, 0.3338, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[9.4875e-01, 5.1253e-02, 8.1398e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2507, 0.2504, 0.2486, 0.2503]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.9969e-01, 3.1419e-04, 6.0320e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2533, 0.2535, 0.2538, 0.2394]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[5.3054e-08, 1.4781e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1454, 0.1487, 0.1487, 0.1343, 0.1393, 0.1487, 0.1349]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[3.2422e-08, 1.0000e+00, 2.0082e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1992, 0.2003, 0.2003, 0.2003, 0.1999]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[1.5897e-07, 1.8283e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1669, 0.1695, 0.1705, 0.1695, 0.1692, 0.1545]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[5.9661e-07, 4.2077e-03, 9.9579e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3301, 0.3257, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 3.2037e-07, 3.8978e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1691, 0.1691, 0.1691, 0.1559, 0.1676, 0.1691]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[1.0000e+00, 4.2533e-06, 6.2882e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0823, 0.0812, 0.0836, 0.0836, 0.0837, 0.0836, 0.0837, 0.0837, 0.0837,\n",
            "         0.0837, 0.0837, 0.0835]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[3.2044e-07, 1.7385e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2446, 0.2511, 0.2513, 0.2530]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 1.0730e-08, 1.2435e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1667, 0.1667, 0.1666, 0.1667, 0.1666, 0.1666]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[2.0099e-10, 9.7625e-01, 2.3752e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3226, 0.3388, 0.3386]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[0.0016, 0.9969, 0.0015]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5046, 0.4954]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[1.3615e-09, 2.2886e-03, 9.9771e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1453, 0.1453, 0.1309, 0.1453, 0.1426, 0.1452, 0.1452]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[7.4987e-05, 7.7816e-01, 2.2177e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2525, 0.2492, 0.2470, 0.2513]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[9.9975e-01, 2.4839e-04, 5.3138e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5034, 0.4966]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[1.2323e-14, 2.6976e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2476, 0.2540, 0.2540, 0.2444]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.9983e-01, 1.6905e-04, 5.4835e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5006, 0.4994]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[5.0178e-08, 4.1680e-05, 9.9996e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1256, 0.1260, 0.1256, 0.1268, 0.1268, 0.1258, 0.1267, 0.1167]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[5.0736e-07, 9.9956e-01, 4.3730e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5242, 0.4758]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[1.3744e-06, 1.0000e+00, 7.7638e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2497, 0.2503, 0.2496, 0.2503]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[8.7958e-04, 9.9911e-01, 1.3428e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1346, 0.1334, 0.1468, 0.1467, 0.1468, 0.1453, 0.1465]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[1.1012e-02, 9.8899e-01, 1.6154e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3333, 0.3329, 0.3338]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[1.2683e-07, 4.4650e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4827, 0.5173]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.8630e-01, 1.3700e-02, 1.0942e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1430, 0.1429, 0.1427, 0.1428, 0.1430, 0.1427, 0.1430]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[6.7060e-04, 9.9870e-01, 6.2676e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3108, 0.3461, 0.3431]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[1.0000e+00, 1.8296e-06, 1.1533e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1252, 0.1252, 0.1248, 0.1252, 0.1251, 0.1251, 0.1242, 0.1252]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[2.1772e-04, 9.9978e-01, 4.5369e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0424, 0.0440, 0.0430, 0.0441, 0.0428, 0.0427, 0.0439, 0.0441, 0.0441,\n",
            "         0.0440, 0.0440, 0.0439, 0.0441, 0.0439, 0.0439, 0.0409, 0.0439, 0.0441,\n",
            "         0.0404, 0.0438, 0.0441, 0.0441, 0.0440]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy9oiyHfTsEf",
        "colab_type": "text"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPf233YQVM_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA7fL3KSTreL",
        "colab_type": "code",
        "outputId": "d43424ae-24d3-44fc-bab6-4ff149cb82c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = 'model_state_'+str(model_num)\n",
        "PATH0 = 'model_'+str(model_num)\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_zsvrBTUOTw",
        "colab_type": "text"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq9SJPrZUPwC",
        "colab_type": "code",
        "outputId": "43917571-421c-4f5d-e558-88237efe5a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.71336; \n",
            "F1-Score (Micro): 0.7133600000000001; \n",
            "F1-Score (Macro): 0.7172436349892788; \n",
            "F1-Score (Weighted): 0.718470922216232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2JV3Bm_noDS_"
      },
      "source": [
        "##MIL FNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ApJFmc4oDTr",
        "colab": {}
      },
      "source": [
        "class AttentionFNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionFNN, self).__init__()\n",
        "        self.L = 500\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(32, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 3),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "\n",
        "        A = self.attention(H)  # NxK\n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        Y_hat = torch.argmax(Y_prob, dim=1).float()\n",
        "\n",
        "        return Y_prob, Y_hat, A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dc1895ff-93f7-4036-b678-1a770f517b41",
        "id": "xf4keNcpoDT3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "411940ff-d812-4967-e38d-9ef4e8536ed2",
        "id": "K7R38JocoDUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = AttentionFNN()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CMQ-GxqmoDUR",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AVq5LtsroDUX",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "__CuLl2RoDUd",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y_FOR5WCoDUj"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5281f44-1881-4797-e631-9196c2ce0cbf",
        "id": "yFuzK6GPoDUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape))\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [10000/257353], Loss: 1.5331, Acc: 0.00\n",
            "Epoch [1/5], Step [20000/257353], Loss: 1.4092, Acc: 0.00\n",
            "Epoch [1/5], Step [30000/257353], Loss: 1.5085, Acc: 0.00\n",
            "Epoch [1/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [50000/257353], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [1/5], Step [60000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [1/5], Step [70000/257353], Loss: 1.1881, Acc: 0.00\n",
            "Epoch [1/5], Step [80000/257353], Loss: 0.6007, Acc: 1.00\n",
            "Epoch [1/5], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [100000/257353], Loss: 0.5541, Acc: 1.00\n",
            "Epoch [1/5], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [130000/257353], Loss: 1.2580, Acc: 0.00\n",
            "Epoch [1/5], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [160000/257353], Loss: 1.1627, Acc: 0.00\n",
            "Epoch [1/5], Step [170000/257353], Loss: 1.5374, Acc: 0.00\n",
            "Epoch [1/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [200000/257353], Loss: 0.5714, Acc: 1.00\n",
            "Epoch [1/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [220000/257353], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [1/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [2/5], Step [10000/257353], Loss: 1.4825, Acc: 0.00\n",
            "Epoch [2/5], Step [20000/257353], Loss: 0.9676, Acc: 0.00\n",
            "Epoch [2/5], Step [30000/257353], Loss: 1.5018, Acc: 0.00\n",
            "Epoch [2/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [50000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [2/5], Step [60000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [2/5], Step [70000/257353], Loss: 1.0510, Acc: 1.00\n",
            "Epoch [2/5], Step [80000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [2/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [100000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [2/5], Step [110000/257353], Loss: 1.3655, Acc: 0.00\n",
            "Epoch [2/5], Step [120000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [2/5], Step [130000/257353], Loss: 1.5465, Acc: 0.00\n",
            "Epoch [2/5], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [160000/257353], Loss: 0.9043, Acc: 1.00\n",
            "Epoch [2/5], Step [170000/257353], Loss: 1.5319, Acc: 0.00\n",
            "Epoch [2/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [200000/257353], Loss: 0.7164, Acc: 1.00\n",
            "Epoch [2/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [10000/257353], Loss: 1.5267, Acc: 0.00\n",
            "Epoch [3/5], Step [20000/257353], Loss: 1.3711, Acc: 0.00\n",
            "Epoch [3/5], Step [30000/257353], Loss: 1.5424, Acc: 0.00\n",
            "Epoch [3/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [3/5], Step [60000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [3/5], Step [70000/257353], Loss: 0.7065, Acc: 1.00\n",
            "Epoch [3/5], Step [80000/257353], Loss: 1.1528, Acc: 0.00\n",
            "Epoch [3/5], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [100000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [3/5], Step [110000/257353], Loss: 0.5848, Acc: 1.00\n",
            "Epoch [3/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [130000/257353], Loss: 0.6201, Acc: 1.00\n",
            "Epoch [3/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [160000/257353], Loss: 0.8650, Acc: 1.00\n",
            "Epoch [3/5], Step [170000/257353], Loss: 1.5446, Acc: 0.00\n",
            "Epoch [3/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [200000/257353], Loss: 0.5871, Acc: 1.00\n",
            "Epoch [3/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [250000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [4/5], Step [10000/257353], Loss: 1.4608, Acc: 0.00\n",
            "Epoch [4/5], Step [20000/257353], Loss: 1.1923, Acc: 0.00\n",
            "Epoch [4/5], Step [30000/257353], Loss: 1.0265, Acc: 0.00\n",
            "Epoch [4/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [50000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [4/5], Step [60000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [4/5], Step [70000/257353], Loss: 0.9958, Acc: 1.00\n",
            "Epoch [4/5], Step [80000/257353], Loss: 0.6519, Acc: 1.00\n",
            "Epoch [4/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [100000/257353], Loss: 0.5805, Acc: 1.00\n",
            "Epoch [4/5], Step [110000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/5], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [130000/257353], Loss: 0.9160, Acc: 1.00\n",
            "Epoch [4/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [160000/257353], Loss: 1.4314, Acc: 0.00\n",
            "Epoch [4/5], Step [170000/257353], Loss: 1.5286, Acc: 0.00\n",
            "Epoch [4/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [200000/257353], Loss: 1.1034, Acc: 0.00\n",
            "Epoch [4/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [10000/257353], Loss: 1.4696, Acc: 0.00\n",
            "Epoch [5/5], Step [20000/257353], Loss: 1.2985, Acc: 0.00\n",
            "Epoch [5/5], Step [30000/257353], Loss: 0.6438, Acc: 1.00\n",
            "Epoch [5/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [5/5], Step [70000/257353], Loss: 0.8866, Acc: 1.00\n",
            "Epoch [5/5], Step [80000/257353], Loss: 0.5944, Acc: 1.00\n",
            "Epoch [5/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [110000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [5/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [130000/257353], Loss: 0.5737, Acc: 1.00\n",
            "Epoch [5/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [160000/257353], Loss: 1.0644, Acc: 0.00\n",
            "Epoch [5/5], Step [170000/257353], Loss: 1.5127, Acc: 0.00\n",
            "Epoch [5/5], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [200000/257353], Loss: 0.7353, Acc: 1.00\n",
            "Epoch [5/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N54Sokd-oDUu"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3d3f401c-4c18-4c6e-cbc0-2d588dd772d9",
        "id": "j_mu-SqUoDUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "69.134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TKYqoS9toDU9"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fd6c9bb1-c28e-4207-ff5e-bea0212cf94d",
        "id": "ZaUKZ1-eoDU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.9862e-01, 7.5398e-04, 6.2402e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5514, 0.4486]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[4.9682e-05, 9.9995e-01, 8.6226e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1415, 0.1924, 0.1415, 0.1415, 0.1497, 0.2335]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[7.8800e-02, 9.2120e-01, 1.3247e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3264, 0.3264, 0.3472]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[1.7460e-06, 7.4849e-03, 9.9251e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0892, 0.0860, 0.1480, 0.1267, 0.0863, 0.0868, 0.0882, 0.1170, 0.0860,\n",
            "         0.0860]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[3.6375e-01, 6.3625e-01, 3.4569e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2571, 0.2457, 0.2512, 0.2461]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[2.8204e-05, 9.9997e-01, 3.6773e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5324, 0.4676]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[5.4471e-09, 1.3886e-01, 8.6114e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1988, 0.1987, 0.2044, 0.1991, 0.1991]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[9.9490e-01, 5.1008e-03, 3.5199e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1458, 0.1417, 0.1441, 0.1425, 0.1417, 0.1417, 0.1425]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[9.2583e-05, 1.2208e-01, 8.7783e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0852, 0.1109, 0.0728, 0.0698, 0.0703, 0.0697, 0.0706, 0.1159, 0.0699,\n",
            "         0.0697, 0.0917, 0.1034]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[9.6250e-01, 3.7501e-02, 8.8082e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2007, 0.1465, 0.1718, 0.1462, 0.1887, 0.1461]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[2.1998e-10, 1.3271e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2705, 0.4613, 0.2681]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[5.6977e-01, 4.3023e-01, 6.1987e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1202, 0.1081, 0.1074, 0.1073, 0.1077, 0.1210, 0.1080, 0.1128, 0.1075]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[9.7953e-01, 2.0092e-02, 3.8075e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1648, 0.1324, 0.1359, 0.1324, 0.1332, 0.1664, 0.1351]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[7.8563e-02, 9.2144e-01, 2.6965e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1452, 0.1736, 0.1357, 0.1382, 0.1357, 0.1358, 0.1357]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[2.0649e-13, 3.4789e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1428, 0.1407, 0.1408, 0.1417, 0.1420, 0.1508, 0.1411]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[0.0977, 0.0986, 0.8037]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1988, 0.2005, 0.1988, 0.2028, 0.1992]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.9987e-01, 1.2824e-04, 1.3402e-26]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3330, 0.3340, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[0.1305, 0.8202, 0.0494]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0929, 0.0700, 0.1191, 0.0785, 0.0791, 0.0707, 0.0694, 0.0692, 0.0693,\n",
            "         0.0694, 0.0692, 0.0736, 0.0696]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[1.2826e-07, 5.2337e-06, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[1.3764e-01, 8.6236e-01, 7.1163e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0702, 0.0692, 0.0961, 0.0692, 0.0821, 0.0691, 0.0722, 0.0750, 0.0718,\n",
            "         0.1068, 0.0691, 0.0692, 0.0801]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[6.0448e-01, 3.9528e-01, 2.4010e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3354, 0.3322, 0.3323]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.9992e-01, 7.5182e-05, 8.5663e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5111, 0.4889]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 2.7449e-07, 1.0566e-32]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3350, 0.3325, 0.3325]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[9.8953e-01, 1.0469e-02, 5.4146e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2491, 0.2494, 0.2510, 0.2505]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.8576e-01, 1.4237e-02, 3.8704e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2392, 0.2495, 0.2384, 0.2729]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[5.8008e-05, 2.5706e-03, 9.9737e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1684, 0.1315, 0.1316, 0.1345, 0.1599, 0.1338, 0.1401]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[9.2618e-09, 1.0000e+00, 4.8833e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2141, 0.1965, 0.1964, 0.1965, 0.1965]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[2.1135e-06, 6.9413e-05, 9.9993e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1528, 0.1496, 0.1470, 0.1478, 0.1666, 0.2361]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[0.0016, 0.0547, 0.9437]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3215, 0.4286, 0.2499]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 3.1561e-07, 2.1132e-20]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1510, 0.1510, 0.1510, 0.2447, 0.1515, 0.1510]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[1.0000e+00, 6.2496e-08, 9.4296e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0840, 0.0920, 0.0826, 0.0824, 0.0824, 0.0824, 0.0824, 0.0824, 0.0824,\n",
            "         0.0824, 0.0824, 0.0824]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[5.1242e-10, 5.0144e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2835, 0.2390, 0.2418, 0.2357]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 7.8694e-09, 3.2292e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1646, 0.1629, 0.1629, 0.1629, 0.1629, 0.1840]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[1.9259e-06, 1.0000e+00, 2.6955e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3171, 0.3150, 0.3679]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[0.0150, 0.9811, 0.0039]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4919, 0.5081]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[4.9899e-10, 8.4967e-03, 9.9150e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1388, 0.1388, 0.1601, 0.1390, 0.1445, 0.1393, 0.1394]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[1.5440e-05, 9.9996e-01, 2.8253e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2356, 0.2442, 0.2815, 0.2388]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[9.9526e-01, 4.7359e-03, 3.3708e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4955, 0.5045]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[3.2008e-16, 5.8481e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2277, 0.2076, 0.2076, 0.3571]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.9841e-01, 1.5892e-03, 1.1347e-20]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4995, 0.5005]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[1.8600e-09, 5.4731e-05, 9.9995e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1147, 0.1143, 0.1139, 0.1133, 0.1135, 0.1290, 0.1133, 0.1878]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[5.7185e-05, 9.9861e-01, 1.3336e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3989, 0.6011]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[1.3551e-04, 9.9986e-01, 3.5583e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2501, 0.2497, 0.2505, 0.2497]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[1.8278e-03, 9.9817e-01, 1.8369e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2060, 0.1928, 0.1197, 0.1198, 0.1197, 0.1199, 0.1221]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[4.8206e-01, 5.1794e-01, 7.0589e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3315, 0.3369, 0.3315]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[1.7131e-05, 7.2646e-04, 9.9926e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.5839, 0.4161]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9964e-01, 3.6399e-04, 3.2286e-20]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1415, 0.1415, 0.1495, 0.1418, 0.1415, 0.1417, 0.1424]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[5.3628e-03, 9.9366e-01, 9.7718e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4417, 0.2586, 0.2997]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[9.9994e-01, 6.0484e-05, 3.2850e-22]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1247, 0.1246, 0.1251, 0.1246, 0.1246, 0.1265, 0.1253, 0.1246]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[1.3806e-03, 9.9862e-01, 4.4670e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0595, 0.0413, 0.0468, 0.0410, 0.0414, 0.0420, 0.0418, 0.0410, 0.0411,\n",
            "         0.0410, 0.0410, 0.0416, 0.0410, 0.0451, 0.0414, 0.0566, 0.0410, 0.0410,\n",
            "         0.0506, 0.0411, 0.0410, 0.0410, 0.0412]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kxlZdn0UoDVH"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea97439d-2853-40b0-80de-1c55efdcda77",
        "id": "mslbXcphoDVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "PATH = 'modelFNN_state_'+str(model_num)\n",
        "PATH0 = 'model_FNN_'+str(model_num)\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AttentionFNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7hgegsFHoDVc"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9df21ab9-b99e-4f11-a12c-bb89df2e4317",
        "id": "esUMI13JoDVf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69134; \n",
            "F1-Score (Micro): 0.69134; \n",
            "F1-Score (Macro): 0.6939714685692596; \n",
            "F1-Score (Weighted): 0.6954128021807596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQ9dWTyq6WaE"
      },
      "source": [
        "##MIL FNNs with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ddr6vHnO6WaY",
        "colab": {}
      },
      "source": [
        "class AttentionFNNDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionFNNDropout, self).__init__()\n",
        "        self.L = 500\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(32, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 3),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "\n",
        "        A = self.attention(H)  # NxK\n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        \n",
        "        Y_hat = torch.argmax(Y_prob, dim=1).float()\n",
        "\n",
        "        return Y_prob, Y_hat, A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e8acca01-a6b6-4204-c6e8-b2ec3b26436d",
        "id": "VT7gXiJH6Was",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b237c074-3ad4-46a5-b596-7c9584f50b7b",
        "id": "NkOWQgu56Wa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = AttentionFNNDropout()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bp-C95LT6WbF",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71u40vfN6WbL",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xcbHREro6WbQ",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_QLbOoF6q5f",
        "colab_type": "text"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "39534983-bb89-4dec-8e44-b97fafcf1fc5",
        "id": "0kdIrd6n6WbW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape))\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [10000/257353], Loss: 1.4770, Acc: 0.00\n",
            "Epoch [1/5], Step [20000/257353], Loss: 0.9089, Acc: 1.00\n",
            "Epoch [1/5], Step [30000/257353], Loss: 0.9226, Acc: 1.00\n",
            "Epoch [1/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [50000/257353], Loss: 0.5652, Acc: 1.00\n",
            "Epoch [1/5], Step [60000/257353], Loss: 0.5595, Acc: 1.00\n",
            "Epoch [1/5], Step [70000/257353], Loss: 0.9238, Acc: 1.00\n",
            "Epoch [1/5], Step [80000/257353], Loss: 0.8253, Acc: 1.00\n",
            "Epoch [1/5], Step [90000/257353], Loss: 0.6351, Acc: 1.00\n",
            "Epoch [1/5], Step [100000/257353], Loss: 1.1722, Acc: 0.00\n",
            "Epoch [1/5], Step [110000/257353], Loss: 0.8628, Acc: 1.00\n",
            "Epoch [1/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [130000/257353], Loss: 1.2916, Acc: 0.00\n",
            "Epoch [1/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [150000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [1/5], Step [160000/257353], Loss: 0.5710, Acc: 1.00\n",
            "Epoch [1/5], Step [170000/257353], Loss: 1.4904, Acc: 0.00\n",
            "Epoch [1/5], Step [180000/257353], Loss: 1.4991, Acc: 0.00\n",
            "Epoch [1/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [200000/257353], Loss: 0.5996, Acc: 1.00\n",
            "Epoch [1/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/5], Step [250000/257353], Loss: 0.5542, Acc: 1.00\n",
            "Epoch [2/5], Step [10000/257353], Loss: 1.1218, Acc: 0.00\n",
            "Epoch [2/5], Step [20000/257353], Loss: 1.1857, Acc: 0.00\n",
            "Epoch [2/5], Step [30000/257353], Loss: 1.3381, Acc: 0.00\n",
            "Epoch [2/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [50000/257353], Loss: 0.6017, Acc: 1.00\n",
            "Epoch [2/5], Step [60000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [2/5], Step [70000/257353], Loss: 1.4590, Acc: 0.00\n",
            "Epoch [2/5], Step [80000/257353], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [2/5], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [100000/257353], Loss: 0.8075, Acc: 1.00\n",
            "Epoch [2/5], Step [110000/257353], Loss: 1.4706, Acc: 0.00\n",
            "Epoch [2/5], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [130000/257353], Loss: 1.5199, Acc: 0.00\n",
            "Epoch [2/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/5], Step [160000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [2/5], Step [170000/257353], Loss: 1.5448, Acc: 0.00\n",
            "Epoch [2/5], Step [180000/257353], Loss: 1.4911, Acc: 0.00\n",
            "Epoch [2/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [200000/257353], Loss: 0.6230, Acc: 1.00\n",
            "Epoch [2/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [220000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [2/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/5], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [10000/257353], Loss: 1.4581, Acc: 0.00\n",
            "Epoch [3/5], Step [20000/257353], Loss: 1.2107, Acc: 0.00\n",
            "Epoch [3/5], Step [30000/257353], Loss: 1.4246, Acc: 0.00\n",
            "Epoch [3/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [50000/257353], Loss: 0.5598, Acc: 1.00\n",
            "Epoch [3/5], Step [60000/257353], Loss: 0.5635, Acc: 1.00\n",
            "Epoch [3/5], Step [70000/257353], Loss: 1.5297, Acc: 0.00\n",
            "Epoch [3/5], Step [80000/257353], Loss: 0.5797, Acc: 1.00\n",
            "Epoch [3/5], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [100000/257353], Loss: 0.5894, Acc: 1.00\n",
            "Epoch [3/5], Step [110000/257353], Loss: 0.9728, Acc: 1.00\n",
            "Epoch [3/5], Step [120000/257353], Loss: 0.5563, Acc: 1.00\n",
            "Epoch [3/5], Step [130000/257353], Loss: 1.5467, Acc: 0.00\n",
            "Epoch [3/5], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [160000/257353], Loss: 0.5756, Acc: 1.00\n",
            "Epoch [3/5], Step [170000/257353], Loss: 1.5200, Acc: 0.00\n",
            "Epoch [3/5], Step [180000/257353], Loss: 1.5492, Acc: 0.00\n",
            "Epoch [3/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [200000/257353], Loss: 0.6140, Acc: 1.00\n",
            "Epoch [3/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/5], Step [220000/257353], Loss: 0.6433, Acc: 1.00\n",
            "Epoch [3/5], Step [230000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [3/5], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/5], Step [250000/257353], Loss: 0.6195, Acc: 1.00\n",
            "Epoch [4/5], Step [10000/257353], Loss: 1.5000, Acc: 0.00\n",
            "Epoch [4/5], Step [20000/257353], Loss: 0.5777, Acc: 1.00\n",
            "Epoch [4/5], Step [30000/257353], Loss: 1.1356, Acc: 0.00\n",
            "Epoch [4/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [50000/257353], Loss: 0.5698, Acc: 1.00\n",
            "Epoch [4/5], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/5], Step [70000/257353], Loss: 1.1377, Acc: 0.00\n",
            "Epoch [4/5], Step [80000/257353], Loss: 1.1014, Acc: 0.00\n",
            "Epoch [4/5], Step [90000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [4/5], Step [100000/257353], Loss: 0.7116, Acc: 1.00\n",
            "Epoch [4/5], Step [110000/257353], Loss: 0.7857, Acc: 1.00\n",
            "Epoch [4/5], Step [120000/257353], Loss: 0.5864, Acc: 1.00\n",
            "Epoch [4/5], Step [130000/257353], Loss: 1.4773, Acc: 0.00\n",
            "Epoch [4/5], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/5], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [160000/257353], Loss: 1.2566, Acc: 0.00\n",
            "Epoch [4/5], Step [170000/257353], Loss: 1.5340, Acc: 0.00\n",
            "Epoch [4/5], Step [180000/257353], Loss: 1.5322, Acc: 0.00\n",
            "Epoch [4/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [200000/257353], Loss: 1.2263, Acc: 0.00\n",
            "Epoch [4/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [220000/257353], Loss: 0.5629, Acc: 1.00\n",
            "Epoch [4/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/5], Step [250000/257353], Loss: 0.5555, Acc: 1.00\n",
            "Epoch [5/5], Step [10000/257353], Loss: 1.4195, Acc: 0.00\n",
            "Epoch [5/5], Step [20000/257353], Loss: 0.8438, Acc: 1.00\n",
            "Epoch [5/5], Step [30000/257353], Loss: 0.8172, Acc: 1.00\n",
            "Epoch [5/5], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [50000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [5/5], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [5/5], Step [70000/257353], Loss: 0.9627, Acc: 1.00\n",
            "Epoch [5/5], Step [80000/257353], Loss: 0.5543, Acc: 1.00\n",
            "Epoch [5/5], Step [90000/257353], Loss: 0.7447, Acc: 1.00\n",
            "Epoch [5/5], Step [100000/257353], Loss: 0.5622, Acc: 1.00\n",
            "Epoch [5/5], Step [110000/257353], Loss: 0.8021, Acc: 1.00\n",
            "Epoch [5/5], Step [120000/257353], Loss: 0.5563, Acc: 1.00\n",
            "Epoch [5/5], Step [130000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [5/5], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/5], Step [160000/257353], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [5/5], Step [170000/257353], Loss: 1.5457, Acc: 0.00\n",
            "Epoch [5/5], Step [180000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [5/5], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [200000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [5/5], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [220000/257353], Loss: 0.5666, Acc: 1.00\n",
            "Epoch [5/5], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/5], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7wY4ewd46Wbe"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b560ae7c-4723-4fa4-cb2f-63751fdde995",
        "id": "fCbQIPPP6Wbh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "69.494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wAX0XhoL6Wbs"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5fbce56-e333-49d9-d653-d0922bede823",
        "id": "AsINgEV46Wbv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[0.9372, 0.0321, 0.0307]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5067, 0.4933]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[6.7071e-04, 9.9724e-01, 2.0922e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1647, 0.1712, 0.1651, 0.1647, 0.1650, 0.1693]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[8.1138e-01, 1.8862e-01, 5.6226e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3329, 0.3329, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[1.5697e-10, 1.4347e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0991, 0.0991, 0.1030, 0.1020, 0.0991, 0.0993, 0.0991, 0.1012, 0.0991,\n",
            "         0.0992]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[3.3137e-01, 6.6837e-01, 2.5737e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2521, 0.2491, 0.2498, 0.2490]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[4.3297e-04, 9.9955e-01, 2.0548e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5010, 0.4990]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[5.3533e-07, 3.5777e-02, 9.6422e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1994, 0.1994, 0.2014, 0.1994, 0.2003]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[5.9842e-01, 4.0158e-01, 5.8768e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1431, 0.1427, 0.1428, 0.1431, 0.1427, 0.1427, 0.1428]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[4.2870e-07, 7.6638e-04, 9.9923e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0827, 0.0827, 0.0827, 0.0827, 0.0827, 0.0826, 0.0842, 0.0849, 0.0826,\n",
            "         0.0827, 0.0841, 0.0855]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[9.8685e-01, 1.3146e-02, 1.3292e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1675, 0.1655, 0.1671, 0.1656, 0.1688, 0.1655]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[1.0671e-07, 4.4169e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3297, 0.3385, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[0.7942, 0.2024, 0.0034]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1104, 0.1105, 0.1108, 0.1103, 0.1119, 0.1122, 0.1104, 0.1125, 0.1110]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[0.6893, 0.0813, 0.2294]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1418, 0.1417, 0.1430, 0.1418, 0.1418, 0.1453, 0.1446]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[6.0514e-01, 3.9486e-01, 1.3528e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1444, 0.1446, 0.1416, 0.1447, 0.1416, 0.1416, 0.1415]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[1.3300e-13, 7.9970e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1422, 0.1422, 0.1422, 0.1431, 0.1424, 0.1457, 0.1422]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[2.6136e-07, 7.0037e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2000, 0.2001, 0.2000, 0.2000, 0.2000]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[8.6429e-01, 1.3571e-01, 9.2705e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3334, 0.3333, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[4.4371e-05, 1.3358e-02, 9.8660e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0766, 0.0767, 0.0788, 0.0768, 0.0769, 0.0773, 0.0765, 0.0765, 0.0766,\n",
            "         0.0767, 0.0765, 0.0775, 0.0767]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[0.0916, 0.0748, 0.8336]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[0.0090, 0.9778, 0.0133]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0770, 0.0766, 0.0773, 0.0767, 0.0771, 0.0766, 0.0767, 0.0766, 0.0769,\n",
            "         0.0785, 0.0766, 0.0766, 0.0766]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[0.2146, 0.6941, 0.0913]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3333, 0.3332, 0.3335]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.9417e-01, 5.8321e-03, 3.8628e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5023, 0.4977]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 1.3969e-07, 2.0786e-29]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3340, 0.3330, 0.3330]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[9.5250e-01, 4.7503e-02, 1.3834e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2490, 0.2488, 0.2491, 0.2531]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.9412e-01, 5.8759e-03, 3.3490e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2491, 0.2494, 0.2491, 0.2524]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[3.5767e-08, 1.1225e-05, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1455, 0.1417, 0.1418, 0.1439, 0.1422, 0.1417, 0.1433]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[1.0830e-05, 9.9999e-01, 8.4388e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2011, 0.1997, 0.1997, 0.1997, 0.1998]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[3.3908e-16, 7.3262e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1706, 0.1649, 0.1649, 0.1649, 0.1650, 0.1696]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[1.1053e-07, 7.0775e-04, 9.9929e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3386, 0.3363, 0.3250]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 1.8439e-06, 2.4369e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1658, 0.1658, 0.1658, 0.1710, 0.1658, 0.1658]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[9.9993e-01, 7.2611e-05, 7.8221e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0847, 0.0835, 0.0833, 0.0832, 0.0832, 0.0832, 0.0832, 0.0832, 0.0832,\n",
            "         0.0832, 0.0832, 0.0832]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[4.4622e-17, 2.4632e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2525, 0.2492, 0.2492, 0.2492]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 3.2575e-06, 2.6198e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1668, 0.1656, 0.1656, 0.1656, 0.1657, 0.1706]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[6.1797e-04, 9.9345e-01, 5.9277e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3300, 0.3291, 0.3409]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[6.2411e-04, 4.0992e-02, 9.5838e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.5022, 0.4978]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[2.0008e-06, 2.1226e-03, 9.9788e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1419, 0.1419, 0.1457, 0.1420, 0.1436, 0.1426, 0.1422]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[6.8772e-08, 5.2320e-04, 9.9948e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2501, 0.2498, 0.2502, 0.2499]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[8.4163e-01, 1.5837e-01, 4.1188e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4959, 0.5041]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[1.1038e-20, 1.8098e-14, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2533, 0.2469, 0.2469, 0.2529]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[1.0000e+00, 2.6803e-06, 8.2506e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4996, 0.5004]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[7.5864e-10, 1.5580e-05, 9.9998e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1248, 0.1246, 0.1251, 0.1245, 0.1245, 0.1259, 0.1245, 0.1260]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[3.8717e-05, 8.4548e-02, 9.1541e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4945, 0.5055]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[1.2198e-05, 9.9141e-01, 8.5751e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2499, 0.2494, 0.2514, 0.2493]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[2.4992e-04, 9.4693e-01, 5.2825e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1455, 0.1436, 0.1417, 0.1421, 0.1417, 0.1429, 0.1424]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[0.0134, 0.9839, 0.0027]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3356, 0.3337, 0.3308]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[1.6244e-06, 4.6591e-05, 9.9995e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.5060, 0.4940]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9986e-01, 1.4319e-04, 1.5344e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1423, 0.1428, 0.1426, 0.1444, 0.1423, 0.1431, 0.1425]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[0.0180, 0.9790, 0.0030]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3392, 0.3307, 0.3301]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[9.9999e-01, 7.6047e-06, 2.1690e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1260, 0.1248, 0.1249, 0.1248, 0.1250, 0.1248, 0.1249, 0.1248]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[0.0143, 0.9823, 0.0034]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0444, 0.0435, 0.0433, 0.0432, 0.0436, 0.0432, 0.0436, 0.0432, 0.0437,\n",
            "         0.0432, 0.0432, 0.0432, 0.0432, 0.0434, 0.0435, 0.0448, 0.0434, 0.0432,\n",
            "         0.0443, 0.0432, 0.0432, 0.0432, 0.0433]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yo5C_WYG6Wb5"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e92cfa74-d189-4e56-973a-14aebacad2a4",
        "id": "jcF8vsP26Wb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = 'modelFNNDropout_state_'+str(model_num)\n",
        "PATH0 = 'model_Dropout_FNN_'+str(model_num)\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AttentionFNNDropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k4pcYJrj6WcD"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "308e7ae8-eafd-4d8d-fd0c-2954b4dde6ad",
        "id": "4r6G60CG6WcF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.69494; \n",
            "F1-Score (Micro): 0.69494; \n",
            "F1-Score (Macro): 0.6877455857614011; \n",
            "F1-Score (Weighted): 0.6897591921896775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rci6OntPMPhq",
        "colab_type": "text"
      },
      "source": [
        "# Training for more Epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yKFO898IMM_U"
      },
      "source": [
        "##MIL CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e6bfd4b4-9090-4907-ab1f-7540517f0761",
        "id": "qKrXezW-MM_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e745aa0d-e520-4862-9e06-47f192904a99",
        "id": "J9eKXPCTMM_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Attention()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lC8CCXo8MNAE",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IkqQ4hmfMNAK",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7IX-V6fMNAP",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cvCwDXm3MNAV"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "82bc5238-a7b3-4855-a93d-c8134ffecedf",
        "id": "J8BK9FGyMNAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [10000/257353], Loss: 1.5238, Acc: 0.00\n",
            "Epoch [1/25], Step [20000/257353], Loss: 1.4695, Acc: 0.00\n",
            "Epoch [1/25], Step [30000/257353], Loss: 0.5774, Acc: 1.00\n",
            "Epoch [1/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [50000/257353], Loss: 0.5827, Acc: 1.00\n",
            "Epoch [1/25], Step [60000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [1/25], Step [70000/257353], Loss: 0.7761, Acc: 1.00\n",
            "Epoch [1/25], Step [80000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [1/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [100000/257353], Loss: 0.5609, Acc: 1.00\n",
            "Epoch [1/25], Step [110000/257353], Loss: 1.3425, Acc: 0.00\n",
            "Epoch [1/25], Step [120000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [1/25], Step [130000/257353], Loss: 1.3984, Acc: 0.00\n",
            "Epoch [1/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [150000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [1/25], Step [160000/257353], Loss: 1.1568, Acc: 0.00\n",
            "Epoch [1/25], Step [170000/257353], Loss: 1.5414, Acc: 0.00\n",
            "Epoch [1/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [200000/257353], Loss: 0.9746, Acc: 0.00\n",
            "Epoch [1/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [220000/257353], Loss: 0.5632, Acc: 1.00\n",
            "Epoch [1/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/25], Step [10000/257353], Loss: 1.4808, Acc: 0.00\n",
            "Epoch [2/25], Step [20000/257353], Loss: 0.7943, Acc: 1.00\n",
            "Epoch [2/25], Step [30000/257353], Loss: 1.4393, Acc: 0.00\n",
            "Epoch [2/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [50000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [2/25], Step [60000/257353], Loss: 0.5539, Acc: 1.00\n",
            "Epoch [2/25], Step [70000/257353], Loss: 1.1458, Acc: 0.00\n",
            "Epoch [2/25], Step [80000/257353], Loss: 0.6210, Acc: 1.00\n",
            "Epoch [2/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [100000/257353], Loss: 0.5583, Acc: 1.00\n",
            "Epoch [2/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/25], Step [130000/257353], Loss: 1.2916, Acc: 0.00\n",
            "Epoch [2/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/25], Step [160000/257353], Loss: 0.9478, Acc: 1.00\n",
            "Epoch [2/25], Step [170000/257353], Loss: 1.5456, Acc: 0.00\n",
            "Epoch [2/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [200000/257353], Loss: 0.5807, Acc: 1.00\n",
            "Epoch [2/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [220000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [2/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [250000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [3/25], Step [10000/257353], Loss: 1.4580, Acc: 0.00\n",
            "Epoch [3/25], Step [20000/257353], Loss: 1.2765, Acc: 0.00\n",
            "Epoch [3/25], Step [30000/257353], Loss: 0.9421, Acc: 1.00\n",
            "Epoch [3/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [50000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [3/25], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [3/25], Step [70000/257353], Loss: 1.0610, Acc: 0.00\n",
            "Epoch [3/25], Step [80000/257353], Loss: 0.5734, Acc: 1.00\n",
            "Epoch [3/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [100000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [3/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [120000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [3/25], Step [130000/257353], Loss: 1.1847, Acc: 0.00\n",
            "Epoch [3/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/25], Step [160000/257353], Loss: 1.2933, Acc: 0.00\n",
            "Epoch [3/25], Step [170000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [3/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [200000/257353], Loss: 0.5623, Acc: 1.00\n",
            "Epoch [3/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [220000/257353], Loss: 0.5543, Acc: 1.00\n",
            "Epoch [3/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/25], Step [10000/257353], Loss: 1.5496, Acc: 0.00\n",
            "Epoch [4/25], Step [20000/257353], Loss: 0.9052, Acc: 1.00\n",
            "Epoch [4/25], Step [30000/257353], Loss: 1.0240, Acc: 0.00\n",
            "Epoch [4/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [50000/257353], Loss: 0.5540, Acc: 1.00\n",
            "Epoch [4/25], Step [60000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [4/25], Step [70000/257353], Loss: 0.6814, Acc: 1.00\n",
            "Epoch [4/25], Step [80000/257353], Loss: 0.5685, Acc: 1.00\n",
            "Epoch [4/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [100000/257353], Loss: 0.5553, Acc: 1.00\n",
            "Epoch [4/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/25], Step [130000/257353], Loss: 1.3536, Acc: 0.00\n",
            "Epoch [4/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/25], Step [160000/257353], Loss: 1.2619, Acc: 0.00\n",
            "Epoch [4/25], Step [170000/257353], Loss: 1.5486, Acc: 0.00\n",
            "Epoch [4/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [200000/257353], Loss: 0.5997, Acc: 1.00\n",
            "Epoch [4/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [220000/257353], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [4/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/25], Step [10000/257353], Loss: 1.5199, Acc: 0.00\n",
            "Epoch [5/25], Step [20000/257353], Loss: 1.2649, Acc: 0.00\n",
            "Epoch [5/25], Step [30000/257353], Loss: 0.8248, Acc: 1.00\n",
            "Epoch [5/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [5/25], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [5/25], Step [70000/257353], Loss: 0.8775, Acc: 1.00\n",
            "Epoch [5/25], Step [80000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [5/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [100000/257353], Loss: 0.5592, Acc: 1.00\n",
            "Epoch [5/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [120000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [5/25], Step [130000/257353], Loss: 1.4129, Acc: 0.00\n",
            "Epoch [5/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/25], Step [160000/257353], Loss: 1.2761, Acc: 0.00\n",
            "Epoch [5/25], Step [170000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [5/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [200000/257353], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [5/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [220000/257353], Loss: 0.5603, Acc: 1.00\n",
            "Epoch [5/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [6/25], Step [10000/257353], Loss: 1.4702, Acc: 0.00\n",
            "Epoch [6/25], Step [20000/257353], Loss: 0.7661, Acc: 1.00\n",
            "Epoch [6/25], Step [30000/257353], Loss: 0.7154, Acc: 1.00\n",
            "Epoch [6/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [6/25], Step [60000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [6/25], Step [70000/257353], Loss: 1.2916, Acc: 0.00\n",
            "Epoch [6/25], Step [80000/257353], Loss: 0.8846, Acc: 1.00\n",
            "Epoch [6/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [100000/257353], Loss: 0.5634, Acc: 1.00\n",
            "Epoch [6/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [120000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [6/25], Step [130000/257353], Loss: 1.4796, Acc: 0.00\n",
            "Epoch [6/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [160000/257353], Loss: 0.9850, Acc: 0.00\n",
            "Epoch [6/25], Step [170000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [6/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [200000/257353], Loss: 0.6767, Acc: 1.00\n",
            "Epoch [6/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [220000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [6/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/25], Step [10000/257353], Loss: 1.4640, Acc: 0.00\n",
            "Epoch [7/25], Step [20000/257353], Loss: 1.4905, Acc: 0.00\n",
            "Epoch [7/25], Step [30000/257353], Loss: 0.8350, Acc: 1.00\n",
            "Epoch [7/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [7/25], Step [60000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [7/25], Step [70000/257353], Loss: 1.0449, Acc: 0.00\n",
            "Epoch [7/25], Step [80000/257353], Loss: 0.8525, Acc: 1.00\n",
            "Epoch [7/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [100000/257353], Loss: 0.5583, Acc: 1.00\n",
            "Epoch [7/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/25], Step [120000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [7/25], Step [130000/257353], Loss: 1.4322, Acc: 0.00\n",
            "Epoch [7/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [160000/257353], Loss: 1.5107, Acc: 0.00\n",
            "Epoch [7/25], Step [170000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [7/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [200000/257353], Loss: 1.3939, Acc: 0.00\n",
            "Epoch [7/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [220000/257353], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [7/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [250000/257353], Loss: 0.5546, Acc: 1.00\n",
            "Epoch [8/25], Step [10000/257353], Loss: 1.5096, Acc: 0.00\n",
            "Epoch [8/25], Step [20000/257353], Loss: 1.3381, Acc: 0.00\n",
            "Epoch [8/25], Step [30000/257353], Loss: 1.0804, Acc: 0.00\n",
            "Epoch [8/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [8/25], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [8/25], Step [70000/257353], Loss: 0.5871, Acc: 1.00\n",
            "Epoch [8/25], Step [80000/257353], Loss: 0.6489, Acc: 1.00\n",
            "Epoch [8/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [100000/257353], Loss: 0.5647, Acc: 1.00\n",
            "Epoch [8/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [120000/257353], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [8/25], Step [130000/257353], Loss: 1.2289, Acc: 0.00\n",
            "Epoch [8/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [160000/257353], Loss: 1.0549, Acc: 0.00\n",
            "Epoch [8/25], Step [170000/257353], Loss: 1.5505, Acc: 0.00\n",
            "Epoch [8/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [200000/257353], Loss: 1.3244, Acc: 0.00\n",
            "Epoch [8/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [220000/257353], Loss: 0.5667, Acc: 1.00\n",
            "Epoch [8/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [9/25], Step [10000/257353], Loss: 1.5421, Acc: 0.00\n",
            "Epoch [9/25], Step [20000/257353], Loss: 0.8828, Acc: 1.00\n",
            "Epoch [9/25], Step [30000/257353], Loss: 0.9923, Acc: 0.00\n",
            "Epoch [9/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/25], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [9/25], Step [70000/257353], Loss: 0.5721, Acc: 1.00\n",
            "Epoch [9/25], Step [80000/257353], Loss: 0.8623, Acc: 1.00\n",
            "Epoch [9/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [100000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [9/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/25], Step [120000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [9/25], Step [130000/257353], Loss: 1.3335, Acc: 0.00\n",
            "Epoch [9/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [160000/257353], Loss: 1.0273, Acc: 0.00\n",
            "Epoch [9/25], Step [170000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [9/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [200000/257353], Loss: 0.6958, Acc: 1.00\n",
            "Epoch [9/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [220000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [9/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [10000/257353], Loss: 1.5133, Acc: 0.00\n",
            "Epoch [10/25], Step [20000/257353], Loss: 0.7619, Acc: 1.00\n",
            "Epoch [10/25], Step [30000/257353], Loss: 0.9158, Acc: 1.00\n",
            "Epoch [10/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [50000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [10/25], Step [60000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [10/25], Step [70000/257353], Loss: 0.7538, Acc: 1.00\n",
            "Epoch [10/25], Step [80000/257353], Loss: 0.9715, Acc: 0.00\n",
            "Epoch [10/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [100000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [10/25], Step [110000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [10/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/25], Step [130000/257353], Loss: 1.4830, Acc: 0.00\n",
            "Epoch [10/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/25], Step [160000/257353], Loss: 0.6131, Acc: 1.00\n",
            "Epoch [10/25], Step [170000/257353], Loss: 1.5479, Acc: 0.00\n",
            "Epoch [10/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [200000/257353], Loss: 0.8571, Acc: 1.00\n",
            "Epoch [10/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [220000/257353], Loss: 0.5865, Acc: 1.00\n",
            "Epoch [10/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [250000/257353], Loss: 0.5654, Acc: 1.00\n",
            "Epoch [11/25], Step [10000/257353], Loss: 1.5477, Acc: 0.00\n",
            "Epoch [11/25], Step [20000/257353], Loss: 1.1729, Acc: 0.00\n",
            "Epoch [11/25], Step [30000/257353], Loss: 0.6564, Acc: 1.00\n",
            "Epoch [11/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [50000/257353], Loss: 0.6627, Acc: 1.00\n",
            "Epoch [11/25], Step [60000/257353], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [11/25], Step [70000/257353], Loss: 1.2783, Acc: 0.00\n",
            "Epoch [11/25], Step [80000/257353], Loss: 0.6221, Acc: 1.00\n",
            "Epoch [11/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [100000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [11/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [120000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [11/25], Step [130000/257353], Loss: 1.5048, Acc: 0.00\n",
            "Epoch [11/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [160000/257353], Loss: 1.1172, Acc: 0.00\n",
            "Epoch [11/25], Step [170000/257353], Loss: 1.5478, Acc: 0.00\n",
            "Epoch [11/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [200000/257353], Loss: 1.1488, Acc: 0.00\n",
            "Epoch [11/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [220000/257353], Loss: 0.5681, Acc: 1.00\n",
            "Epoch [11/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [12/25], Step [10000/257353], Loss: 1.5149, Acc: 0.00\n",
            "Epoch [12/25], Step [20000/257353], Loss: 1.1882, Acc: 0.00\n",
            "Epoch [12/25], Step [30000/257353], Loss: 0.5687, Acc: 1.00\n",
            "Epoch [12/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [12/25], Step [60000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [12/25], Step [70000/257353], Loss: 1.1010, Acc: 0.00\n",
            "Epoch [12/25], Step [80000/257353], Loss: 0.5716, Acc: 1.00\n",
            "Epoch [12/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [12/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [12/25], Step [130000/257353], Loss: 1.5278, Acc: 0.00\n",
            "Epoch [12/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/25], Step [160000/257353], Loss: 1.1259, Acc: 0.00\n",
            "Epoch [12/25], Step [170000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [12/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [200000/257353], Loss: 1.2822, Acc: 0.00\n",
            "Epoch [12/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [220000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [12/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [250000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [13/25], Step [10000/257353], Loss: 1.5506, Acc: 0.00\n",
            "Epoch [13/25], Step [20000/257353], Loss: 0.8616, Acc: 1.00\n",
            "Epoch [13/25], Step [30000/257353], Loss: 0.5778, Acc: 1.00\n",
            "Epoch [13/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/25], Step [60000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [13/25], Step [70000/257353], Loss: 1.2927, Acc: 0.00\n",
            "Epoch [13/25], Step [80000/257353], Loss: 0.5931, Acc: 1.00\n",
            "Epoch [13/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [100000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [13/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/25], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [13/25], Step [130000/257353], Loss: 1.3279, Acc: 0.00\n",
            "Epoch [13/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [160000/257353], Loss: 0.6246, Acc: 1.00\n",
            "Epoch [13/25], Step [170000/257353], Loss: 1.5478, Acc: 0.00\n",
            "Epoch [13/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [200000/257353], Loss: 1.2237, Acc: 0.00\n",
            "Epoch [13/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [220000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [13/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/25], Step [10000/257353], Loss: 1.4579, Acc: 0.00\n",
            "Epoch [14/25], Step [20000/257353], Loss: 1.0138, Acc: 0.00\n",
            "Epoch [14/25], Step [30000/257353], Loss: 1.1609, Acc: 0.00\n",
            "Epoch [14/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [14/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/25], Step [70000/257353], Loss: 0.8432, Acc: 1.00\n",
            "Epoch [14/25], Step [80000/257353], Loss: 0.5995, Acc: 1.00\n",
            "Epoch [14/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [100000/257353], Loss: 0.5740, Acc: 1.00\n",
            "Epoch [14/25], Step [110000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [14/25], Step [120000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [14/25], Step [130000/257353], Loss: 0.7445, Acc: 1.00\n",
            "Epoch [14/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [160000/257353], Loss: 1.3364, Acc: 0.00\n",
            "Epoch [14/25], Step [170000/257353], Loss: 1.5415, Acc: 0.00\n",
            "Epoch [14/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [200000/257353], Loss: 1.1432, Acc: 0.00\n",
            "Epoch [14/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [220000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [14/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [10000/257353], Loss: 1.4581, Acc: 0.00\n",
            "Epoch [15/25], Step [20000/257353], Loss: 1.4862, Acc: 0.00\n",
            "Epoch [15/25], Step [30000/257353], Loss: 0.7036, Acc: 1.00\n",
            "Epoch [15/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/25], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [15/25], Step [70000/257353], Loss: 1.0427, Acc: 0.00\n",
            "Epoch [15/25], Step [80000/257353], Loss: 0.5972, Acc: 1.00\n",
            "Epoch [15/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [100000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [15/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [120000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [15/25], Step [130000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [15/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [160000/257353], Loss: 0.6574, Acc: 1.00\n",
            "Epoch [15/25], Step [170000/257353], Loss: 1.5065, Acc: 0.00\n",
            "Epoch [15/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [200000/257353], Loss: 0.7625, Acc: 1.00\n",
            "Epoch [15/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [250000/257353], Loss: 0.5593, Acc: 1.00\n",
            "Epoch [16/25], Step [10000/257353], Loss: 1.4724, Acc: 0.00\n",
            "Epoch [16/25], Step [20000/257353], Loss: 1.3901, Acc: 0.00\n",
            "Epoch [16/25], Step [30000/257353], Loss: 0.5662, Acc: 1.00\n",
            "Epoch [16/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/25], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [16/25], Step [70000/257353], Loss: 0.7048, Acc: 1.00\n",
            "Epoch [16/25], Step [80000/257353], Loss: 0.5683, Acc: 1.00\n",
            "Epoch [16/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [100000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [16/25], Step [110000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [16/25], Step [120000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [16/25], Step [130000/257353], Loss: 0.7281, Acc: 1.00\n",
            "Epoch [16/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [160000/257353], Loss: 0.9409, Acc: 1.00\n",
            "Epoch [16/25], Step [170000/257353], Loss: 1.5403, Acc: 0.00\n",
            "Epoch [16/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [200000/257353], Loss: 0.6446, Acc: 1.00\n",
            "Epoch [16/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [16/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/25], Step [10000/257353], Loss: 1.5171, Acc: 0.00\n",
            "Epoch [17/25], Step [20000/257353], Loss: 0.7215, Acc: 1.00\n",
            "Epoch [17/25], Step [30000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [17/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/25], Step [60000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [17/25], Step [70000/257353], Loss: 0.7392, Acc: 1.00\n",
            "Epoch [17/25], Step [80000/257353], Loss: 0.5629, Acc: 1.00\n",
            "Epoch [17/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [100000/257353], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [17/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [120000/257353], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [17/25], Step [130000/257353], Loss: 0.9952, Acc: 0.00\n",
            "Epoch [17/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [160000/257353], Loss: 0.8242, Acc: 1.00\n",
            "Epoch [17/25], Step [170000/257353], Loss: 1.5480, Acc: 0.00\n",
            "Epoch [17/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [200000/257353], Loss: 0.5826, Acc: 1.00\n",
            "Epoch [17/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/25], Step [10000/257353], Loss: 1.5244, Acc: 0.00\n",
            "Epoch [18/25], Step [20000/257353], Loss: 1.4772, Acc: 0.00\n",
            "Epoch [18/25], Step [30000/257353], Loss: 0.6677, Acc: 1.00\n",
            "Epoch [18/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/25], Step [60000/257353], Loss: 0.5680, Acc: 1.00\n",
            "Epoch [18/25], Step [70000/257353], Loss: 0.5921, Acc: 1.00\n",
            "Epoch [18/25], Step [80000/257353], Loss: 0.6386, Acc: 1.00\n",
            "Epoch [18/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [100000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [18/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [120000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [18/25], Step [130000/257353], Loss: 1.4573, Acc: 0.00\n",
            "Epoch [18/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [160000/257353], Loss: 1.2999, Acc: 0.00\n",
            "Epoch [18/25], Step [170000/257353], Loss: 1.5492, Acc: 0.00\n",
            "Epoch [18/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [200000/257353], Loss: 0.5585, Acc: 1.00\n",
            "Epoch [18/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [220000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [18/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [250000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [19/25], Step [10000/257353], Loss: 1.5488, Acc: 0.00\n",
            "Epoch [19/25], Step [20000/257353], Loss: 1.5267, Acc: 0.00\n",
            "Epoch [19/25], Step [30000/257353], Loss: 0.5713, Acc: 1.00\n",
            "Epoch [19/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/25], Step [70000/257353], Loss: 0.6992, Acc: 1.00\n",
            "Epoch [19/25], Step [80000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [19/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [100000/257353], Loss: 0.5583, Acc: 1.00\n",
            "Epoch [19/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [120000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [19/25], Step [130000/257353], Loss: 1.2212, Acc: 0.00\n",
            "Epoch [19/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [160000/257353], Loss: 0.5849, Acc: 1.00\n",
            "Epoch [19/25], Step [170000/257353], Loss: 1.5309, Acc: 0.00\n",
            "Epoch [19/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [200000/257353], Loss: 0.9377, Acc: 1.00\n",
            "Epoch [19/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [220000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [19/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [10000/257353], Loss: 1.4620, Acc: 0.00\n",
            "Epoch [20/25], Step [20000/257353], Loss: 0.9544, Acc: 1.00\n",
            "Epoch [20/25], Step [30000/257353], Loss: 1.2029, Acc: 0.00\n",
            "Epoch [20/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [60000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [20/25], Step [70000/257353], Loss: 0.6391, Acc: 1.00\n",
            "Epoch [20/25], Step [80000/257353], Loss: 0.5553, Acc: 1.00\n",
            "Epoch [20/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [100000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [20/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [120000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [20/25], Step [130000/257353], Loss: 0.6532, Acc: 1.00\n",
            "Epoch [20/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [160000/257353], Loss: 0.8183, Acc: 1.00\n",
            "Epoch [20/25], Step [170000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [20/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [200000/257353], Loss: 0.5636, Acc: 1.00\n",
            "Epoch [20/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [250000/257353], Loss: 0.8253, Acc: 1.00\n",
            "Epoch [21/25], Step [10000/257353], Loss: 1.4720, Acc: 0.00\n",
            "Epoch [21/25], Step [20000/257353], Loss: 0.9655, Acc: 0.00\n",
            "Epoch [21/25], Step [30000/257353], Loss: 1.4237, Acc: 0.00\n",
            "Epoch [21/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [60000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [21/25], Step [70000/257353], Loss: 0.8682, Acc: 1.00\n",
            "Epoch [21/25], Step [80000/257353], Loss: 0.6167, Acc: 1.00\n",
            "Epoch [21/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [100000/257353], Loss: 0.5658, Acc: 1.00\n",
            "Epoch [21/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [130000/257353], Loss: 1.1349, Acc: 0.00\n",
            "Epoch [21/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [160000/257353], Loss: 1.1598, Acc: 0.00\n",
            "Epoch [21/25], Step [170000/257353], Loss: 1.5501, Acc: 0.00\n",
            "Epoch [21/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [21/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [200000/257353], Loss: 0.5544, Acc: 1.00\n",
            "Epoch [21/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [22/25], Step [10000/257353], Loss: 1.5152, Acc: 0.00\n",
            "Epoch [22/25], Step [20000/257353], Loss: 1.1957, Acc: 0.00\n",
            "Epoch [22/25], Step [30000/257353], Loss: 0.6143, Acc: 1.00\n",
            "Epoch [22/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [60000/257353], Loss: 0.5575, Acc: 1.00\n",
            "Epoch [22/25], Step [70000/257353], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [22/25], Step [80000/257353], Loss: 0.5733, Acc: 1.00\n",
            "Epoch [22/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [100000/257353], Loss: 0.5704, Acc: 1.00\n",
            "Epoch [22/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [22/25], Step [120000/257353], Loss: 0.5544, Acc: 1.00\n",
            "Epoch [22/25], Step [130000/257353], Loss: 1.2569, Acc: 0.00\n",
            "Epoch [22/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [160000/257353], Loss: 1.0129, Acc: 0.00\n",
            "Epoch [22/25], Step [170000/257353], Loss: 1.5389, Acc: 0.00\n",
            "Epoch [22/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [22/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [200000/257353], Loss: 0.8107, Acc: 1.00\n",
            "Epoch [22/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [23/25], Step [10000/257353], Loss: 1.5241, Acc: 0.00\n",
            "Epoch [23/25], Step [20000/257353], Loss: 0.7622, Acc: 1.00\n",
            "Epoch [23/25], Step [30000/257353], Loss: 1.3967, Acc: 0.00\n",
            "Epoch [23/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [23/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [23/25], Step [70000/257353], Loss: 0.6289, Acc: 1.00\n",
            "Epoch [23/25], Step [80000/257353], Loss: 0.5972, Acc: 1.00\n",
            "Epoch [23/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [100000/257353], Loss: 0.5760, Acc: 1.00\n",
            "Epoch [23/25], Step [110000/257353], Loss: 0.5580, Acc: 1.00\n",
            "Epoch [23/25], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [23/25], Step [130000/257353], Loss: 1.2080, Acc: 0.00\n",
            "Epoch [23/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [160000/257353], Loss: 1.2879, Acc: 0.00\n",
            "Epoch [23/25], Step [170000/257353], Loss: 1.5397, Acc: 0.00\n",
            "Epoch [23/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [200000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [23/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [23/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [24/25], Step [10000/257353], Loss: 1.4794, Acc: 0.00\n",
            "Epoch [24/25], Step [20000/257353], Loss: 0.6365, Acc: 1.00\n",
            "Epoch [24/25], Step [30000/257353], Loss: 1.1996, Acc: 0.00\n",
            "Epoch [24/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [60000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [24/25], Step [70000/257353], Loss: 1.5453, Acc: 0.00\n",
            "Epoch [24/25], Step [80000/257353], Loss: 0.5665, Acc: 1.00\n",
            "Epoch [24/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [100000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [24/25], Step [110000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [24/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [24/25], Step [130000/257353], Loss: 0.5817, Acc: 1.00\n",
            "Epoch [24/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [160000/257353], Loss: 0.8678, Acc: 1.00\n",
            "Epoch [24/25], Step [170000/257353], Loss: 1.5493, Acc: 0.00\n",
            "Epoch [24/25], Step [180000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [24/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [200000/257353], Loss: 1.5345, Acc: 0.00\n",
            "Epoch [24/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [250000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [25/25], Step [10000/257353], Loss: 1.5288, Acc: 0.00\n",
            "Epoch [25/25], Step [20000/257353], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [25/25], Step [30000/257353], Loss: 1.4866, Acc: 0.00\n",
            "Epoch [25/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [25/25], Step [60000/257353], Loss: 0.5831, Acc: 1.00\n",
            "Epoch [25/25], Step [70000/257353], Loss: 0.5764, Acc: 1.00\n",
            "Epoch [25/25], Step [80000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [25/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [100000/257353], Loss: 0.5818, Acc: 1.00\n",
            "Epoch [25/25], Step [110000/257353], Loss: 0.5564, Acc: 1.00\n",
            "Epoch [25/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [25/25], Step [130000/257353], Loss: 0.6011, Acc: 1.00\n",
            "Epoch [25/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [160000/257353], Loss: 1.3397, Acc: 0.00\n",
            "Epoch [25/25], Step [170000/257353], Loss: 1.5476, Acc: 0.00\n",
            "Epoch [25/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [25/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [200000/257353], Loss: 1.4390, Acc: 0.00\n",
            "Epoch [25/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [220000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [25/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5DUSjumYMNAg"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9435ac7b-c3c6-43ee-9678-b92224b7a68e",
        "id": "o9LhV4zdMNAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data) \n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "74.078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1idw6BwKMNAu"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9babb11f-ee23-49a8-d7ea-9af800a19816",
        "id": "ThDV7AgzMNAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[0.9070, 0.0644, 0.0286]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4401, 0.5599]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[6.2645e-07, 9.9934e-01, 6.5891e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2015, 0.1554, 0.2010, 0.2010, 0.1975, 0.0436]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[2.2847e-01, 7.7153e-01, 1.0482e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3969, 0.3975, 0.2055]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[8.7134e-13, 1.0988e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1046, 0.1289, 0.0456, 0.0450, 0.1289, 0.1249, 0.1290, 0.0551, 0.1290,\n",
            "         0.1090]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[9.9722e-01, 2.7811e-03, 1.4083e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1814, 0.3212, 0.1623, 0.3352]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[2.4563e-05, 9.7163e-01, 2.8350e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4679, 0.5321]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[3.4649e-12, 1.6578e-03, 9.9834e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2006, 0.2006, 0.1980, 0.2004, 0.2004]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[6.4742e-01, 3.5258e-01, 1.0143e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1147, 0.1549, 0.1238, 0.1469, 0.1550, 0.1549, 0.1498]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[3.9815e-13, 1.1136e-05, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0916, 0.0749, 0.0946, 0.0953, 0.0958, 0.0960, 0.0957, 0.0537, 0.0960,\n",
            "         0.0958, 0.0388, 0.0719]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[8.4227e-01, 1.5770e-01, 2.2722e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1173, 0.1769, 0.1648, 0.1670, 0.1850, 0.1890]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[2.8283e-15, 1.0756e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4361, 0.1178, 0.4461]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[0.0128, 0.7980, 0.1892]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1009, 0.1320, 0.1178, 0.1316, 0.1191, 0.0972, 0.1142, 0.0557, 0.1317]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[9.9795e-01, 2.0189e-03, 2.9965e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1285, 0.1679, 0.1600, 0.1681, 0.1611, 0.0580, 0.1565]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[7.7299e-02, 9.2270e-01, 4.3577e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1237, 0.1298, 0.1670, 0.0786, 0.1676, 0.1659, 0.1674]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[4.5532e-15, 8.1400e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1525, 0.1557, 0.1557, 0.1511, 0.1535, 0.0757, 0.1557]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[1.2048e-03, 5.9833e-04, 9.9820e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2196, 0.1482, 0.2194, 0.1947, 0.2182]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.5734e-01, 4.2665e-02, 1.6648e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3430, 0.3135, 0.3435]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[6.0277e-06, 1.3434e-03, 9.9865e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0760, 0.0924, 0.0428, 0.0470, 0.0702, 0.0807, 0.0926, 0.0909, 0.0920,\n",
            "         0.0448, 0.0933, 0.0859, 0.0914]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[0.0022, 0.0027, 0.9951]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[9.4319e-02, 9.0568e-01, 3.6459e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0870, 0.0916, 0.0423, 0.0899, 0.0645, 0.0931, 0.0750, 0.0574, 0.0737,\n",
            "         0.0521, 0.0931, 0.0931, 0.0873]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[0.1177, 0.1004, 0.7819]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1952, 0.4102, 0.3946]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.7175e-01, 2.8245e-02, 6.3704e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4637, 0.5363]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[9.9993e-01, 6.7502e-05, 3.1422e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2653, 0.3674, 0.3673]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[5.0040e-02, 9.4996e-01, 1.2012e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2935, 0.2676, 0.1381, 0.3007]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.9441e-01, 5.5765e-03, 1.7373e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2982, 0.1830, 0.3461, 0.1727]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[1.0273e-08, 2.2779e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0986, 0.2013, 0.1992, 0.1485, 0.0424, 0.1999, 0.1101]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[5.4072e-08, 9.9975e-01, 2.5202e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1404, 0.2156, 0.2156, 0.2147, 0.2137]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[8.2396e-11, 3.5457e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1090, 0.1577, 0.2005, 0.1962, 0.1986, 0.1379]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[2.0395e-08, 8.7823e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2056, 0.2291, 0.5653]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 1.6217e-06, 1.9023e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1839, 0.1820, 0.1828, 0.0838, 0.1835, 0.1839]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[9.9999e-01, 1.3744e-05, 9.9143e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0338, 0.0712, 0.0893, 0.0852, 0.0903, 0.0895, 0.0903, 0.0903, 0.0903,\n",
            "         0.0903, 0.0899, 0.0896]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[1.3922e-10, 2.5167e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2038, 0.2659, 0.2642, 0.2661]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[9.9999e-01, 5.1433e-06, 6.9588e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1691, 0.1411, 0.2003, 0.2007, 0.2005, 0.0883]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[5.3386e-09, 8.2891e-01, 1.7109e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2539, 0.3848, 0.3612]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[0.0910, 0.6896, 0.2194]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5061, 0.4939]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[4.4086e-14, 3.6050e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1636, 0.1636, 0.0534, 0.1635, 0.1613, 0.1627, 0.1319]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[2.2113e-06, 3.6054e-01, 6.3945e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2662, 0.2459, 0.2361, 0.2518]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[9.0644e-01, 9.3557e-02, 7.9232e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.6008, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[8.1054e-21, 1.7508e-15, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " tensor([[0.2908, 0.3095, 0.3095, 0.0903]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.9981e-01, 1.9080e-04, 2.1827e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4637, 0.5363]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[3.5104e-11, 5.2670e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1324, 0.0976, 0.1485, 0.1561, 0.1548, 0.0881, 0.1561, 0.0662]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[2.7901e-06, 2.6695e-01, 7.3305e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.6552, 0.3448]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[3.2491e-07, 9.9999e-01, 1.1144e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1900, 0.2717, 0.2669, 0.2715]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[9.6430e-06, 9.8826e-01, 1.1734e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0506, 0.0991, 0.1718, 0.1650, 0.1721, 0.1706, 0.1708]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[8.3168e-07, 9.9890e-01, 1.1023e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3336, 0.3296, 0.3367]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[1.0700e-08, 1.6423e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3332, 0.6668]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9845e-01, 1.5466e-03, 1.2111e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1497, 0.1472, 0.1389, 0.1427, 0.1497, 0.1298, 0.1420]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[2.5418e-06, 9.9638e-01, 3.6189e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1457, 0.4239, 0.4303]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[9.9996e-01, 3.6166e-05, 3.5953e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1223, 0.1379, 0.0838, 0.1387, 0.1155, 0.1364, 0.1266, 0.1388]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[0.0045, 0.9401, 0.0554]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0285, 0.0465, 0.0431, 0.0486, 0.0480, 0.0467, 0.0275, 0.0481, 0.0485,\n",
            "         0.0477, 0.0478, 0.0452, 0.0486, 0.0414, 0.0399, 0.0383, 0.0470, 0.0487,\n",
            "         0.0361, 0.0317, 0.0486, 0.0477, 0.0458]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZjY3tH9uMNA6"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n2BIQ2LcMNA7",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1041966e-4775-45ca-ef0b-c90baa0a3e09",
        "id": "nroYU7U1MNBA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = 'model_state_be'+str(model_num)\n",
        "PATH0 = 'model_be'+str(model_num)\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MC3_tUFdMNBI"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b12d9768-35ba-48c4-8dfd-0952c4438208",
        "id": "rOAYtHGeMNBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.74078; \n",
            "F1-Score (Micro): 0.74078; \n",
            "F1-Score (Macro): 0.7380059779498985; \n",
            "F1-Score (Weighted): 0.7393540803169427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IgrCaAXyEEy",
        "colab_type": "text"
      },
      "source": [
        "##FNNs Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "01848fa4-2673-4fc7-969c-487362460151",
        "id": "1bMPqBecyXVr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f84848bc-3f8f-45ee-8ca6-04edbb875821",
        "id": "HfmzZH8kyXWa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = AttentionFNNDropout()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1NMI_dmAyXWo",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8AsmfhOGyXWu",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EaSqshryXW0",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "23172ca8-bc71-4062-933b-5671b9110866",
        "id": "LNMB1Ud_yXW6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape))\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "     \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [10000/257353], Loss: 1.4590, Acc: 0.00\n",
            "Epoch [1/25], Step [20000/257353], Loss: 1.4977, Acc: 0.00\n",
            "Epoch [1/25], Step [30000/257353], Loss: 1.0014, Acc: 0.00\n",
            "Epoch [1/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [50000/257353], Loss: 0.5785, Acc: 1.00\n",
            "Epoch [1/25], Step [60000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [1/25], Step [70000/257353], Loss: 0.9225, Acc: 1.00\n",
            "Epoch [1/25], Step [80000/257353], Loss: 0.7044, Acc: 1.00\n",
            "Epoch [1/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [100000/257353], Loss: 0.5738, Acc: 1.00\n",
            "Epoch [1/25], Step [110000/257353], Loss: 0.5880, Acc: 1.00\n",
            "Epoch [1/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/25], Step [130000/257353], Loss: 1.2140, Acc: 0.00\n",
            "Epoch [1/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [150000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [1/25], Step [160000/257353], Loss: 0.8252, Acc: 1.00\n",
            "Epoch [1/25], Step [170000/257353], Loss: 1.5192, Acc: 0.00\n",
            "Epoch [1/25], Step [180000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [1/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [200000/257353], Loss: 0.5591, Acc: 1.00\n",
            "Epoch [1/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [220000/257353], Loss: 0.6002, Acc: 1.00\n",
            "Epoch [1/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/25], Step [10000/257353], Loss: 1.4822, Acc: 0.00\n",
            "Epoch [2/25], Step [20000/257353], Loss: 0.8057, Acc: 1.00\n",
            "Epoch [2/25], Step [30000/257353], Loss: 1.1871, Acc: 0.00\n",
            "Epoch [2/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [50000/257353], Loss: 0.5631, Acc: 1.00\n",
            "Epoch [2/25], Step [60000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [2/25], Step [70000/257353], Loss: 0.8110, Acc: 1.00\n",
            "Epoch [2/25], Step [80000/257353], Loss: 0.6534, Acc: 1.00\n",
            "Epoch [2/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [100000/257353], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [2/25], Step [110000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [2/25], Step [120000/257353], Loss: 0.5557, Acc: 1.00\n",
            "Epoch [2/25], Step [130000/257353], Loss: 1.2241, Acc: 0.00\n",
            "Epoch [2/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [150000/257353], Loss: 0.5838, Acc: 1.00\n",
            "Epoch [2/25], Step [160000/257353], Loss: 0.5628, Acc: 1.00\n",
            "Epoch [2/25], Step [170000/257353], Loss: 1.5461, Acc: 0.00\n",
            "Epoch [2/25], Step [180000/257353], Loss: 1.5507, Acc: 0.00\n",
            "Epoch [2/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [200000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [2/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [220000/257353], Loss: 0.8028, Acc: 1.00\n",
            "Epoch [2/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/25], Step [10000/257353], Loss: 1.3357, Acc: 0.00\n",
            "Epoch [3/25], Step [20000/257353], Loss: 0.6562, Acc: 1.00\n",
            "Epoch [3/25], Step [30000/257353], Loss: 1.3864, Acc: 0.00\n",
            "Epoch [3/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [50000/257353], Loss: 0.5708, Acc: 1.00\n",
            "Epoch [3/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/25], Step [70000/257353], Loss: 0.6513, Acc: 1.00\n",
            "Epoch [3/25], Step [80000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [3/25], Step [90000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [3/25], Step [100000/257353], Loss: 0.9614, Acc: 0.00\n",
            "Epoch [3/25], Step [110000/257353], Loss: 0.6070, Acc: 1.00\n",
            "Epoch [3/25], Step [120000/257353], Loss: 0.5557, Acc: 1.00\n",
            "Epoch [3/25], Step [130000/257353], Loss: 0.9838, Acc: 0.00\n",
            "Epoch [3/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/25], Step [160000/257353], Loss: 1.3382, Acc: 0.00\n",
            "Epoch [3/25], Step [170000/257353], Loss: 1.5494, Acc: 0.00\n",
            "Epoch [3/25], Step [180000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [3/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [200000/257353], Loss: 1.5241, Acc: 0.00\n",
            "Epoch [3/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [220000/257353], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [3/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/25], Step [10000/257353], Loss: 1.4752, Acc: 0.00\n",
            "Epoch [4/25], Step [20000/257353], Loss: 1.0912, Acc: 0.00\n",
            "Epoch [4/25], Step [30000/257353], Loss: 1.2650, Acc: 0.00\n",
            "Epoch [4/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [50000/257353], Loss: 0.5883, Acc: 1.00\n",
            "Epoch [4/25], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/25], Step [70000/257353], Loss: 1.3862, Acc: 0.00\n",
            "Epoch [4/25], Step [80000/257353], Loss: 0.5619, Acc: 1.00\n",
            "Epoch [4/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [100000/257353], Loss: 0.5660, Acc: 1.00\n",
            "Epoch [4/25], Step [110000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [4/25], Step [120000/257353], Loss: 0.5756, Acc: 1.00\n",
            "Epoch [4/25], Step [130000/257353], Loss: 1.4971, Acc: 0.00\n",
            "Epoch [4/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [150000/257353], Loss: 0.5838, Acc: 1.00\n",
            "Epoch [4/25], Step [160000/257353], Loss: 0.8264, Acc: 1.00\n",
            "Epoch [4/25], Step [170000/257353], Loss: 1.4642, Acc: 0.00\n",
            "Epoch [4/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [200000/257353], Loss: 1.4695, Acc: 0.00\n",
            "Epoch [4/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [220000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [4/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/25], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [5/25], Step [10000/257353], Loss: 1.4590, Acc: 0.00\n",
            "Epoch [5/25], Step [20000/257353], Loss: 1.0209, Acc: 0.00\n",
            "Epoch [5/25], Step [30000/257353], Loss: 0.8179, Acc: 1.00\n",
            "Epoch [5/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [50000/257353], Loss: 0.5600, Acc: 1.00\n",
            "Epoch [5/25], Step [60000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [5/25], Step [70000/257353], Loss: 1.2503, Acc: 0.00\n",
            "Epoch [5/25], Step [80000/257353], Loss: 0.5869, Acc: 1.00\n",
            "Epoch [5/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/25], Step [100000/257353], Loss: 0.6038, Acc: 1.00\n",
            "Epoch [5/25], Step [110000/257353], Loss: 0.7462, Acc: 1.00\n",
            "Epoch [5/25], Step [120000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [5/25], Step [130000/257353], Loss: 1.5338, Acc: 0.00\n",
            "Epoch [5/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [150000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [5/25], Step [160000/257353], Loss: 0.7056, Acc: 1.00\n",
            "Epoch [5/25], Step [170000/257353], Loss: 1.4692, Acc: 0.00\n",
            "Epoch [5/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [200000/257353], Loss: 1.3965, Acc: 0.00\n",
            "Epoch [5/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [220000/257353], Loss: 0.5692, Acc: 1.00\n",
            "Epoch [5/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [10000/257353], Loss: 1.5076, Acc: 0.00\n",
            "Epoch [6/25], Step [20000/257353], Loss: 0.6615, Acc: 1.00\n",
            "Epoch [6/25], Step [30000/257353], Loss: 1.5382, Acc: 0.00\n",
            "Epoch [6/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [50000/257353], Loss: 0.5588, Acc: 1.00\n",
            "Epoch [6/25], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [6/25], Step [70000/257353], Loss: 0.9649, Acc: 0.00\n",
            "Epoch [6/25], Step [80000/257353], Loss: 0.9856, Acc: 0.00\n",
            "Epoch [6/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/25], Step [100000/257353], Loss: 0.5979, Acc: 1.00\n",
            "Epoch [6/25], Step [110000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [6/25], Step [120000/257353], Loss: 0.5793, Acc: 1.00\n",
            "Epoch [6/25], Step [130000/257353], Loss: 1.5234, Acc: 0.00\n",
            "Epoch [6/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/25], Step [160000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [6/25], Step [170000/257353], Loss: 1.5400, Acc: 0.00\n",
            "Epoch [6/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [200000/257353], Loss: 0.9276, Acc: 1.00\n",
            "Epoch [6/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [220000/257353], Loss: 0.5737, Acc: 1.00\n",
            "Epoch [6/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [7/25], Step [10000/257353], Loss: 1.5029, Acc: 0.00\n",
            "Epoch [7/25], Step [20000/257353], Loss: 1.2744, Acc: 0.00\n",
            "Epoch [7/25], Step [30000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [7/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [50000/257353], Loss: 0.5625, Acc: 1.00\n",
            "Epoch [7/25], Step [60000/257353], Loss: 0.5541, Acc: 1.00\n",
            "Epoch [7/25], Step [70000/257353], Loss: 1.3599, Acc: 0.00\n",
            "Epoch [7/25], Step [80000/257353], Loss: 0.5616, Acc: 1.00\n",
            "Epoch [7/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [100000/257353], Loss: 0.9363, Acc: 1.00\n",
            "Epoch [7/25], Step [110000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [7/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/25], Step [130000/257353], Loss: 1.4782, Acc: 0.00\n",
            "Epoch [7/25], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [160000/257353], Loss: 1.4039, Acc: 0.00\n",
            "Epoch [7/25], Step [170000/257353], Loss: 1.4899, Acc: 0.00\n",
            "Epoch [7/25], Step [180000/257353], Loss: 1.5497, Acc: 0.00\n",
            "Epoch [7/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [200000/257353], Loss: 0.7143, Acc: 1.00\n",
            "Epoch [7/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/25], Step [250000/257353], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [8/25], Step [10000/257353], Loss: 1.3105, Acc: 0.00\n",
            "Epoch [8/25], Step [20000/257353], Loss: 0.7240, Acc: 1.00\n",
            "Epoch [8/25], Step [30000/257353], Loss: 0.7875, Acc: 1.00\n",
            "Epoch [8/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [50000/257353], Loss: 0.5733, Acc: 1.00\n",
            "Epoch [8/25], Step [60000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [8/25], Step [70000/257353], Loss: 1.1850, Acc: 0.00\n",
            "Epoch [8/25], Step [80000/257353], Loss: 0.5668, Acc: 1.00\n",
            "Epoch [8/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/25], Step [100000/257353], Loss: 0.6882, Acc: 1.00\n",
            "Epoch [8/25], Step [110000/257353], Loss: 0.5661, Acc: 1.00\n",
            "Epoch [8/25], Step [120000/257353], Loss: 0.6776, Acc: 1.00\n",
            "Epoch [8/25], Step [130000/257353], Loss: 1.2799, Acc: 0.00\n",
            "Epoch [8/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [150000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [8/25], Step [160000/257353], Loss: 1.3003, Acc: 0.00\n",
            "Epoch [8/25], Step [170000/257353], Loss: 1.4579, Acc: 0.00\n",
            "Epoch [8/25], Step [180000/257353], Loss: 1.5501, Acc: 0.00\n",
            "Epoch [8/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [200000/257353], Loss: 0.6271, Acc: 1.00\n",
            "Epoch [8/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [220000/257353], Loss: 0.6016, Acc: 1.00\n",
            "Epoch [8/25], Step [230000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/25], Step [250000/257353], Loss: 0.5617, Acc: 1.00\n",
            "Epoch [9/25], Step [10000/257353], Loss: 1.5461, Acc: 0.00\n",
            "Epoch [9/25], Step [20000/257353], Loss: 1.0007, Acc: 0.00\n",
            "Epoch [9/25], Step [30000/257353], Loss: 1.4632, Acc: 0.00\n",
            "Epoch [9/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [50000/257353], Loss: 0.5675, Acc: 1.00\n",
            "Epoch [9/25], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [9/25], Step [70000/257353], Loss: 1.3775, Acc: 0.00\n",
            "Epoch [9/25], Step [80000/257353], Loss: 0.6903, Acc: 1.00\n",
            "Epoch [9/25], Step [90000/257353], Loss: 0.5558, Acc: 1.00\n",
            "Epoch [9/25], Step [100000/257353], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [9/25], Step [110000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [9/25], Step [120000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [9/25], Step [130000/257353], Loss: 1.0791, Acc: 0.00\n",
            "Epoch [9/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/25], Step [160000/257353], Loss: 0.6122, Acc: 1.00\n",
            "Epoch [9/25], Step [170000/257353], Loss: 1.5432, Acc: 0.00\n",
            "Epoch [9/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [200000/257353], Loss: 0.6789, Acc: 1.00\n",
            "Epoch [9/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [220000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [9/25], Step [230000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/25], Step [250000/257353], Loss: 0.7037, Acc: 1.00\n",
            "Epoch [10/25], Step [10000/257353], Loss: 1.4702, Acc: 0.00\n",
            "Epoch [10/25], Step [20000/257353], Loss: 1.3247, Acc: 0.00\n",
            "Epoch [10/25], Step [30000/257353], Loss: 1.0400, Acc: 0.00\n",
            "Epoch [10/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [50000/257353], Loss: 0.5576, Acc: 1.00\n",
            "Epoch [10/25], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [10/25], Step [70000/257353], Loss: 1.5022, Acc: 0.00\n",
            "Epoch [10/25], Step [80000/257353], Loss: 0.9008, Acc: 1.00\n",
            "Epoch [10/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/25], Step [100000/257353], Loss: 0.6465, Acc: 1.00\n",
            "Epoch [10/25], Step [110000/257353], Loss: 0.5961, Acc: 1.00\n",
            "Epoch [10/25], Step [120000/257353], Loss: 0.5653, Acc: 1.00\n",
            "Epoch [10/25], Step [130000/257353], Loss: 1.2951, Acc: 0.00\n",
            "Epoch [10/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/25], Step [160000/257353], Loss: 0.9189, Acc: 1.00\n",
            "Epoch [10/25], Step [170000/257353], Loss: 1.5445, Acc: 0.00\n",
            "Epoch [10/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [200000/257353], Loss: 0.9201, Acc: 1.00\n",
            "Epoch [10/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [220000/257353], Loss: 0.5952, Acc: 1.00\n",
            "Epoch [10/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [11/25], Step [10000/257353], Loss: 1.5416, Acc: 0.00\n",
            "Epoch [11/25], Step [20000/257353], Loss: 0.9900, Acc: 1.00\n",
            "Epoch [11/25], Step [30000/257353], Loss: 0.5654, Acc: 1.00\n",
            "Epoch [11/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [50000/257353], Loss: 0.5626, Acc: 1.00\n",
            "Epoch [11/25], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [11/25], Step [70000/257353], Loss: 0.9649, Acc: 1.00\n",
            "Epoch [11/25], Step [80000/257353], Loss: 0.6322, Acc: 1.00\n",
            "Epoch [11/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [100000/257353], Loss: 0.5786, Acc: 1.00\n",
            "Epoch [11/25], Step [110000/257353], Loss: 1.3291, Acc: 0.00\n",
            "Epoch [11/25], Step [120000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [11/25], Step [130000/257353], Loss: 1.4809, Acc: 0.00\n",
            "Epoch [11/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [150000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [11/25], Step [160000/257353], Loss: 0.5915, Acc: 1.00\n",
            "Epoch [11/25], Step [170000/257353], Loss: 1.5444, Acc: 0.00\n",
            "Epoch [11/25], Step [180000/257353], Loss: 1.5506, Acc: 0.00\n",
            "Epoch [11/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [200000/257353], Loss: 0.6030, Acc: 1.00\n",
            "Epoch [11/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [220000/257353], Loss: 0.5591, Acc: 1.00\n",
            "Epoch [11/25], Step [230000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [11/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/25], Step [10000/257353], Loss: 1.4701, Acc: 0.00\n",
            "Epoch [12/25], Step [20000/257353], Loss: 0.8221, Acc: 1.00\n",
            "Epoch [12/25], Step [30000/257353], Loss: 1.4834, Acc: 0.00\n",
            "Epoch [12/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [50000/257353], Loss: 0.5624, Acc: 1.00\n",
            "Epoch [12/25], Step [60000/257353], Loss: 0.5538, Acc: 1.00\n",
            "Epoch [12/25], Step [70000/257353], Loss: 0.7653, Acc: 1.00\n",
            "Epoch [12/25], Step [80000/257353], Loss: 0.6004, Acc: 1.00\n",
            "Epoch [12/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [100000/257353], Loss: 0.6179, Acc: 1.00\n",
            "Epoch [12/25], Step [110000/257353], Loss: 1.4525, Acc: 0.00\n",
            "Epoch [12/25], Step [120000/257353], Loss: 0.5991, Acc: 1.00\n",
            "Epoch [12/25], Step [130000/257353], Loss: 0.8621, Acc: 1.00\n",
            "Epoch [12/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [150000/257353], Loss: 0.5541, Acc: 1.00\n",
            "Epoch [12/25], Step [160000/257353], Loss: 0.6096, Acc: 1.00\n",
            "Epoch [12/25], Step [170000/257353], Loss: 1.5219, Acc: 0.00\n",
            "Epoch [12/25], Step [180000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [12/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [200000/257353], Loss: 0.9780, Acc: 0.00\n",
            "Epoch [12/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [220000/257353], Loss: 0.6325, Acc: 1.00\n",
            "Epoch [12/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/25], Step [10000/257353], Loss: 1.4609, Acc: 0.00\n",
            "Epoch [13/25], Step [20000/257353], Loss: 0.9014, Acc: 1.00\n",
            "Epoch [13/25], Step [30000/257353], Loss: 0.8590, Acc: 1.00\n",
            "Epoch [13/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [50000/257353], Loss: 0.5614, Acc: 1.00\n",
            "Epoch [13/25], Step [60000/257353], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [13/25], Step [70000/257353], Loss: 1.5416, Acc: 0.00\n",
            "Epoch [13/25], Step [80000/257353], Loss: 0.7613, Acc: 1.00\n",
            "Epoch [13/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [100000/257353], Loss: 0.6583, Acc: 1.00\n",
            "Epoch [13/25], Step [110000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [13/25], Step [120000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [13/25], Step [130000/257353], Loss: 1.5495, Acc: 0.00\n",
            "Epoch [13/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [150000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [13/25], Step [160000/257353], Loss: 0.7646, Acc: 1.00\n",
            "Epoch [13/25], Step [170000/257353], Loss: 1.4949, Acc: 0.00\n",
            "Epoch [13/25], Step [180000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [13/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [200000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [13/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [220000/257353], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [13/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/25], Step [250000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [14/25], Step [10000/257353], Loss: 1.2152, Acc: 0.00\n",
            "Epoch [14/25], Step [20000/257353], Loss: 0.8125, Acc: 1.00\n",
            "Epoch [14/25], Step [30000/257353], Loss: 0.6585, Acc: 1.00\n",
            "Epoch [14/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [50000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [14/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/25], Step [70000/257353], Loss: 1.0809, Acc: 0.00\n",
            "Epoch [14/25], Step [80000/257353], Loss: 0.8358, Acc: 1.00\n",
            "Epoch [14/25], Step [90000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [14/25], Step [100000/257353], Loss: 0.5945, Acc: 1.00\n",
            "Epoch [14/25], Step [110000/257353], Loss: 0.5855, Acc: 1.00\n",
            "Epoch [14/25], Step [120000/257353], Loss: 0.5965, Acc: 1.00\n",
            "Epoch [14/25], Step [130000/257353], Loss: 1.5455, Acc: 0.00\n",
            "Epoch [14/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [150000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [14/25], Step [160000/257353], Loss: 1.5368, Acc: 0.00\n",
            "Epoch [14/25], Step [170000/257353], Loss: 1.5098, Acc: 0.00\n",
            "Epoch [14/25], Step [180000/257353], Loss: 1.5505, Acc: 0.00\n",
            "Epoch [14/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [200000/257353], Loss: 0.6422, Acc: 1.00\n",
            "Epoch [14/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [220000/257353], Loss: 0.5872, Acc: 1.00\n",
            "Epoch [14/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/25], Step [250000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [15/25], Step [10000/257353], Loss: 1.5168, Acc: 0.00\n",
            "Epoch [15/25], Step [20000/257353], Loss: 0.8295, Acc: 1.00\n",
            "Epoch [15/25], Step [30000/257353], Loss: 1.5347, Acc: 0.00\n",
            "Epoch [15/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [50000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [15/25], Step [60000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [15/25], Step [70000/257353], Loss: 0.9272, Acc: 1.00\n",
            "Epoch [15/25], Step [80000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [15/25], Step [90000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [15/25], Step [100000/257353], Loss: 0.5815, Acc: 1.00\n",
            "Epoch [15/25], Step [110000/257353], Loss: 0.6669, Acc: 1.00\n",
            "Epoch [15/25], Step [120000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [15/25], Step [130000/257353], Loss: 1.5325, Acc: 0.00\n",
            "Epoch [15/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [150000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [15/25], Step [160000/257353], Loss: 0.5857, Acc: 1.00\n",
            "Epoch [15/25], Step [170000/257353], Loss: 1.4617, Acc: 0.00\n",
            "Epoch [15/25], Step [180000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [15/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [200000/257353], Loss: 1.0036, Acc: 0.00\n",
            "Epoch [15/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [220000/257353], Loss: 0.6417, Acc: 1.00\n",
            "Epoch [15/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/25], Step [10000/257353], Loss: 1.5431, Acc: 0.00\n",
            "Epoch [16/25], Step [20000/257353], Loss: 0.9476, Acc: 1.00\n",
            "Epoch [16/25], Step [30000/257353], Loss: 1.5220, Acc: 0.00\n",
            "Epoch [16/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [50000/257353], Loss: 0.5608, Acc: 1.00\n",
            "Epoch [16/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/25], Step [70000/257353], Loss: 1.4151, Acc: 0.00\n",
            "Epoch [16/25], Step [80000/257353], Loss: 0.5767, Acc: 1.00\n",
            "Epoch [16/25], Step [90000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [16/25], Step [100000/257353], Loss: 0.7433, Acc: 1.00\n",
            "Epoch [16/25], Step [110000/257353], Loss: 1.4830, Acc: 0.00\n",
            "Epoch [16/25], Step [120000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [16/25], Step [130000/257353], Loss: 1.5454, Acc: 0.00\n",
            "Epoch [16/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/25], Step [160000/257353], Loss: 0.7667, Acc: 1.00\n",
            "Epoch [16/25], Step [170000/257353], Loss: 1.5378, Acc: 0.00\n",
            "Epoch [16/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [200000/257353], Loss: 1.4838, Acc: 0.00\n",
            "Epoch [16/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [220000/257353], Loss: 0.7363, Acc: 1.00\n",
            "Epoch [16/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/25], Step [250000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [17/25], Step [10000/257353], Loss: 1.4707, Acc: 0.00\n",
            "Epoch [17/25], Step [20000/257353], Loss: 0.8701, Acc: 1.00\n",
            "Epoch [17/25], Step [30000/257353], Loss: 0.5998, Acc: 1.00\n",
            "Epoch [17/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [50000/257353], Loss: 0.6436, Acc: 1.00\n",
            "Epoch [17/25], Step [60000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [17/25], Step [70000/257353], Loss: 0.9412, Acc: 1.00\n",
            "Epoch [17/25], Step [80000/257353], Loss: 0.6188, Acc: 1.00\n",
            "Epoch [17/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [100000/257353], Loss: 0.5699, Acc: 1.00\n",
            "Epoch [17/25], Step [110000/257353], Loss: 1.0647, Acc: 0.00\n",
            "Epoch [17/25], Step [120000/257353], Loss: 0.5579, Acc: 1.00\n",
            "Epoch [17/25], Step [130000/257353], Loss: 1.5480, Acc: 0.00\n",
            "Epoch [17/25], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [160000/257353], Loss: 0.6094, Acc: 1.00\n",
            "Epoch [17/25], Step [170000/257353], Loss: 1.5272, Acc: 0.00\n",
            "Epoch [17/25], Step [180000/257353], Loss: 1.5484, Acc: 0.00\n",
            "Epoch [17/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [200000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [17/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [220000/257353], Loss: 0.5587, Acc: 1.00\n",
            "Epoch [17/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/25], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [10000/257353], Loss: 1.5453, Acc: 0.00\n",
            "Epoch [18/25], Step [20000/257353], Loss: 0.8459, Acc: 1.00\n",
            "Epoch [18/25], Step [30000/257353], Loss: 0.8791, Acc: 1.00\n",
            "Epoch [18/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [50000/257353], Loss: 0.5561, Acc: 1.00\n",
            "Epoch [18/25], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [18/25], Step [70000/257353], Loss: 0.6018, Acc: 1.00\n",
            "Epoch [18/25], Step [80000/257353], Loss: 0.5771, Acc: 1.00\n",
            "Epoch [18/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [100000/257353], Loss: 0.5797, Acc: 1.00\n",
            "Epoch [18/25], Step [110000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [18/25], Step [120000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [18/25], Step [130000/257353], Loss: 1.5405, Acc: 0.00\n",
            "Epoch [18/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/25], Step [160000/257353], Loss: 0.7583, Acc: 1.00\n",
            "Epoch [18/25], Step [170000/257353], Loss: 1.5429, Acc: 0.00\n",
            "Epoch [18/25], Step [180000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [18/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [200000/257353], Loss: 1.5383, Acc: 0.00\n",
            "Epoch [18/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [220000/257353], Loss: 0.7525, Acc: 1.00\n",
            "Epoch [18/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/25], Step [10000/257353], Loss: 1.5049, Acc: 0.00\n",
            "Epoch [19/25], Step [20000/257353], Loss: 0.6930, Acc: 1.00\n",
            "Epoch [19/25], Step [30000/257353], Loss: 0.9348, Acc: 1.00\n",
            "Epoch [19/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [50000/257353], Loss: 0.5606, Acc: 1.00\n",
            "Epoch [19/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/25], Step [70000/257353], Loss: 1.1632, Acc: 0.00\n",
            "Epoch [19/25], Step [80000/257353], Loss: 0.6362, Acc: 1.00\n",
            "Epoch [19/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/25], Step [100000/257353], Loss: 0.6434, Acc: 1.00\n",
            "Epoch [19/25], Step [110000/257353], Loss: 0.7850, Acc: 1.00\n",
            "Epoch [19/25], Step [120000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [19/25], Step [130000/257353], Loss: 1.4737, Acc: 0.00\n",
            "Epoch [19/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [150000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [19/25], Step [160000/257353], Loss: 1.5371, Acc: 0.00\n",
            "Epoch [19/25], Step [170000/257353], Loss: 1.4850, Acc: 0.00\n",
            "Epoch [19/25], Step [180000/257353], Loss: 1.5485, Acc: 0.00\n",
            "Epoch [19/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [200000/257353], Loss: 0.9402, Acc: 1.00\n",
            "Epoch [19/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [220000/257353], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [19/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/25], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [10000/257353], Loss: 1.5270, Acc: 0.00\n",
            "Epoch [20/25], Step [20000/257353], Loss: 1.4992, Acc: 0.00\n",
            "Epoch [20/25], Step [30000/257353], Loss: 1.5493, Acc: 0.00\n",
            "Epoch [20/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [50000/257353], Loss: 0.5719, Acc: 1.00\n",
            "Epoch [20/25], Step [60000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [20/25], Step [70000/257353], Loss: 0.9320, Acc: 1.00\n",
            "Epoch [20/25], Step [80000/257353], Loss: 0.5934, Acc: 1.00\n",
            "Epoch [20/25], Step [90000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [100000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [20/25], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [130000/257353], Loss: 1.3920, Acc: 0.00\n",
            "Epoch [20/25], Step [140000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [160000/257353], Loss: 0.9139, Acc: 1.00\n",
            "Epoch [20/25], Step [170000/257353], Loss: 1.4806, Acc: 0.00\n",
            "Epoch [20/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [200000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [20/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [220000/257353], Loss: 0.5687, Acc: 1.00\n",
            "Epoch [20/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/25], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [21/25], Step [10000/257353], Loss: 1.5321, Acc: 0.00\n",
            "Epoch [21/25], Step [20000/257353], Loss: 0.6222, Acc: 1.00\n",
            "Epoch [21/25], Step [30000/257353], Loss: 1.1968, Acc: 0.00\n",
            "Epoch [21/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [50000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [21/25], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [70000/257353], Loss: 1.2361, Acc: 0.00\n",
            "Epoch [21/25], Step [80000/257353], Loss: 0.5691, Acc: 1.00\n",
            "Epoch [21/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [100000/257353], Loss: 0.6074, Acc: 1.00\n",
            "Epoch [21/25], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/25], Step [120000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [21/25], Step [130000/257353], Loss: 1.4354, Acc: 0.00\n",
            "Epoch [21/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [150000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [21/25], Step [160000/257353], Loss: 0.7375, Acc: 1.00\n",
            "Epoch [21/25], Step [170000/257353], Loss: 1.5436, Acc: 0.00\n",
            "Epoch [21/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [21/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [200000/257353], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [21/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [220000/257353], Loss: 0.5907, Acc: 1.00\n",
            "Epoch [21/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/25], Step [240000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [21/25], Step [250000/257353], Loss: 0.6308, Acc: 1.00\n",
            "Epoch [22/25], Step [10000/257353], Loss: 1.4409, Acc: 0.00\n",
            "Epoch [22/25], Step [20000/257353], Loss: 1.1795, Acc: 0.00\n",
            "Epoch [22/25], Step [30000/257353], Loss: 1.3144, Acc: 0.00\n",
            "Epoch [22/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [50000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [22/25], Step [60000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [22/25], Step [70000/257353], Loss: 1.4711, Acc: 0.00\n",
            "Epoch [22/25], Step [80000/257353], Loss: 0.5610, Acc: 1.00\n",
            "Epoch [22/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [100000/257353], Loss: 0.6730, Acc: 1.00\n",
            "Epoch [22/25], Step [110000/257353], Loss: 0.6381, Acc: 1.00\n",
            "Epoch [22/25], Step [120000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [22/25], Step [130000/257353], Loss: 1.5118, Acc: 0.00\n",
            "Epoch [22/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [160000/257353], Loss: 1.0937, Acc: 0.00\n",
            "Epoch [22/25], Step [170000/257353], Loss: 1.4646, Acc: 0.00\n",
            "Epoch [22/25], Step [180000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [22/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [200000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [22/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [220000/257353], Loss: 0.5597, Acc: 1.00\n",
            "Epoch [22/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/25], Step [250000/257353], Loss: 0.5711, Acc: 1.00\n",
            "Epoch [23/25], Step [10000/257353], Loss: 1.4689, Acc: 0.00\n",
            "Epoch [23/25], Step [20000/257353], Loss: 0.8664, Acc: 1.00\n",
            "Epoch [23/25], Step [30000/257353], Loss: 1.4194, Acc: 0.00\n",
            "Epoch [23/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [50000/257353], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [23/25], Step [60000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [23/25], Step [70000/257353], Loss: 1.4225, Acc: 0.00\n",
            "Epoch [23/25], Step [80000/257353], Loss: 0.5612, Acc: 1.00\n",
            "Epoch [23/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [100000/257353], Loss: 0.5844, Acc: 1.00\n",
            "Epoch [23/25], Step [110000/257353], Loss: 0.5774, Acc: 1.00\n",
            "Epoch [23/25], Step [120000/257353], Loss: 0.6040, Acc: 1.00\n",
            "Epoch [23/25], Step [130000/257353], Loss: 0.9512, Acc: 1.00\n",
            "Epoch [23/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [23/25], Step [160000/257353], Loss: 0.5842, Acc: 1.00\n",
            "Epoch [23/25], Step [170000/257353], Loss: 1.5421, Acc: 0.00\n",
            "Epoch [23/25], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [200000/257353], Loss: 0.6499, Acc: 1.00\n",
            "Epoch [23/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [220000/257353], Loss: 0.5658, Acc: 1.00\n",
            "Epoch [23/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/25], Step [250000/257353], Loss: 0.5613, Acc: 1.00\n",
            "Epoch [24/25], Step [10000/257353], Loss: 1.5372, Acc: 0.00\n",
            "Epoch [24/25], Step [20000/257353], Loss: 0.6786, Acc: 1.00\n",
            "Epoch [24/25], Step [30000/257353], Loss: 0.9177, Acc: 1.00\n",
            "Epoch [24/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [50000/257353], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [24/25], Step [60000/257353], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [24/25], Step [70000/257353], Loss: 1.1763, Acc: 0.00\n",
            "Epoch [24/25], Step [80000/257353], Loss: 1.4258, Acc: 0.00\n",
            "Epoch [24/25], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [100000/257353], Loss: 0.7240, Acc: 1.00\n",
            "Epoch [24/25], Step [110000/257353], Loss: 0.5625, Acc: 1.00\n",
            "Epoch [24/25], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [130000/257353], Loss: 1.2249, Acc: 0.00\n",
            "Epoch [24/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [160000/257353], Loss: 0.7335, Acc: 1.00\n",
            "Epoch [24/25], Step [170000/257353], Loss: 1.5062, Acc: 0.00\n",
            "Epoch [24/25], Step [180000/257353], Loss: 1.5466, Acc: 0.00\n",
            "Epoch [24/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [200000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [24/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [220000/257353], Loss: 0.5921, Acc: 1.00\n",
            "Epoch [24/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/25], Step [250000/257353], Loss: 0.5890, Acc: 1.00\n",
            "Epoch [25/25], Step [10000/257353], Loss: 1.4595, Acc: 0.00\n",
            "Epoch [25/25], Step [20000/257353], Loss: 0.7597, Acc: 1.00\n",
            "Epoch [25/25], Step [30000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [25/25], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [50000/257353], Loss: 0.5683, Acc: 1.00\n",
            "Epoch [25/25], Step [60000/257353], Loss: 0.5557, Acc: 1.00\n",
            "Epoch [25/25], Step [70000/257353], Loss: 1.4070, Acc: 0.00\n",
            "Epoch [25/25], Step [80000/257353], Loss: 0.7246, Acc: 1.00\n",
            "Epoch [25/25], Step [90000/257353], Loss: 0.5731, Acc: 1.00\n",
            "Epoch [25/25], Step [100000/257353], Loss: 0.5818, Acc: 1.00\n",
            "Epoch [25/25], Step [110000/257353], Loss: 1.3528, Acc: 0.00\n",
            "Epoch [25/25], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [25/25], Step [130000/257353], Loss: 1.5491, Acc: 0.00\n",
            "Epoch [25/25], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [25/25], Step [160000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [25/25], Step [170000/257353], Loss: 1.5480, Acc: 0.00\n",
            "Epoch [25/25], Step [180000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [25/25], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [200000/257353], Loss: 1.1566, Acc: 0.00\n",
            "Epoch [25/25], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [220000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [25/25], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/25], Step [250000/257353], Loss: 0.5616, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UT_xZYuYyXXC"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d107fb90-f04c-4a8c-a42c-6542314202d1",
        "id": "Ok1zKV0gyXXE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "70.39999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR5ldWGdyXXN"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "21909f70-1f50-49e0-d223-a4826c581640",
        "id": "jo9tHZZSyXXP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape))\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        " \n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.9934e-01, 6.5722e-04, 5.4343e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2101, 0.7899]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[2.1187e-05, 9.9970e-01, 2.7887e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1988, 0.1586, 0.1988, 0.1988, 0.1924, 0.0526]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[4.3420e-01, 5.6580e-01, 3.9207e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3341, 0.3341, 0.3318]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[2.6412e-11, 7.4072e-06, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1130, 0.1190, 0.1139, 0.0314, 0.1190, 0.1178, 0.1190, 0.0891, 0.1190,\n",
            "         0.0587]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[0.0998, 0.8899, 0.0103]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2776, 0.1669, 0.2641, 0.2913]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[1.1318e-02, 9.8868e-01, 3.9244e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4184, 0.5816]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[1.1239e-06, 9.2769e-02, 9.0723e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2074, 0.2078, 0.1924, 0.2078, 0.1846]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[6.7392e-01, 3.2608e-01, 5.8827e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1462, 0.1468, 0.1464, 0.1216, 0.1468, 0.1468, 0.1455]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[7.1426e-09, 1.4081e-04, 9.9986e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0591, 0.0961, 0.0959, 0.0817, 0.0944, 0.0965, 0.0919, 0.0411, 0.0965,\n",
            "         0.0955, 0.0744, 0.0769]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[9.8132e-01, 1.8677e-02, 7.3702e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1171, 0.2018, 0.0783, 0.1981, 0.2021, 0.2027]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[3.4261e-18, 1.6274e-13, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3379, 0.3241, 0.3380]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[0.1552, 0.8436, 0.0012]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1345, 0.1345, 0.1350, 0.1352, 0.1013, 0.0793, 0.0308, 0.1234, 0.1261]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[9.8097e-01, 1.8948e-02, 8.4001e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1188, 0.1828, 0.1727, 0.1828, 0.1828, 0.1025, 0.0576]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[2.7685e-02, 9.7231e-01, 2.8186e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1400, 0.1514, 0.1679, 0.0375, 0.1680, 0.1675, 0.1676]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[4.3922e-10, 1.8941e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1462, 0.1463, 0.1464, 0.1384, 0.1403, 0.1450, 0.1374]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[0.8647, 0.1335, 0.0018]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2291, 0.1246, 0.2291, 0.1977, 0.2195]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.5644e-01, 4.3556e-02, 1.9558e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4270, 0.1459, 0.4270]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[3.1922e-04, 4.6220e-02, 9.5346e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0664, 0.0857, 0.0583, 0.0763, 0.0683, 0.0781, 0.0900, 0.0908, 0.0771,\n",
            "         0.0820, 0.0909, 0.0820, 0.0542]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[4.5540e-04, 5.0292e-03, 9.9452e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[8.0155e-02, 9.1982e-01, 2.4077e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0833, 0.0870, 0.0615, 0.0870, 0.0690, 0.0869, 0.0627, 0.0734, 0.0868,\n",
            "         0.0540, 0.0870, 0.0861, 0.0753]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[0.0988, 0.8996, 0.0016]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3278, 0.3366, 0.3356]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.7996e-01, 2.0035e-02, 1.1897e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2351, 0.7649]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[9.9999e-01, 5.5022e-06, 3.9273e-31]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1842, 0.4079, 0.4079]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[1.1656e-01, 8.8344e-01, 8.1551e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2858, 0.2769, 0.1575, 0.2798]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.9908e-01, 9.1530e-04, 7.7849e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3169, 0.3148, 0.3010, 0.0673]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[5.7988e-14, 2.4337e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1314, 0.1884, 0.1884, 0.1757, 0.0639, 0.1884, 0.0639]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[3.7283e-04, 9.9961e-01, 1.7331e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1994, 0.2079, 0.2070, 0.1784, 0.2073]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[1.4371e-09, 4.6513e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1378, 0.1941, 0.1965, 0.1959, 0.1953, 0.0805]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[3.8868e-05, 1.4193e-01, 8.5803e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3548, 0.2092, 0.4360]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[9.9999e-01, 6.0436e-06, 4.1892e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2132, 0.2130, 0.2127, 0.0555, 0.0923, 0.2132]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[9.9785e-01, 2.1466e-03, 6.5581e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0751, 0.0814, 0.0746, 0.0849, 0.0855, 0.0855, 0.0856, 0.0855, 0.0856,\n",
            "         0.0856, 0.0854, 0.0855]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[1.0161e-09, 2.2900e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1447, 0.2855, 0.2835, 0.2863]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 1.2605e-06, 6.3879e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1732, 0.1739, 0.1738, 0.1739, 0.1739, 0.1314]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[5.2625e-04, 9.9915e-01, 3.2066e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2880, 0.3714, 0.3406]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[3.6014e-11, 1.8266e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4589, 0.5411]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[5.4967e-11, 1.0399e-05, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1641, 0.1641, 0.0387, 0.1633, 0.1638, 0.1578, 0.1482]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[4.5993e-06, 9.9639e-01, 3.6054e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3076, 0.3075, 0.0787, 0.3062]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[9.9792e-01, 2.0803e-03, 2.6365e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5589, 0.4411]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[3.9692e-19, 1.0604e-13, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2039, 0.3582, 0.3582, 0.0797]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.9903e-01, 9.7307e-04, 3.7898e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5097, 0.4903]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[4.7481e-11, 2.9149e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1360, 0.1349, 0.1340, 0.1367, 0.1366, 0.1289, 0.1367, 0.0562]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[9.2757e-07, 2.0231e-01, 7.9769e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.8250, 0.1750]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[2.0406e-03, 9.9796e-01, 7.7984e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2513, 0.2519, 0.2489, 0.2479]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[1.1594e-03, 9.9873e-01, 1.0738e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0834, 0.0944, 0.1653, 0.1653, 0.1653, 0.1650, 0.1613]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[2.4436e-02, 9.7556e-01, 2.0629e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3509, 0.2975, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[1.5000e-05, 3.7327e-04, 9.9961e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4232, 0.5768]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.2965e-01, 7.0346e-02, 2.3485e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1467, 0.1466, 0.1262, 0.1462, 0.1467, 0.1429, 0.1447]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[1.3727e-01, 8.6263e-01, 9.9843e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1638, 0.4565, 0.3796]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[9.9969e-01, 3.0670e-04, 4.2710e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1272, 0.1270, 0.1265, 0.1272, 0.1263, 0.1272, 0.1116, 0.1269]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[0.0144, 0.9830, 0.0026]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0183, 0.0358, 0.0293, 0.0510, 0.0419, 0.0345, 0.0425, 0.0510, 0.0504,\n",
            "         0.0484, 0.0511, 0.0471, 0.0469, 0.0511, 0.0499, 0.0293, 0.0454, 0.0510,\n",
            "         0.0405, 0.0383, 0.0511, 0.0502, 0.0450]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qx65rC9byXXY"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "db3a2cb3-ed7d-462f-ec1b-42388bdd928d",
        "id": "H2fUSb3iyXXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "model_num = 0\n",
        "PATH = 'modelFNNDropout_state25epoch_'+str(model_num)\n",
        "PATH0 = 'model_Dropout_FNN_25epoch'+str(model_num)\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AttentionFNNDropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3tKRwRInyXXj"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9bb2d824-1b5b-48fb-8878-de2d13718048",
        "id": "sfy9tfwmyXXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.704; \n",
            "F1-Score (Micro): 0.704; \n",
            "F1-Score (Macro): 0.7025013749328766; \n",
            "F1-Score (Weighted): 0.704093438859345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-dkGxYypyQlz"
      },
      "source": [
        "##MIL CNNs 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f54e0266-79d0-4aab-e633-2155cd055990",
        "id": "cu0o3s2CyQmH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d749dbd9-40a9-494e-802d-4f41b32fa8ff",
        "id": "GiK3Ree6yQme",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Attention()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3oXB1w2MyQmp",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WiZLzzcuyQmv",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tz5LnF2myQm0",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2UCcTQIEE1Y",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint feature to be able to resume Training from a certain checkpoint. Since training for 25 epochs on Colab's GPU took more than 12 hours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39bSXzerzkbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_chk = './checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "umRAKJ63yQm7"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf9RyFI7g-2X",
        "colab_type": "code",
        "outputId": "4ff197eb-7a04-49b6-9e79-789d1fb669cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 20, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(10, 50, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=25, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7ed7ae7e-ab9c-4ca3-b8e8-c5f90f28092b",
        "id": "y6bBGTH5yQm9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emb_val, lab_val) # Validation after each epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [10000/257353], Loss: 1.5507, Acc: 0.00\n",
            "Epoch [1/50], Step [20000/257353], Loss: 0.7622, Acc: 1.00\n",
            "Epoch [1/50], Step [30000/257353], Loss: 1.4583, Acc: 0.00\n",
            "Epoch [1/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [50000/257353], Loss: 0.5658, Acc: 1.00\n",
            "Epoch [1/50], Step [60000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [1/50], Step [70000/257353], Loss: 1.2660, Acc: 0.00\n",
            "Epoch [1/50], Step [80000/257353], Loss: 0.5580, Acc: 1.00\n",
            "Epoch [1/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [100000/257353], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [1/50], Step [110000/257353], Loss: 0.5630, Acc: 1.00\n",
            "Epoch [1/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/50], Step [130000/257353], Loss: 1.4219, Acc: 0.00\n",
            "Epoch [1/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [160000/257353], Loss: 0.8457, Acc: 1.00\n",
            "Epoch [1/50], Step [170000/257353], Loss: 1.4774, Acc: 0.00\n",
            "Epoch [1/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [200000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [1/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [220000/257353], Loss: 0.5542, Acc: 1.00\n",
            "Epoch [1/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  72.33800000000001\n",
            "Epoch [2/50], Step [10000/257353], Loss: 1.4983, Acc: 0.00\n",
            "Epoch [2/50], Step [20000/257353], Loss: 1.3266, Acc: 0.00\n",
            "Epoch [2/50], Step [30000/257353], Loss: 1.5390, Acc: 0.00\n",
            "Epoch [2/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [50000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [2/50], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [2/50], Step [70000/257353], Loss: 1.4831, Acc: 0.00\n",
            "Epoch [2/50], Step [80000/257353], Loss: 0.5584, Acc: 1.00\n",
            "Epoch [2/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [2/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [130000/257353], Loss: 1.4770, Acc: 0.00\n",
            "Epoch [2/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [160000/257353], Loss: 0.8841, Acc: 1.00\n",
            "Epoch [2/50], Step [170000/257353], Loss: 1.5491, Acc: 0.00\n",
            "Epoch [2/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [200000/257353], Loss: 1.2317, Acc: 0.00\n",
            "Epoch [2/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [220000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [2/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/50], Step [250000/257353], Loss: 0.5588, Acc: 1.00\n",
            "Validation Accuracy :  71.244\n",
            "Epoch [3/50], Step [10000/257353], Loss: 1.4729, Acc: 0.00\n",
            "Epoch [3/50], Step [20000/257353], Loss: 1.2602, Acc: 0.00\n",
            "Epoch [3/50], Step [30000/257353], Loss: 0.5551, Acc: 1.00\n",
            "Epoch [3/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [50000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [3/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [3/50], Step [70000/257353], Loss: 1.3916, Acc: 0.00\n",
            "Epoch [3/50], Step [80000/257353], Loss: 0.5597, Acc: 1.00\n",
            "Epoch [3/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [100000/257353], Loss: 0.5576, Acc: 1.00\n",
            "Epoch [3/50], Step [110000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [3/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/50], Step [130000/257353], Loss: 1.5490, Acc: 0.00\n",
            "Epoch [3/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [160000/257353], Loss: 0.8174, Acc: 1.00\n",
            "Epoch [3/50], Step [170000/257353], Loss: 1.5130, Acc: 0.00\n",
            "Epoch [3/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [200000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [3/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [220000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [3/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [250000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Validation Accuracy :  71.468\n",
            "Epoch [4/50], Step [10000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/50], Step [20000/257353], Loss: 0.7377, Acc: 1.00\n",
            "Epoch [4/50], Step [30000/257353], Loss: 1.0234, Acc: 0.00\n",
            "Epoch [4/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [50000/257353], Loss: 0.5604, Acc: 1.00\n",
            "Epoch [4/50], Step [60000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [4/50], Step [70000/257353], Loss: 1.3761, Acc: 0.00\n",
            "Epoch [4/50], Step [80000/257353], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [4/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [100000/257353], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [4/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [4/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [130000/257353], Loss: 1.5355, Acc: 0.00\n",
            "Epoch [4/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [160000/257353], Loss: 1.4953, Acc: 0.00\n",
            "Epoch [4/50], Step [170000/257353], Loss: 1.5453, Acc: 0.00\n",
            "Epoch [4/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [200000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [4/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [220000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [4/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/50], Step [250000/257353], Loss: 0.6373, Acc: 1.00\n",
            "Validation Accuracy :  73.92800000000001\n",
            "Epoch [5/50], Step [10000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [5/50], Step [20000/257353], Loss: 1.5167, Acc: 0.00\n",
            "Epoch [5/50], Step [30000/257353], Loss: 1.2589, Acc: 0.00\n",
            "Epoch [5/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/50], Step [70000/257353], Loss: 1.5439, Acc: 0.00\n",
            "Epoch [5/50], Step [80000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [5/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [100000/257353], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [5/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [130000/257353], Loss: 1.5423, Acc: 0.00\n",
            "Epoch [5/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [160000/257353], Loss: 0.8170, Acc: 1.00\n",
            "Epoch [5/50], Step [170000/257353], Loss: 1.5459, Acc: 0.00\n",
            "Epoch [5/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [200000/257353], Loss: 1.1185, Acc: 0.00\n",
            "Epoch [5/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [220000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [5/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [250000/257353], Loss: 0.5551, Acc: 1.00\n",
            "Validation Accuracy :  74.22\n",
            "Epoch [6/50], Step [10000/257353], Loss: 1.5163, Acc: 0.00\n",
            "Epoch [6/50], Step [20000/257353], Loss: 1.1310, Acc: 0.00\n",
            "Epoch [6/50], Step [30000/257353], Loss: 1.5308, Acc: 0.00\n",
            "Epoch [6/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [6/50], Step [70000/257353], Loss: 1.4727, Acc: 0.00\n",
            "Epoch [6/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [100000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [6/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [130000/257353], Loss: 1.5487, Acc: 0.00\n",
            "Epoch [6/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [160000/257353], Loss: 0.5555, Acc: 1.00\n",
            "Epoch [6/50], Step [170000/257353], Loss: 1.5377, Acc: 0.00\n",
            "Epoch [6/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [200000/257353], Loss: 0.8348, Acc: 1.00\n",
            "Epoch [6/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [220000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [6/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [250000/257353], Loss: 0.5924, Acc: 1.00\n",
            "Validation Accuracy :  68.268\n",
            "Epoch [7/50], Step [10000/257353], Loss: 1.5382, Acc: 0.00\n",
            "Epoch [7/50], Step [20000/257353], Loss: 1.0655, Acc: 0.00\n",
            "Epoch [7/50], Step [30000/257353], Loss: 1.0462, Acc: 0.00\n",
            "Epoch [7/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [50000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [7/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/50], Step [70000/257353], Loss: 1.1333, Acc: 0.00\n",
            "Epoch [7/50], Step [80000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [7/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [100000/257353], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [7/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [130000/257353], Loss: 1.3330, Acc: 0.00\n",
            "Epoch [7/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [160000/257353], Loss: 0.9936, Acc: 0.00\n",
            "Epoch [7/50], Step [170000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [200000/257353], Loss: 1.5456, Acc: 0.00\n",
            "Epoch [7/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [220000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [7/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/50], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Validation Accuracy :  74.476\n",
            "Epoch [8/50], Step [10000/257353], Loss: 1.5417, Acc: 0.00\n",
            "Epoch [8/50], Step [20000/257353], Loss: 0.9863, Acc: 0.00\n",
            "Epoch [8/50], Step [30000/257353], Loss: 0.5739, Acc: 1.00\n",
            "Epoch [8/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/50], Step [70000/257353], Loss: 0.9043, Acc: 1.00\n",
            "Epoch [8/50], Step [80000/257353], Loss: 0.5668, Acc: 1.00\n",
            "Epoch [8/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [100000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [8/50], Step [110000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [8/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [130000/257353], Loss: 0.9157, Acc: 1.00\n",
            "Epoch [8/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [160000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [8/50], Step [170000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [200000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [8/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [220000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [8/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  68.984\n",
            "Epoch [9/50], Step [10000/257353], Loss: 1.4909, Acc: 0.00\n",
            "Epoch [9/50], Step [20000/257353], Loss: 1.3640, Acc: 0.00\n",
            "Epoch [9/50], Step [30000/257353], Loss: 1.3379, Acc: 0.00\n",
            "Epoch [9/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/50], Step [70000/257353], Loss: 1.4824, Acc: 0.00\n",
            "Epoch [9/50], Step [80000/257353], Loss: 0.7625, Acc: 1.00\n",
            "Epoch [9/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [130000/257353], Loss: 1.2896, Acc: 0.00\n",
            "Epoch [9/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [160000/257353], Loss: 0.9938, Acc: 0.00\n",
            "Epoch [9/50], Step [170000/257353], Loss: 1.5391, Acc: 0.00\n",
            "Epoch [9/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [200000/257353], Loss: 1.4896, Acc: 0.00\n",
            "Epoch [9/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [220000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [9/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [250000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Validation Accuracy :  74.90400000000001\n",
            "Epoch [10/50], Step [10000/257353], Loss: 1.5471, Acc: 0.00\n",
            "Epoch [10/50], Step [20000/257353], Loss: 0.8977, Acc: 1.00\n",
            "Epoch [10/50], Step [30000/257353], Loss: 0.5541, Acc: 1.00\n",
            "Epoch [10/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [10/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/50], Step [70000/257353], Loss: 1.3533, Acc: 0.00\n",
            "Epoch [10/50], Step [80000/257353], Loss: 0.5613, Acc: 1.00\n",
            "Epoch [10/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [100000/257353], Loss: 0.5538, Acc: 1.00\n",
            "Epoch [10/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/50], Step [120000/257353], Loss: 0.5558, Acc: 1.00\n",
            "Epoch [10/50], Step [130000/257353], Loss: 1.5420, Acc: 0.00\n",
            "Epoch [10/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [160000/257353], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [10/50], Step [170000/257353], Loss: 1.5404, Acc: 0.00\n",
            "Epoch [10/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [200000/257353], Loss: 1.5009, Acc: 0.00\n",
            "Epoch [10/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [220000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [10/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  73.138\n",
            "Epoch [11/50], Step [10000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [11/50], Step [20000/257353], Loss: 0.8505, Acc: 1.00\n",
            "Epoch [11/50], Step [30000/257353], Loss: 0.6457, Acc: 1.00\n",
            "Epoch [11/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [50000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [11/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/50], Step [70000/257353], Loss: 1.5380, Acc: 0.00\n",
            "Epoch [11/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [11/50], Step [110000/257353], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [11/50], Step [120000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [11/50], Step [130000/257353], Loss: 1.5148, Acc: 0.00\n",
            "Epoch [11/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [160000/257353], Loss: 1.5374, Acc: 0.00\n",
            "Epoch [11/50], Step [170000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [11/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [200000/257353], Loss: 1.3039, Acc: 0.00\n",
            "Epoch [11/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [250000/257353], Loss: 0.5571, Acc: 1.00\n",
            "Validation Accuracy :  72.89\n",
            "Epoch [12/50], Step [10000/257353], Loss: 1.5397, Acc: 0.00\n",
            "Epoch [12/50], Step [20000/257353], Loss: 1.5026, Acc: 0.00\n",
            "Epoch [12/50], Step [30000/257353], Loss: 0.5630, Acc: 1.00\n",
            "Epoch [12/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [12/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [12/50], Step [70000/257353], Loss: 1.5434, Acc: 0.00\n",
            "Epoch [12/50], Step [80000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [12/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [100000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [12/50], Step [110000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [12/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [130000/257353], Loss: 0.8018, Acc: 1.00\n",
            "Epoch [12/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [160000/257353], Loss: 0.8866, Acc: 1.00\n",
            "Epoch [12/50], Step [170000/257353], Loss: 1.5428, Acc: 0.00\n",
            "Epoch [12/50], Step [180000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [12/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [200000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [12/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [220000/257353], Loss: 0.5699, Acc: 1.00\n",
            "Epoch [12/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/50], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.22\n",
            "Epoch [13/50], Step [10000/257353], Loss: 1.5228, Acc: 0.00\n",
            "Epoch [13/50], Step [20000/257353], Loss: 1.3218, Acc: 0.00\n",
            "Epoch [13/50], Step [30000/257353], Loss: 0.7863, Acc: 1.00\n",
            "Epoch [13/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/50], Step [60000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [13/50], Step [70000/257353], Loss: 1.4790, Acc: 0.00\n",
            "Epoch [13/50], Step [80000/257353], Loss: 0.9487, Acc: 1.00\n",
            "Epoch [13/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [100000/257353], Loss: 0.5544, Acc: 1.00\n",
            "Epoch [13/50], Step [110000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [13/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [130000/257353], Loss: 0.6942, Acc: 1.00\n",
            "Epoch [13/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [160000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [13/50], Step [170000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [200000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [13/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [220000/257353], Loss: 0.5752, Acc: 1.00\n",
            "Epoch [13/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/50], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Validation Accuracy :  73.81\n",
            "Epoch [14/50], Step [10000/257353], Loss: 1.4581, Acc: 0.00\n",
            "Epoch [14/50], Step [20000/257353], Loss: 1.4630, Acc: 0.00\n",
            "Epoch [14/50], Step [30000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [14/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [50000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [14/50], Step [60000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [14/50], Step [70000/257353], Loss: 1.2307, Acc: 0.00\n",
            "Epoch [14/50], Step [80000/257353], Loss: 0.5596, Acc: 1.00\n",
            "Epoch [14/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [130000/257353], Loss: 0.6541, Acc: 1.00\n",
            "Epoch [14/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [160000/257353], Loss: 1.2852, Acc: 0.00\n",
            "Epoch [14/50], Step [170000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [14/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [200000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [14/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [220000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [14/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Validation Accuracy :  73.788\n",
            "Epoch [15/50], Step [10000/257353], Loss: 1.5480, Acc: 0.00\n",
            "Epoch [15/50], Step [20000/257353], Loss: 0.8610, Acc: 1.00\n",
            "Epoch [15/50], Step [30000/257353], Loss: 1.0424, Acc: 0.00\n",
            "Epoch [15/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/50], Step [70000/257353], Loss: 1.1263, Acc: 0.00\n",
            "Epoch [15/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [100000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [15/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [130000/257353], Loss: 0.5609, Acc: 1.00\n",
            "Epoch [15/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [160000/257353], Loss: 0.7239, Acc: 1.00\n",
            "Epoch [15/50], Step [170000/257353], Loss: 1.5424, Acc: 0.00\n",
            "Epoch [15/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [200000/257353], Loss: 0.5859, Acc: 1.00\n",
            "Epoch [15/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [15/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [250000/257353], Loss: 0.7271, Acc: 1.00\n",
            "Validation Accuracy :  73.004\n",
            "Epoch [16/50], Step [10000/257353], Loss: 1.5461, Acc: 0.00\n",
            "Epoch [16/50], Step [20000/257353], Loss: 0.8164, Acc: 1.00\n",
            "Epoch [16/50], Step [30000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [16/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [50000/257353], Loss: 0.7999, Acc: 1.00\n",
            "Epoch [16/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/50], Step [70000/257353], Loss: 1.1748, Acc: 0.00\n",
            "Epoch [16/50], Step [80000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [16/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [100000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [16/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [130000/257353], Loss: 0.8199, Acc: 1.00\n",
            "Epoch [16/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [160000/257353], Loss: 0.6552, Acc: 1.00\n",
            "Epoch [16/50], Step [170000/257353], Loss: 1.4824, Acc: 0.00\n",
            "Epoch [16/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [200000/257353], Loss: 1.4878, Acc: 0.00\n",
            "Epoch [16/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [220000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [16/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/50], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Validation Accuracy :  75.118\n",
            "Epoch [17/50], Step [10000/257353], Loss: 1.5079, Acc: 0.00\n",
            "Epoch [17/50], Step [20000/257353], Loss: 1.4919, Acc: 0.00\n",
            "Epoch [17/50], Step [30000/257353], Loss: 1.1395, Acc: 0.00\n",
            "Epoch [17/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [17/50], Step [60000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [17/50], Step [70000/257353], Loss: 0.7077, Acc: 1.00\n",
            "Epoch [17/50], Step [80000/257353], Loss: 0.5581, Acc: 1.00\n",
            "Epoch [17/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/50], Step [110000/257353], Loss: 0.5588, Acc: 1.00\n",
            "Epoch [17/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [130000/257353], Loss: 0.5878, Acc: 1.00\n",
            "Epoch [17/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/50], Step [160000/257353], Loss: 0.5911, Acc: 1.00\n",
            "Epoch [17/50], Step [170000/257353], Loss: 1.5495, Acc: 0.00\n",
            "Epoch [17/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [200000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [17/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [17/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  70.94\n",
            "Epoch [18/50], Step [10000/257353], Loss: 1.5488, Acc: 0.00\n",
            "Epoch [18/50], Step [20000/257353], Loss: 1.4655, Acc: 0.00\n",
            "Epoch [18/50], Step [30000/257353], Loss: 1.1268, Acc: 0.00\n",
            "Epoch [18/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [70000/257353], Loss: 1.1088, Acc: 0.00\n",
            "Epoch [18/50], Step [80000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [18/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [130000/257353], Loss: 0.5896, Acc: 1.00\n",
            "Epoch [18/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [150000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [160000/257353], Loss: 1.3372, Acc: 0.00\n",
            "Epoch [18/50], Step [170000/257353], Loss: 1.5379, Acc: 0.00\n",
            "Epoch [18/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [18/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Validation Accuracy :  72.066\n",
            "Epoch [19/50], Step [10000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/50], Step [20000/257353], Loss: 1.5362, Acc: 0.00\n",
            "Epoch [19/50], Step [30000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [19/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/50], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [70000/257353], Loss: 1.0337, Acc: 1.00\n",
            "Epoch [19/50], Step [80000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [19/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [100000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [110000/257353], Loss: 0.5670, Acc: 1.00\n",
            "Epoch [19/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [130000/257353], Loss: 1.5133, Acc: 0.00\n",
            "Epoch [19/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [160000/257353], Loss: 0.5643, Acc: 1.00\n",
            "Epoch [19/50], Step [170000/257353], Loss: 1.4834, Acc: 0.00\n",
            "Epoch [19/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [200000/257353], Loss: 0.8948, Acc: 1.00\n",
            "Epoch [19/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [250000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Validation Accuracy :  71.28\n",
            "Epoch [20/50], Step [10000/257353], Loss: 1.5496, Acc: 0.00\n",
            "Epoch [20/50], Step [20000/257353], Loss: 1.5196, Acc: 0.00\n",
            "Epoch [20/50], Step [30000/257353], Loss: 1.5469, Acc: 0.00\n",
            "Epoch [20/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [20/50], Step [70000/257353], Loss: 1.2074, Acc: 0.00\n",
            "Epoch [20/50], Step [80000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [20/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [100000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [20/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [130000/257353], Loss: 1.1699, Acc: 0.00\n",
            "Epoch [20/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [160000/257353], Loss: 0.5679, Acc: 1.00\n",
            "Epoch [20/50], Step [170000/257353], Loss: 1.5300, Acc: 0.00\n",
            "Epoch [20/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [220000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [20/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [250000/257353], Loss: 0.5714, Acc: 1.00\n",
            "Validation Accuracy :  74.91600000000001\n",
            "Epoch [21/50], Step [10000/257353], Loss: 1.5283, Acc: 0.00\n",
            "Epoch [21/50], Step [20000/257353], Loss: 1.5363, Acc: 0.00\n",
            "Epoch [21/50], Step [30000/257353], Loss: 0.5541, Acc: 1.00\n",
            "Epoch [21/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [21/50], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [21/50], Step [70000/257353], Loss: 1.5446, Acc: 0.00\n",
            "Epoch [21/50], Step [80000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [21/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [21/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [130000/257353], Loss: 0.7045, Acc: 1.00\n",
            "Epoch [21/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [160000/257353], Loss: 1.2225, Acc: 0.00\n",
            "Epoch [21/50], Step [170000/257353], Loss: 1.4766, Acc: 0.00\n",
            "Epoch [21/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [21/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [200000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [21/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [220000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [21/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [250000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Validation Accuracy :  69.89999999999999\n",
            "Epoch [22/50], Step [10000/257353], Loss: 1.5499, Acc: 0.00\n",
            "Epoch [22/50], Step [20000/257353], Loss: 1.5042, Acc: 0.00\n",
            "Epoch [22/50], Step [30000/257353], Loss: 0.6489, Acc: 1.00\n",
            "Epoch [22/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [22/50], Step [60000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [22/50], Step [70000/257353], Loss: 0.8206, Acc: 1.00\n",
            "Epoch [22/50], Step [80000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [22/50], Step [110000/257353], Loss: 0.5595, Acc: 1.00\n",
            "Epoch [22/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [130000/257353], Loss: 0.5539, Acc: 1.00\n",
            "Epoch [22/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [160000/257353], Loss: 0.5962, Acc: 1.00\n",
            "Epoch [22/50], Step [170000/257353], Loss: 1.5498, Acc: 0.00\n",
            "Epoch [22/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [22/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [200000/257353], Loss: 0.7374, Acc: 1.00\n",
            "Epoch [22/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [250000/257353], Loss: 0.5594, Acc: 1.00\n",
            "Validation Accuracy :  74.07000000000001\n",
            "Epoch [23/50], Step [10000/257353], Loss: 1.5191, Acc: 0.00\n",
            "Epoch [23/50], Step [20000/257353], Loss: 1.3420, Acc: 0.00\n",
            "Epoch [23/50], Step [30000/257353], Loss: 1.5503, Acc: 0.00\n",
            "Epoch [23/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [23/50], Step [60000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [23/50], Step [70000/257353], Loss: 0.8474, Acc: 1.00\n",
            "Epoch [23/50], Step [80000/257353], Loss: 0.5597, Acc: 1.00\n",
            "Epoch [23/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [100000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [23/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [130000/257353], Loss: 1.2252, Acc: 0.00\n",
            "Epoch [23/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [160000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/50], Step [170000/257353], Loss: 1.5462, Acc: 0.00\n",
            "Epoch [23/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [200000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [23/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [23/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [250000/257353], Loss: 0.8569, Acc: 1.00\n",
            "Validation Accuracy :  73.784\n",
            "Epoch [24/50], Step [10000/257353], Loss: 1.5470, Acc: 0.00\n",
            "Epoch [24/50], Step [20000/257353], Loss: 0.9989, Acc: 0.00\n",
            "Epoch [24/50], Step [30000/257353], Loss: 0.6052, Acc: 1.00\n",
            "Epoch [24/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [24/50], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [24/50], Step [70000/257353], Loss: 1.5449, Acc: 0.00\n",
            "Epoch [24/50], Step [80000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [24/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [100000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [24/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [130000/257353], Loss: 0.5625, Acc: 1.00\n",
            "Epoch [24/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [160000/257353], Loss: 0.8068, Acc: 1.00\n",
            "Epoch [24/50], Step [170000/257353], Loss: 1.5497, Acc: 0.00\n",
            "Epoch [24/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [24/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [200000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [24/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [24/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Validation Accuracy :  71.746\n",
            "Epoch [25/50], Step [10000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [25/50], Step [20000/257353], Loss: 1.5409, Acc: 0.00\n",
            "Epoch [25/50], Step [30000/257353], Loss: 1.2882, Acc: 0.00\n",
            "Epoch [25/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [50000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [25/50], Step [60000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [25/50], Step [70000/257353], Loss: 1.4899, Acc: 0.00\n",
            "Epoch [25/50], Step [80000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [25/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [25/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [25/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [25/50], Step [130000/257353], Loss: 0.6472, Acc: 1.00\n",
            "Epoch [25/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [160000/257353], Loss: 0.5714, Acc: 1.00\n",
            "Epoch [25/50], Step [170000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [25/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [25/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [200000/257353], Loss: 0.5642, Acc: 1.00\n",
            "Epoch [25/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.768\n",
            "Epoch [26/50], Step [10000/257353], Loss: 1.5486, Acc: 0.00\n",
            "Epoch [26/50], Step [20000/257353], Loss: 1.0249, Acc: 0.00\n",
            "Epoch [26/50], Step [30000/257353], Loss: 0.9153, Acc: 1.00\n",
            "Epoch [26/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [26/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [26/50], Step [70000/257353], Loss: 0.9375, Acc: 1.00\n",
            "Epoch [26/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [26/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [26/50], Step [110000/257353], Loss: 0.6851, Acc: 1.00\n",
            "Epoch [26/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [130000/257353], Loss: 0.5751, Acc: 1.00\n",
            "Epoch [26/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [160000/257353], Loss: 0.6783, Acc: 1.00\n",
            "Epoch [26/50], Step [170000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [26/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [26/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [200000/257353], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [26/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [26/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [26/50], Step [250000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Validation Accuracy :  71.76599999999999\n",
            "Epoch [27/50], Step [10000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [27/50], Step [20000/257353], Loss: 0.6144, Acc: 1.00\n",
            "Epoch [27/50], Step [30000/257353], Loss: 0.6485, Acc: 1.00\n",
            "Epoch [27/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [27/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [27/50], Step [70000/257353], Loss: 1.4305, Acc: 0.00\n",
            "Epoch [27/50], Step [80000/257353], Loss: 0.6647, Acc: 1.00\n",
            "Epoch [27/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [100000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [27/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [27/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [27/50], Step [130000/257353], Loss: 0.5863, Acc: 1.00\n",
            "Epoch [27/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [160000/257353], Loss: 0.6753, Acc: 1.00\n",
            "Epoch [27/50], Step [170000/257353], Loss: 1.4587, Acc: 0.00\n",
            "Epoch [27/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [27/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [200000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [27/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [220000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [27/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [250000/257353], Loss: 0.7252, Acc: 1.00\n",
            "Validation Accuracy :  72.832\n",
            "Epoch [28/50], Step [10000/257353], Loss: 1.5385, Acc: 0.00\n",
            "Epoch [28/50], Step [20000/257353], Loss: 1.0091, Acc: 0.00\n",
            "Epoch [28/50], Step [30000/257353], Loss: 1.5418, Acc: 0.00\n",
            "Epoch [28/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [28/50], Step [60000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [28/50], Step [70000/257353], Loss: 0.6746, Acc: 1.00\n",
            "Epoch [28/50], Step [80000/257353], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [28/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [28/50], Step [110000/257353], Loss: 0.5546, Acc: 1.00\n",
            "Epoch [28/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [28/50], Step [130000/257353], Loss: 0.5657, Acc: 1.00\n",
            "Epoch [28/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [160000/257353], Loss: 0.6810, Acc: 1.00\n",
            "Epoch [28/50], Step [170000/257353], Loss: 1.5499, Acc: 0.00\n",
            "Epoch [28/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [28/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [200000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [28/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [28/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [250000/257353], Loss: 0.5573, Acc: 1.00\n",
            "Validation Accuracy :  71.95\n",
            "Epoch [29/50], Step [10000/257353], Loss: 1.5505, Acc: 0.00\n",
            "Epoch [29/50], Step [20000/257353], Loss: 0.5542, Acc: 1.00\n",
            "Epoch [29/50], Step [30000/257353], Loss: 1.5386, Acc: 0.00\n",
            "Epoch [29/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [29/50], Step [60000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [29/50], Step [70000/257353], Loss: 1.0403, Acc: 0.00\n",
            "Epoch [29/50], Step [80000/257353], Loss: 0.5632, Acc: 1.00\n",
            "Epoch [29/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [100000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [29/50], Step [110000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [29/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [130000/257353], Loss: 0.5861, Acc: 1.00\n",
            "Epoch [29/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [160000/257353], Loss: 0.6292, Acc: 1.00\n",
            "Epoch [29/50], Step [170000/257353], Loss: 1.5434, Acc: 0.00\n",
            "Epoch [29/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [29/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [200000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [29/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [29/50], Step [250000/257353], Loss: 0.5923, Acc: 1.00\n",
            "Validation Accuracy :  71.236\n",
            "Epoch [30/50], Step [10000/257353], Loss: 1.4961, Acc: 0.00\n",
            "Epoch [30/50], Step [20000/257353], Loss: 0.6602, Acc: 1.00\n",
            "Epoch [30/50], Step [30000/257353], Loss: 1.4695, Acc: 0.00\n",
            "Epoch [30/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [50000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [30/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [30/50], Step [70000/257353], Loss: 1.5462, Acc: 0.00\n",
            "Epoch [30/50], Step [80000/257353], Loss: 0.5619, Acc: 1.00\n",
            "Epoch [30/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [30/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [130000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [30/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [160000/257353], Loss: 0.6588, Acc: 1.00\n",
            "Epoch [30/50], Step [170000/257353], Loss: 1.5493, Acc: 0.00\n",
            "Epoch [30/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [30/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [200000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [30/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [30/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [30/50], Step [250000/257353], Loss: 0.6544, Acc: 1.00\n",
            "Validation Accuracy :  69.718\n",
            "Epoch [31/50], Step [10000/257353], Loss: 1.4874, Acc: 0.00\n",
            "Epoch [31/50], Step [20000/257353], Loss: 0.8168, Acc: 1.00\n",
            "Epoch [31/50], Step [30000/257353], Loss: 1.5425, Acc: 0.00\n",
            "Epoch [31/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [50000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [31/50], Step [60000/257353], Loss: 0.5590, Acc: 1.00\n",
            "Epoch [31/50], Step [70000/257353], Loss: 0.7605, Acc: 1.00\n",
            "Epoch [31/50], Step [80000/257353], Loss: 0.5894, Acc: 1.00\n",
            "Epoch [31/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [100000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [31/50], Step [130000/257353], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [31/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [160000/257353], Loss: 0.5889, Acc: 1.00\n",
            "Epoch [31/50], Step [170000/257353], Loss: 1.5454, Acc: 0.00\n",
            "Epoch [31/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [31/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [200000/257353], Loss: 0.5562, Acc: 1.00\n",
            "Epoch [31/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [220000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [31/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [250000/257353], Loss: 0.5646, Acc: 1.00\n",
            "Validation Accuracy :  74.636\n",
            "Epoch [32/50], Step [10000/257353], Loss: 1.5369, Acc: 0.00\n",
            "Epoch [32/50], Step [20000/257353], Loss: 0.5589, Acc: 1.00\n",
            "Epoch [32/50], Step [30000/257353], Loss: 1.5372, Acc: 0.00\n",
            "Epoch [32/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [32/50], Step [60000/257353], Loss: 0.5561, Acc: 1.00\n",
            "Epoch [32/50], Step [70000/257353], Loss: 0.8648, Acc: 1.00\n",
            "Epoch [32/50], Step [80000/257353], Loss: 0.6024, Acc: 1.00\n",
            "Epoch [32/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [100000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [32/50], Step [110000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [32/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [130000/257353], Loss: 0.9004, Acc: 1.00\n",
            "Epoch [32/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [160000/257353], Loss: 1.3309, Acc: 0.00\n",
            "Epoch [32/50], Step [170000/257353], Loss: 1.5492, Acc: 0.00\n",
            "Epoch [32/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [32/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [200000/257353], Loss: 0.5572, Acc: 1.00\n",
            "Epoch [32/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [32/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [250000/257353], Loss: 0.7284, Acc: 1.00\n",
            "Validation Accuracy :  72.658\n",
            "Epoch [33/50], Step [10000/257353], Loss: 1.5473, Acc: 0.00\n",
            "Epoch [33/50], Step [20000/257353], Loss: 1.1231, Acc: 0.00\n",
            "Epoch [33/50], Step [30000/257353], Loss: 0.8591, Acc: 1.00\n",
            "Epoch [33/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [50000/257353], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [33/50], Step [60000/257353], Loss: 0.5700, Acc: 1.00\n",
            "Epoch [33/50], Step [70000/257353], Loss: 1.4626, Acc: 0.00\n",
            "Epoch [33/50], Step [80000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [33/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [100000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [33/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [130000/257353], Loss: 0.6355, Acc: 1.00\n",
            "Epoch [33/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [160000/257353], Loss: 1.1350, Acc: 0.00\n",
            "Epoch [33/50], Step [170000/257353], Loss: 1.5370, Acc: 0.00\n",
            "Epoch [33/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [33/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [33/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [33/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [250000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Validation Accuracy :  73.378\n",
            "Epoch [34/50], Step [10000/257353], Loss: 1.5361, Acc: 0.00\n",
            "Epoch [34/50], Step [20000/257353], Loss: 0.5764, Acc: 1.00\n",
            "Epoch [34/50], Step [30000/257353], Loss: 1.5391, Acc: 0.00\n",
            "Epoch [34/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [34/50], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [34/50], Step [70000/257353], Loss: 1.0387, Acc: 0.00\n",
            "Epoch [34/50], Step [80000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [34/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [34/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [34/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [130000/257353], Loss: 0.8191, Acc: 1.00\n",
            "Epoch [34/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [160000/257353], Loss: 0.6993, Acc: 1.00\n",
            "Epoch [34/50], Step [170000/257353], Loss: 1.5114, Acc: 0.00\n",
            "Epoch [34/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [34/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [200000/257353], Loss: 0.6048, Acc: 1.00\n",
            "Epoch [34/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [34/50], Step [250000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Validation Accuracy :  70.49\n",
            "Epoch [35/50], Step [10000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [35/50], Step [20000/257353], Loss: 0.6233, Acc: 1.00\n",
            "Epoch [35/50], Step [30000/257353], Loss: 0.6462, Acc: 1.00\n",
            "Epoch [35/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [50000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [35/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [35/50], Step [70000/257353], Loss: 1.3504, Acc: 0.00\n",
            "Epoch [35/50], Step [80000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [35/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [100000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [35/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [35/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [35/50], Step [130000/257353], Loss: 1.1482, Acc: 0.00\n",
            "Epoch [35/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [160000/257353], Loss: 1.0236, Acc: 0.00\n",
            "Epoch [35/50], Step [170000/257353], Loss: 1.5181, Acc: 0.00\n",
            "Epoch [35/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [35/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [200000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [35/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [35/50], Step [250000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Validation Accuracy :  69.71000000000001\n",
            "Epoch [36/50], Step [10000/257353], Loss: 1.5283, Acc: 0.00\n",
            "Epoch [36/50], Step [20000/257353], Loss: 0.6449, Acc: 1.00\n",
            "Epoch [36/50], Step [30000/257353], Loss: 1.5453, Acc: 0.00\n",
            "Epoch [36/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [50000/257353], Loss: 0.5553, Acc: 1.00\n",
            "Epoch [36/50], Step [60000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [36/50], Step [70000/257353], Loss: 1.0448, Acc: 0.00\n",
            "Epoch [36/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [36/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [100000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [36/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [36/50], Step [130000/257353], Loss: 0.6332, Acc: 1.00\n",
            "Epoch [36/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [160000/257353], Loss: 0.5651, Acc: 1.00\n",
            "Epoch [36/50], Step [170000/257353], Loss: 1.5474, Acc: 0.00\n",
            "Epoch [36/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [36/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [200000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [36/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [36/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [250000/257353], Loss: 0.6864, Acc: 1.00\n",
            "Validation Accuracy :  70.306\n",
            "Epoch [37/50], Step [10000/257353], Loss: 1.5487, Acc: 0.00\n",
            "Epoch [37/50], Step [20000/257353], Loss: 0.5991, Acc: 1.00\n",
            "Epoch [37/50], Step [30000/257353], Loss: 0.6521, Acc: 1.00\n",
            "Epoch [37/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [50000/257353], Loss: 0.5738, Acc: 1.00\n",
            "Epoch [37/50], Step [60000/257353], Loss: 0.5584, Acc: 1.00\n",
            "Epoch [37/50], Step [70000/257353], Loss: 1.5272, Acc: 0.00\n",
            "Epoch [37/50], Step [80000/257353], Loss: 0.5923, Acc: 1.00\n",
            "Epoch [37/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [37/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [37/50], Step [130000/257353], Loss: 0.5807, Acc: 1.00\n",
            "Epoch [37/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [160000/257353], Loss: 0.7989, Acc: 1.00\n",
            "Epoch [37/50], Step [170000/257353], Loss: 1.5432, Acc: 0.00\n",
            "Epoch [37/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [37/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [200000/257353], Loss: 0.5713, Acc: 1.00\n",
            "Epoch [37/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [250000/257353], Loss: 0.5901, Acc: 1.00\n",
            "Validation Accuracy :  67.5\n",
            "Epoch [38/50], Step [10000/257353], Loss: 1.5450, Acc: 0.00\n",
            "Epoch [38/50], Step [20000/257353], Loss: 0.6460, Acc: 1.00\n",
            "Epoch [38/50], Step [30000/257353], Loss: 0.5551, Acc: 1.00\n",
            "Epoch [38/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [50000/257353], Loss: 0.5610, Acc: 1.00\n",
            "Epoch [38/50], Step [60000/257353], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [38/50], Step [70000/257353], Loss: 1.0588, Acc: 0.00\n",
            "Epoch [38/50], Step [80000/257353], Loss: 0.5999, Acc: 1.00\n",
            "Epoch [38/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [100000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [120000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [38/50], Step [130000/257353], Loss: 0.6457, Acc: 1.00\n",
            "Epoch [38/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [160000/257353], Loss: 0.5871, Acc: 1.00\n",
            "Epoch [38/50], Step [170000/257353], Loss: 1.5498, Acc: 0.00\n",
            "Epoch [38/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [38/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [200000/257353], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [38/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [38/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [250000/257353], Loss: 0.6668, Acc: 1.00\n",
            "Validation Accuracy :  71.60600000000001\n",
            "Epoch [39/50], Step [10000/257353], Loss: 1.5424, Acc: 0.00\n",
            "Epoch [39/50], Step [20000/257353], Loss: 1.0723, Acc: 0.00\n",
            "Epoch [39/50], Step [30000/257353], Loss: 1.2390, Acc: 0.00\n",
            "Epoch [39/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [50000/257353], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [39/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [39/50], Step [70000/257353], Loss: 1.3663, Acc: 0.00\n",
            "Epoch [39/50], Step [80000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [39/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [100000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [39/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [39/50], Step [130000/257353], Loss: 0.5812, Acc: 1.00\n",
            "Epoch [39/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [160000/257353], Loss: 0.6199, Acc: 1.00\n",
            "Epoch [39/50], Step [170000/257353], Loss: 1.5415, Acc: 0.00\n",
            "Epoch [39/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [39/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [200000/257353], Loss: 0.5586, Acc: 1.00\n",
            "Epoch [39/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [39/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [250000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Validation Accuracy :  70.94\n",
            "Epoch [40/50], Step [10000/257353], Loss: 1.5497, Acc: 0.00\n",
            "Epoch [40/50], Step [20000/257353], Loss: 1.2771, Acc: 0.00\n",
            "Epoch [40/50], Step [30000/257353], Loss: 0.8275, Acc: 1.00\n",
            "Epoch [40/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [40/50], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [40/50], Step [70000/257353], Loss: 1.1093, Acc: 0.00\n",
            "Epoch [40/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [40/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [40/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [130000/257353], Loss: 0.8930, Acc: 1.00\n",
            "Epoch [40/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [160000/257353], Loss: 0.6506, Acc: 1.00\n",
            "Epoch [40/50], Step [170000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [40/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [40/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [200000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [40/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [250000/257353], Loss: 0.6801, Acc: 1.00\n",
            "Validation Accuracy :  73.61\n",
            "Epoch [41/50], Step [10000/257353], Loss: 1.4738, Acc: 0.00\n",
            "Epoch [41/50], Step [20000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [41/50], Step [30000/257353], Loss: 1.3268, Acc: 0.00\n",
            "Epoch [41/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [50000/257353], Loss: 0.5590, Acc: 1.00\n",
            "Epoch [41/50], Step [60000/257353], Loss: 0.5579, Acc: 1.00\n",
            "Epoch [41/50], Step [70000/257353], Loss: 1.4191, Acc: 0.00\n",
            "Epoch [41/50], Step [80000/257353], Loss: 0.7303, Acc: 1.00\n",
            "Epoch [41/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [100000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [41/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [41/50], Step [120000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [41/50], Step [130000/257353], Loss: 0.5775, Acc: 1.00\n",
            "Epoch [41/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [160000/257353], Loss: 1.4072, Acc: 0.00\n",
            "Epoch [41/50], Step [170000/257353], Loss: 1.5472, Acc: 0.00\n",
            "Epoch [41/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [41/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [200000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [41/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [220000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [41/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [250000/257353], Loss: 0.5614, Acc: 1.00\n",
            "Validation Accuracy :  72.7\n",
            "Epoch [42/50], Step [10000/257353], Loss: 1.4754, Acc: 0.00\n",
            "Epoch [42/50], Step [20000/257353], Loss: 0.5691, Acc: 1.00\n",
            "Epoch [42/50], Step [30000/257353], Loss: 1.5482, Acc: 0.00\n",
            "Epoch [42/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [50000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [42/50], Step [60000/257353], Loss: 0.5753, Acc: 1.00\n",
            "Epoch [42/50], Step [70000/257353], Loss: 1.4588, Acc: 0.00\n",
            "Epoch [42/50], Step [80000/257353], Loss: 0.6802, Acc: 1.00\n",
            "Epoch [42/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [100000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [130000/257353], Loss: 0.5553, Acc: 1.00\n",
            "Epoch [42/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [160000/257353], Loss: 0.6166, Acc: 1.00\n",
            "Epoch [42/50], Step [170000/257353], Loss: 1.5505, Acc: 0.00\n",
            "Epoch [42/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [42/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [200000/257353], Loss: 0.8236, Acc: 1.00\n",
            "Epoch [42/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [220000/257353], Loss: 0.5553, Acc: 1.00\n",
            "Epoch [42/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [250000/257353], Loss: 0.5664, Acc: 1.00\n",
            "Validation Accuracy :  73.788\n",
            "Epoch [43/50], Step [10000/257353], Loss: 1.5503, Acc: 0.00\n",
            "Epoch [43/50], Step [20000/257353], Loss: 0.5572, Acc: 1.00\n",
            "Epoch [43/50], Step [30000/257353], Loss: 1.5008, Acc: 0.00\n",
            "Epoch [43/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [50000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [43/50], Step [60000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [43/50], Step [70000/257353], Loss: 1.2541, Acc: 0.00\n",
            "Epoch [43/50], Step [80000/257353], Loss: 1.5416, Acc: 0.00\n",
            "Epoch [43/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [100000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [43/50], Step [110000/257353], Loss: 0.5566, Acc: 1.00\n",
            "Epoch [43/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [130000/257353], Loss: 0.5673, Acc: 1.00\n",
            "Epoch [43/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [160000/257353], Loss: 0.5670, Acc: 1.00\n",
            "Epoch [43/50], Step [170000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [43/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [43/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [200000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [43/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  74.016\n",
            "Epoch [44/50], Step [10000/257353], Loss: 1.5373, Acc: 0.00\n",
            "Epoch [44/50], Step [20000/257353], Loss: 0.5604, Acc: 1.00\n",
            "Epoch [44/50], Step [30000/257353], Loss: 1.2227, Acc: 0.00\n",
            "Epoch [44/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [50000/257353], Loss: 0.5766, Acc: 1.00\n",
            "Epoch [44/50], Step [60000/257353], Loss: 0.5960, Acc: 1.00\n",
            "Epoch [44/50], Step [70000/257353], Loss: 0.9668, Acc: 1.00\n",
            "Epoch [44/50], Step [80000/257353], Loss: 0.5654, Acc: 1.00\n",
            "Epoch [44/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [100000/257353], Loss: 0.5558, Acc: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkwrPSxoDv5V",
        "colab_type": "text"
      },
      "source": [
        "#####Resume Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGQ4k6HEaYh",
        "colab_type": "text"
      },
      "source": [
        "Restore state dictionaries to be able to resume. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCj6O_t7XZmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load('checkpoint43.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_chkpoint = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IHFWYP0Dzrj",
        "colab_type": "code",
        "outputId": "295f77e4-6909-4e33-e1c1-c2b3083d0659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epoch_chkpoint, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emb_val, lab_val) # Validation after each epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [43/50], Step [10000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [43/50], Step [20000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [43/50], Step [30000/257353], Loss: 0.9474, Acc: 1.00\n",
            "Epoch [43/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [50000/257353], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [43/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [43/50], Step [70000/257353], Loss: 1.5191, Acc: 0.00\n",
            "Epoch [43/50], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [43/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [100000/257353], Loss: 0.6179, Acc: 1.00\n",
            "Epoch [43/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [130000/257353], Loss: 0.7747, Acc: 1.00\n",
            "Epoch [43/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [160000/257353], Loss: 0.6611, Acc: 1.00\n",
            "Epoch [43/50], Step [170000/257353], Loss: 1.5380, Acc: 0.00\n",
            "Epoch [43/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [43/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [200000/257353], Loss: 0.7086, Acc: 1.00\n",
            "Epoch [43/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [220000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [43/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [43/50], Step [250000/257353], Loss: 0.5519, Acc: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  70.25\n",
            "Epoch [44/50], Step [10000/257353], Loss: 1.5498, Acc: 0.00\n",
            "Epoch [44/50], Step [20000/257353], Loss: 0.5653, Acc: 1.00\n",
            "Epoch [44/50], Step [30000/257353], Loss: 0.8900, Acc: 1.00\n",
            "Epoch [44/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [50000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [44/50], Step [60000/257353], Loss: 0.5592, Acc: 1.00\n",
            "Epoch [44/50], Step [70000/257353], Loss: 0.9073, Acc: 1.00\n",
            "Epoch [44/50], Step [80000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [44/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [44/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [44/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [130000/257353], Loss: 0.5631, Acc: 1.00\n",
            "Epoch [44/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [160000/257353], Loss: 0.7817, Acc: 1.00\n",
            "Epoch [44/50], Step [170000/257353], Loss: 1.5461, Acc: 0.00\n",
            "Epoch [44/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [44/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [200000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [44/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [44/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [44/50], Step [250000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Validation Accuracy :  69.77600000000001\n",
            "Epoch [45/50], Step [10000/257353], Loss: 1.5345, Acc: 0.00\n",
            "Epoch [45/50], Step [20000/257353], Loss: 0.6331, Acc: 1.00\n",
            "Epoch [45/50], Step [30000/257353], Loss: 0.8910, Acc: 1.00\n",
            "Epoch [45/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [50000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [45/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [45/50], Step [70000/257353], Loss: 1.4822, Acc: 0.00\n",
            "Epoch [45/50], Step [80000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [45/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [100000/257353], Loss: 0.6759, Acc: 1.00\n",
            "Epoch [45/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [45/50], Step [130000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [45/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [160000/257353], Loss: 0.7462, Acc: 1.00\n",
            "Epoch [45/50], Step [170000/257353], Loss: 1.5488, Acc: 0.00\n",
            "Epoch [45/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [45/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [200000/257353], Loss: 0.7544, Acc: 1.00\n",
            "Epoch [45/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Validation Accuracy :  69.606\n",
            "Epoch [46/50], Step [10000/257353], Loss: 1.5389, Acc: 0.00\n",
            "Epoch [46/50], Step [20000/257353], Loss: 0.8696, Acc: 1.00\n",
            "Epoch [46/50], Step [30000/257353], Loss: 0.6295, Acc: 1.00\n",
            "Epoch [46/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [46/50], Step [60000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [46/50], Step [70000/257353], Loss: 1.3468, Acc: 0.00\n",
            "Epoch [46/50], Step [80000/257353], Loss: 0.5558, Acc: 1.00\n",
            "Epoch [46/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [100000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [46/50], Step [110000/257353], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [46/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [130000/257353], Loss: 0.5598, Acc: 1.00\n",
            "Epoch [46/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [160000/257353], Loss: 0.8769, Acc: 1.00\n",
            "Epoch [46/50], Step [170000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [46/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [46/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [200000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [46/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [46/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [46/50], Step [250000/257353], Loss: 0.5810, Acc: 1.00\n",
            "Validation Accuracy :  70.69800000000001\n",
            "Epoch [47/50], Step [10000/257353], Loss: 1.4834, Acc: 0.00\n",
            "Epoch [47/50], Step [20000/257353], Loss: 0.9174, Acc: 1.00\n",
            "Epoch [47/50], Step [30000/257353], Loss: 0.5579, Acc: 1.00\n",
            "Epoch [47/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [50000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [47/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [47/50], Step [70000/257353], Loss: 0.9651, Acc: 1.00\n",
            "Epoch [47/50], Step [80000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [47/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [47/50], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [47/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [130000/257353], Loss: 0.5870, Acc: 1.00\n",
            "Epoch [47/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [160000/257353], Loss: 0.6281, Acc: 1.00\n",
            "Epoch [47/50], Step [170000/257353], Loss: 1.4812, Acc: 0.00\n",
            "Epoch [47/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [47/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [200000/257353], Loss: 0.6340, Acc: 1.00\n",
            "Epoch [47/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [220000/257353], Loss: 0.5680, Acc: 1.00\n",
            "Epoch [47/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [47/50], Step [250000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Validation Accuracy :  74.812\n",
            "Epoch [48/50], Step [10000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [48/50], Step [20000/257353], Loss: 0.9168, Acc: 1.00\n",
            "Epoch [48/50], Step [30000/257353], Loss: 0.8303, Acc: 1.00\n",
            "Epoch [48/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [50000/257353], Loss: 0.5580, Acc: 1.00\n",
            "Epoch [48/50], Step [60000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [48/50], Step [70000/257353], Loss: 1.4631, Acc: 0.00\n",
            "Epoch [48/50], Step [80000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [48/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [48/50], Step [110000/257353], Loss: 0.5761, Acc: 1.00\n",
            "Epoch [48/50], Step [120000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [48/50], Step [130000/257353], Loss: 0.5715, Acc: 1.00\n",
            "Epoch [48/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [160000/257353], Loss: 0.6380, Acc: 1.00\n",
            "Epoch [48/50], Step [170000/257353], Loss: 1.5507, Acc: 0.00\n",
            "Epoch [48/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [48/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [200000/257353], Loss: 0.6170, Acc: 1.00\n",
            "Epoch [48/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [220000/257353], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [48/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [250000/257353], Loss: 0.6630, Acc: 1.00\n",
            "Validation Accuracy :  69.56\n",
            "Epoch [49/50], Step [10000/257353], Loss: 1.5388, Acc: 0.00\n",
            "Epoch [49/50], Step [20000/257353], Loss: 0.6384, Acc: 1.00\n",
            "Epoch [49/50], Step [30000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [49/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [50000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [49/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [49/50], Step [70000/257353], Loss: 1.1696, Acc: 0.00\n",
            "Epoch [49/50], Step [80000/257353], Loss: 0.6569, Acc: 1.00\n",
            "Epoch [49/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [100000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [49/50], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [130000/257353], Loss: 0.9620, Acc: 0.00\n",
            "Epoch [49/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [160000/257353], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [49/50], Step [170000/257353], Loss: 1.5471, Acc: 0.00\n",
            "Epoch [49/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [49/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [200000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [49/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [250000/257353], Loss: 0.6003, Acc: 1.00\n",
            "Validation Accuracy :  70.108\n",
            "Epoch [50/50], Step [10000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [50/50], Step [20000/257353], Loss: 0.5796, Acc: 1.00\n",
            "Epoch [50/50], Step [30000/257353], Loss: 0.5587, Acc: 1.00\n",
            "Epoch [50/50], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [50000/257353], Loss: 0.5687, Acc: 1.00\n",
            "Epoch [50/50], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [50/50], Step [70000/257353], Loss: 1.4587, Acc: 0.00\n",
            "Epoch [50/50], Step [80000/257353], Loss: 0.6062, Acc: 1.00\n",
            "Epoch [50/50], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [100000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [50/50], Step [110000/257353], Loss: 0.5858, Acc: 1.00\n",
            "Epoch [50/50], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [130000/257353], Loss: 0.5641, Acc: 1.00\n",
            "Epoch [50/50], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [160000/257353], Loss: 0.5819, Acc: 1.00\n",
            "Epoch [50/50], Step [170000/257353], Loss: 1.5281, Acc: 0.00\n",
            "Epoch [50/50], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [50/50], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [200000/257353], Loss: 1.2541, Acc: 0.00\n",
            "Epoch [50/50], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [50/50], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [250000/257353], Loss: 0.5747, Acc: 1.00\n",
            "Validation Accuracy :  73.324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N49QKT7LyQnF"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "41a57c5f-8416-4c89-88d4-476716976890",
        "id": "q1dXkUW4yQnJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "  \n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "73.088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb76Vs6McICr",
        "colab_type": "text"
      },
      "source": [
        "####Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_GdZmmjcKuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prob, hat, A = model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "diqEqxEVyQnT"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "56484d1d-e4e3-47dc-c2de-2ee9bfc7ffcf",
        "id": "nEIr0THCyQnX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.9765e-01, 2.3454e-03, 8.8873e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3797, 0.6203]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[3.6781e-11, 9.9993e-01, 7.1167e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1810, 0.1338, 0.1803, 0.1804, 0.1810, 0.1435]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[2.0375e-04, 9.9980e-01, 1.3246e-23]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3463, 0.3463, 0.3074]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[1.1957e-12, 1.1499e-02, 9.8850e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1096, 0.1110, 0.1032, 0.0653, 0.1109, 0.1097, 0.1110, 0.0597, 0.1110,\n",
            "         0.1085]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[8.8451e-01, 1.1549e-01, 5.3005e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2204, 0.2646, 0.2538, 0.2612]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[6.0902e-13, 9.9755e-01, 2.4493e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4994, 0.5006]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[3.2755e-19, 6.0339e-01, 3.9661e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2004, 0.2004, 0.1989, 0.2004, 0.1999]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[7.3406e-04, 9.9927e-01, 1.3557e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1432, 0.1426, 0.1418, 0.1432, 0.1434, 0.1433, 0.1425]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.4859e-11, 1.1543e-01, 8.8457e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0847, 0.0874, 0.0879, 0.0903, 0.0885, 0.0912, 0.0873, 0.0326, 0.0912,\n",
            "         0.0908, 0.0841, 0.0839]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[9.5646e-01, 4.3542e-02, 3.0265e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1016, 0.1835, 0.1697, 0.1829, 0.1788, 0.1836]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[1.8736e-20, 6.4748e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3461, 0.3076, 0.3463]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[1.2160e-02, 9.8738e-01, 4.6373e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1143, 0.1173, 0.1074, 0.1173, 0.1096, 0.1108, 0.1129, 0.0973, 0.1132]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[7.3601e-01, 2.6386e-01, 1.2675e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0641, 0.1604, 0.1466, 0.1604, 0.1576, 0.1517, 0.1593]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[5.7526e-05, 9.9994e-01, 6.8606e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1603, 0.1083, 0.1604, 0.0904, 0.1603, 0.1599, 0.1604]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[1.4447e-15, 8.7645e-10, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1430, 0.1439, 0.1439, 0.1435, 0.1429, 0.1389, 0.1439]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[1.9278e-05, 8.7503e-02, 9.1248e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2003, 0.1990, 0.2003, 0.2002, 0.2003]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.9707e-01, 2.9265e-03, 1.5835e-20]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3336, 0.3323, 0.3342]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[2.6252e-09, 5.3987e-01, 4.6013e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0740, 0.0807, 0.0504, 0.0786, 0.0820, 0.0635, 0.0823, 0.0818, 0.0818,\n",
            "         0.0797, 0.0823, 0.0810, 0.0820]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[5.7116e-07, 3.9838e-03, 9.9602e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[5.4026e-05, 9.9995e-01, 6.0884e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0805, 0.0838, 0.0763, 0.0835, 0.0527, 0.0838, 0.0837, 0.0835, 0.0827,\n",
            "         0.0398, 0.0838, 0.0838, 0.0821]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[9.3257e-05, 1.6186e-01, 8.3805e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3073, 0.3515, 0.3412]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[1.0000e+00, 9.0958e-11, 5.6686e-20]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3479, 0.6521]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 1.4746e-11, 2.3511e-36]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3042, 0.3479, 0.3479]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[1.0938e-03, 9.9891e-01, 5.9641e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2520, 0.2535, 0.2432, 0.2512]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.9764e-01, 2.3587e-03, 8.7844e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2578, 0.2482, 0.2666, 0.2274]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[9.0744e-12, 5.2626e-06, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1502, 0.1695, 0.1696, 0.0607, 0.1331, 0.1682, 0.1487]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[8.2407e-13, 1.0000e+00, 3.6977e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1835, 0.2068, 0.2060, 0.2065, 0.1971]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[6.8191e-15, 9.9728e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1091, 0.1871, 0.1934, 0.1933, 0.1928, 0.1242]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[2.2413e-16, 4.2002e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1727, 0.3233, 0.5041]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 2.2892e-12, 3.9394e-24]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1863, 0.1861, 0.1860, 0.0757, 0.1796, 0.1863]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[9.9999e-01, 1.3827e-05, 1.0175e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0648, 0.0856, 0.0853, 0.0839, 0.0856, 0.0841, 0.0856, 0.0856, 0.0855,\n",
            "         0.0856, 0.0853, 0.0832]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[1.9279e-15, 7.2865e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2405, 0.2532, 0.2531, 0.2532]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 3.0237e-12, 2.3155e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1628, 0.1691, 0.1688, 0.1693, 0.1692, 0.1607]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[4.5265e-08, 9.9795e-01, 2.0510e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3266, 0.3428, 0.3307]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[1.4860e-07, 9.9163e-01, 8.3722e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4979, 0.5021]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[1.8117e-20, 6.8842e-05, 9.9993e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1521, 0.1521, 0.0889, 0.1520, 0.1518, 0.1514, 0.1517]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[2.2511e-15, 9.9732e-01, 2.6821e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2511, 0.2510, 0.2492, 0.2487]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[3.0052e-02, 9.6994e-01, 7.4974e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5174, 0.4826]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[2.6785e-27, 5.3988e-19, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2408, 0.2947, 0.2947, 0.1697]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[1.0000e+00, 1.5466e-09, 1.2947e-22]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5012, 0.4988]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[1.8863e-15, 4.7121e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1330, 0.1278, 0.1347, 0.1350, 0.1350, 0.0880, 0.1350, 0.1115]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[1.5307e-14, 9.9995e-01, 5.1496e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5013, 0.4987]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[9.7778e-10, 1.0000e+00, 4.5532e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2300, 0.2585, 0.2516, 0.2599]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[3.2026e-09, 9.9999e-01, 8.2053e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0657, 0.1232, 0.1632, 0.1628, 0.1632, 0.1627, 0.1592]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[1.0334e-07, 1.0000e+00, 8.6418e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3323, 0.3300, 0.3377]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[5.4491e-12, 5.2118e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3723, 0.6277]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9996e-01, 3.9738e-05, 1.7028e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1446, 0.1438, 0.1425, 0.1415, 0.1446, 0.1399, 0.1431]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[6.7423e-09, 9.9903e-01, 9.7473e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1764, 0.4104, 0.4132]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[1.0000e+00, 3.9797e-06, 3.7394e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1246, 0.1263, 0.1260, 0.1264, 0.1241, 0.1253, 0.1212, 0.1263]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[3.9936e-08, 1.0000e+00, 4.7126e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0391, 0.0452, 0.0451, 0.0452, 0.0254, 0.0429, 0.0435, 0.0451, 0.0452,\n",
            "         0.0448, 0.0446, 0.0435, 0.0452, 0.0438, 0.0447, 0.0436, 0.0451, 0.0452,\n",
            "         0.0451, 0.0428, 0.0452, 0.0451, 0.0448]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UtA1xqZXyQng"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "al5dzkoxyQni",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0e0b334b-f97a-47f9-bfad-d275ad2bcead",
        "id": "DvjfohsWyQnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "PATH = 'model_state_CNN_50_Epochs'+str(model_num)+'.pt'\n",
        "PATH0 = 'model__CNN_50_Epochs'+str(model_num)+'.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xAja-e5-yQnx"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "554bca1c-415a-403f-eb8d-7480ec3d10ab",
        "id": "XhVED925yQny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.73088; \n",
            "F1-Score (Micro): 0.7308799999999999; \n",
            "F1-Score (Macro): 0.733557673398536; \n",
            "F1-Score (Weighted): 0.7346164932833777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuxgdt0C13kX",
        "colab_type": "text"
      },
      "source": [
        "###Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wamm2SsM19Gm",
        "colab_type": "text"
      },
      "source": [
        "####Loading Organic Dataset\n",
        "\n",
        "Dataset is already pre-processed and converted into *Bert Embeddings and Bag Level Labels*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFVzw0ZI17Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.load('O_train_bag.npy')\n",
        "test = np.load('O_test_bag.npy')\n",
        "val = np.load('O_val_bag.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCRXVIEu2tuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emd_train = train[0]\n",
        "lab_train = train[1]\n",
        "\n",
        "\n",
        "emd_test = test[0]\n",
        "lab_test = test[1]\n",
        "\n",
        "\n",
        "emd_val = val[0]\n",
        "lab_val = val[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63GkjNWS3K3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load('model__CNN_50_Epochs1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p11nX1i5cH1",
        "colab_type": "code",
        "outputId": "5a4b02fb-765e-456a-98ac-a2c794543e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "  (1): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhCP_u-7HQ5n",
        "colab_type": "text"
      },
      "source": [
        "Load the pre-trained model on Amazon Fine Foods Dataset for fine-tuning it on Organic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm0miatX5ewH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "organic_model = Attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEJjJtgi5hsl",
        "colab_type": "code",
        "outputId": "8ffb25ce-02a2-4659-d4f1-3252a357359e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "organic_model.load_state_dict(model.state_dict(), strict=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPU13ahC5qnE",
        "colab_type": "code",
        "outputId": "c83149b2-dabf-424c-87f9-059246bff870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n",
        "\n",
        "    \n",
        "if torch.cuda.is_available():\n",
        "    organic_model.cuda()\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "cuda = True\n",
        "\n",
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr2Z54ah-ogB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(organic_model.parameters(), lr=0.005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dgedv245upx",
        "colab_type": "text"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUq1lizL5yWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_chk = './checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7tOhnB52xv",
        "colab_type": "code",
        "outputId": "18ed38e3-3f5d-4da2-9eac-099086ec77a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "organic_model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 20, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(10, 50, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=25, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkJWx_p554wG",
        "colab_type": "code",
        "outputId": "ce67abd1-2621-467e-9416-f4d6b555d24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emd_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 1000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emd_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            # writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emd_val, lab_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [2/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [3/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [4/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [5/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [6/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/50], Step [3000/4687], Loss: 1.5341, Acc: 0.00\n",
            "Epoch [6/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [7/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/50], Step [2000/4687], Loss: 1.3388, Acc: 0.00\n",
            "Epoch [7/50], Step [3000/4687], Loss: 0.5574, Acc: 1.00\n",
            "Epoch [7/50], Step [4000/4687], Loss: 0.5516, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [8/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [9/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [10/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [11/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/50], Step [3000/4687], Loss: 0.5662, Acc: 1.00\n",
            "Epoch [11/50], Step [4000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [12/50], Step [1000/4687], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [12/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [13/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [14/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [15/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/50], Step [4000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [16/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/50], Step [4000/4687], Loss: 1.1166, Acc: 0.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [17/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [18/50], Step [1000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [18/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [19/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/50], Step [3000/4687], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [19/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [20/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [21/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [21/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [21/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [21/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [22/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [22/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [22/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [22/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [23/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [23/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [23/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [24/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [24/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [24/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [24/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [25/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [25/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [25/50], Step [4000/4687], Loss: 0.5524, Acc: 1.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [26/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [26/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [26/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [26/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [27/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [27/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [27/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [27/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [28/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [28/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [28/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [28/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [29/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [29/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [29/50], Step [3000/4687], Loss: 0.5638, Acc: 1.00\n",
            "Epoch [29/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [30/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [30/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [30/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [30/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [31/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [31/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [31/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [31/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [32/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [2000/4687], Loss: 1.2132, Acc: 0.00\n",
            "Epoch [32/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [32/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [33/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [33/50], Step [2000/4687], Loss: 1.5503, Acc: 0.00\n",
            "Epoch [33/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [33/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [34/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [34/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [34/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [34/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [35/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [35/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [35/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [35/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [36/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [36/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [36/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [37/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [37/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [37/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [38/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [38/50], Step [2000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [38/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [38/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [39/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [39/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [39/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [40/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [40/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [40/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [40/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [41/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [41/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [41/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [41/50], Step [4000/4687], Loss: 0.5549, Acc: 1.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [42/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [42/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [42/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [42/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [43/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [43/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [43/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [43/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [44/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [44/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [44/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [44/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [45/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [45/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [45/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [45/50], Step [4000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [46/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [46/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [46/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [46/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [47/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [47/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [47/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [47/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n",
            "Epoch [48/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [48/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [48/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [48/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  30.676328502415455\n",
            "Epoch [49/50], Step [1000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [49/50], Step [2000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [3000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [49/50], Step [4000/4687], Loss: 0.5585, Acc: 1.00\n",
            "Validation Accuracy :  29.71014492753623\n",
            "Epoch [50/50], Step [1000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [50/50], Step [2000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [50/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [50/50], Step [4000/4687], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  39.61352657004831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhJcVKLr5-7T",
        "colab_type": "text"
      },
      "source": [
        "####Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7_bRFVZ6Du6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-GTOw7_6GYv",
        "colab_type": "text"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t7SOxJk6IC-",
        "colab_type": "code",
        "outputId": "aab5931f-0183-4420-a6be-c622a586dd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        " \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "41.30434782608695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW_SynJZ6L3v",
        "colab_type": "text"
      },
      "source": [
        "####Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alKDvNfJ6Nd_",
        "colab_type": "code",
        "outputId": "9f2d41c6-bf20-4cab-aa5d-a91e4e193c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        " \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.2078e-06, 9.9999e-01, 2.4476e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[5.9808e-06, 9.9999e-01, 1.5863e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[2.5246e-06, 1.0000e+00, 6.4302e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[5.8604e-07, 1.0000e+00, 1.3563e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[1.3260e-05, 9.9998e-01, 3.5313e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[5.8210e-07, 1.0000e+00, 1.3462e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[9.7006e-06, 9.9999e-01, 2.5838e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[2.3894e-06, 1.0000e+00, 6.0648e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[4.4560e-06, 9.9999e-01, 1.1779e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[7.7417e-06, 9.9999e-01, 2.0613e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[6.1851e-06, 9.9999e-01, 1.6436e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[6.1851e-06, 9.9999e-01, 1.6436e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[6.1851e-06, 9.9999e-01, 1.6436e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[6.1851e-06, 9.9999e-01, 1.6436e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[1.0229e-06, 1.0000e+00, 2.4555e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[3.2975e-06, 1.0000e+00, 8.5465e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[7.7645e-06, 9.9999e-01, 2.0690e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[8.8045e-06, 9.9999e-01, 2.3521e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[7.3858e-06, 9.9999e-01, 1.9726e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[6.1340e-06, 9.9999e-01, 1.6319e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[1.7139e-06, 1.0000e+00, 4.2562e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[8.6094e-06, 9.9999e-01, 2.2950e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[6.4965e-06, 9.9999e-01, 1.7296e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[1.8126e-06, 1.0000e+00, 4.5185e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[4.2954e-06, 9.9999e-01, 1.1330e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[3.8065e-06, 1.0000e+00, 9.9614e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[1.0501e-06, 1.0000e+00, 2.5247e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[7.2317e-06, 9.9999e-01, 1.9261e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[4.2539e-06, 9.9999e-01, 1.1212e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[2.3941e-06, 1.0000e+00, 6.0771e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[1.9529e-06, 1.0000e+00, 4.8929e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[1.3553e-06, 1.0000e+00, 3.3138e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[4.7258e-06, 9.9999e-01, 1.2544e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[2.0863e-06, 1.0000e+00, 5.2473e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[1.6416e-05, 9.9998e-01, 4.3910e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[1.3751e-06, 1.0000e+00, 3.3654e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[2.0632e-06, 1.0000e+00, 5.1876e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[3.8067e-06, 1.0000e+00, 9.9612e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[1.3140e-05, 9.9998e-01, 3.5090e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[4.5889e-06, 9.9999e-01, 1.2158e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[9.0404e-07, 1.0000e+00, 2.1523e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[6.0406e-06, 9.9999e-01, 1.6057e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[2.3383e-05, 9.9997e-01, 6.2658e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[1.2959e-06, 1.0000e+00, 3.1595e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[1.6654e-06, 1.0000e+00, 4.1284e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[2.3980e-06, 1.0000e+00, 6.0874e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[1.3202e-05, 9.9998e-01, 3.5242e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[9.6295e-06, 9.9999e-01, 2.5696e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[3.2572e-06, 1.0000e+00, 8.4360e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[2.1515e-06, 1.0000e+00, 5.4226e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RvnFdV6RBT",
        "colab_type": "text"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abSOCz8z6atG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CTgzIwY6QhW",
        "colab_type": "code",
        "outputId": "9720b301-7791-41ed-f5e9-386d5e44af2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "PATH = 'organicTF50epoch'+str(model_num)+'.pt'\n",
        "PATH0 = 'organicTF50_epoch_model'+str(model_num)+'.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5J-Bv9F6cB7",
        "colab_type": "text"
      },
      "source": [
        "####Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkUvmrgL6dCk",
        "colab_type": "code",
        "outputId": "bf4b24a2-9cf1-43ea-c33d-c3994dd77a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.41304347826086957; \n",
            "F1-Score (Micro): 0.41304347826086957; \n",
            "F1-Score (Macro): 0.19487179487179487; \n",
            "F1-Score (Weighted): 0.24147157190635452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-aXwq9u-6Nw",
        "colab_type": "text"
      },
      "source": [
        "###Fine-Tuning with very low learning rate.\n",
        "\n",
        "We saw that the fine-tuning did not went well with the learning rate we used for training the pre-trained model on Fine Food Dataset. Let us try to decrease the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "takRi0fHJYvE",
        "colab": {}
      },
      "source": [
        "model = torch.load('model__CNN_50_Epochs1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8540120b-8dda-4175-9ded-22fc11b2c8a4",
        "id": "i2OBQSTYJYvo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "  (1): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Sa37lHeJYv5",
        "colab": {}
      },
      "source": [
        "organic_model = Attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d359f192-bb87-45c6-fa98-10a9a2491c88",
        "id": "4JFKy2RaJYv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "organic_model.load_state_dict(model.state_dict(), strict=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1f003e54-b6ab-41e9-b2b6-56763b71f182",
        "id": "J05ZeTD-JYwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n",
        "\n",
        "    \n",
        "if torch.cuda.is_available():\n",
        "    organic_model.cuda()\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "cuda = True\n",
        "\n",
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wenVaZRNJYwT",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(organic_model.parameters(), lr=0.00005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9fd167ff-fa36-457d-f435-8c325c78966d",
        "id": "cYz2wV1fJYwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "organic_model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 20, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(10, 50, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=25, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ_-ZR_mJhAX",
        "colab_type": "text"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7d6b94f0-f755-4921-90e3-36e89d77e062",
        "id": "P_Oa98U8JYwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emd_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 1000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emd_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            # writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emd_val, lab_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [1000/4687], Loss: 1.5502, Acc: 0.00\n",
            "Epoch [1/50], Step [2000/4687], Loss: 0.9052, Acc: 1.00\n",
            "Epoch [1/50], Step [3000/4687], Loss: 0.5632, Acc: 1.00\n",
            "Epoch [1/50], Step [4000/4687], Loss: 1.5272, Acc: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [2/50], Step [1000/4687], Loss: 1.4746, Acc: 0.00\n",
            "Epoch [2/50], Step [2000/4687], Loss: 1.1743, Acc: 0.00\n",
            "Epoch [2/50], Step [3000/4687], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [2/50], Step [4000/4687], Loss: 1.0564, Acc: 0.00\n",
            "Validation Accuracy :  50.48309178743962\n",
            "Epoch [3/50], Step [1000/4687], Loss: 0.8718, Acc: 1.00\n",
            "Epoch [3/50], Step [2000/4687], Loss: 1.4813, Acc: 0.00\n",
            "Epoch [3/50], Step [3000/4687], Loss: 0.5747, Acc: 1.00\n",
            "Epoch [3/50], Step [4000/4687], Loss: 0.5742, Acc: 1.00\n",
            "Validation Accuracy :  51.20772946859904\n",
            "Epoch [4/50], Step [1000/4687], Loss: 0.6116, Acc: 1.00\n",
            "Epoch [4/50], Step [2000/4687], Loss: 1.5128, Acc: 0.00\n",
            "Epoch [4/50], Step [3000/4687], Loss: 0.5817, Acc: 1.00\n",
            "Epoch [4/50], Step [4000/4687], Loss: 0.5957, Acc: 1.00\n",
            "Validation Accuracy :  52.89855072463768\n",
            "Epoch [5/50], Step [1000/4687], Loss: 0.5844, Acc: 1.00\n",
            "Epoch [5/50], Step [2000/4687], Loss: 1.5066, Acc: 0.00\n",
            "Epoch [5/50], Step [3000/4687], Loss: 0.6341, Acc: 1.00\n",
            "Epoch [5/50], Step [4000/4687], Loss: 0.6371, Acc: 1.00\n",
            "Validation Accuracy :  54.589371980676326\n",
            "Epoch [6/50], Step [1000/4687], Loss: 0.6431, Acc: 1.00\n",
            "Epoch [6/50], Step [2000/4687], Loss: 1.4613, Acc: 0.00\n",
            "Epoch [6/50], Step [3000/4687], Loss: 0.6633, Acc: 1.00\n",
            "Epoch [6/50], Step [4000/4687], Loss: 0.6628, Acc: 1.00\n",
            "Validation Accuracy :  56.28019323671497\n",
            "Epoch [7/50], Step [1000/4687], Loss: 0.6743, Acc: 1.00\n",
            "Epoch [7/50], Step [2000/4687], Loss: 1.3885, Acc: 0.00\n",
            "Epoch [7/50], Step [3000/4687], Loss: 0.6405, Acc: 1.00\n",
            "Epoch [7/50], Step [4000/4687], Loss: 0.7321, Acc: 1.00\n",
            "Validation Accuracy :  58.454106280193244\n",
            "Epoch [8/50], Step [1000/4687], Loss: 0.7401, Acc: 1.00\n",
            "Epoch [8/50], Step [2000/4687], Loss: 1.3717, Acc: 0.00\n",
            "Epoch [8/50], Step [3000/4687], Loss: 0.6191, Acc: 1.00\n",
            "Epoch [8/50], Step [4000/4687], Loss: 0.8894, Acc: 1.00\n",
            "Validation Accuracy :  57.48792270531401\n",
            "Epoch [9/50], Step [1000/4687], Loss: 0.7968, Acc: 1.00\n",
            "Epoch [9/50], Step [2000/4687], Loss: 1.3257, Acc: 0.00\n",
            "Epoch [9/50], Step [3000/4687], Loss: 0.5986, Acc: 1.00\n",
            "Epoch [9/50], Step [4000/4687], Loss: 0.9795, Acc: 0.00\n",
            "Validation Accuracy :  57.729468599033815\n",
            "Epoch [10/50], Step [1000/4687], Loss: 0.8056, Acc: 1.00\n",
            "Epoch [10/50], Step [2000/4687], Loss: 1.3128, Acc: 0.00\n",
            "Epoch [10/50], Step [3000/4687], Loss: 0.5869, Acc: 1.00\n",
            "Epoch [10/50], Step [4000/4687], Loss: 1.0317, Acc: 0.00\n",
            "Validation Accuracy :  58.212560386473434\n",
            "Epoch [11/50], Step [1000/4687], Loss: 0.7710, Acc: 1.00\n",
            "Epoch [11/50], Step [2000/4687], Loss: 1.3409, Acc: 0.00\n",
            "Epoch [11/50], Step [3000/4687], Loss: 0.5755, Acc: 1.00\n",
            "Epoch [11/50], Step [4000/4687], Loss: 1.0573, Acc: 0.00\n",
            "Validation Accuracy :  58.212560386473434\n",
            "Epoch [12/50], Step [1000/4687], Loss: 0.7415, Acc: 1.00\n",
            "Epoch [12/50], Step [2000/4687], Loss: 1.2628, Acc: 0.00\n",
            "Epoch [12/50], Step [3000/4687], Loss: 0.5688, Acc: 1.00\n",
            "Epoch [12/50], Step [4000/4687], Loss: 1.0894, Acc: 0.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [13/50], Step [1000/4687], Loss: 0.6962, Acc: 1.00\n",
            "Epoch [13/50], Step [2000/4687], Loss: 1.1747, Acc: 0.00\n",
            "Epoch [13/50], Step [3000/4687], Loss: 0.5645, Acc: 1.00\n",
            "Epoch [13/50], Step [4000/4687], Loss: 1.0718, Acc: 0.00\n",
            "Validation Accuracy :  57.2463768115942\n",
            "Epoch [14/50], Step [1000/4687], Loss: 0.6556, Acc: 1.00\n",
            "Epoch [14/50], Step [2000/4687], Loss: 0.9341, Acc: 1.00\n",
            "Epoch [14/50], Step [3000/4687], Loss: 0.5630, Acc: 1.00\n",
            "Epoch [14/50], Step [4000/4687], Loss: 1.0431, Acc: 0.00\n",
            "Validation Accuracy :  56.763285024154584\n",
            "Epoch [15/50], Step [1000/4687], Loss: 0.6364, Acc: 1.00\n",
            "Epoch [15/50], Step [2000/4687], Loss: 0.6755, Acc: 1.00\n",
            "Epoch [15/50], Step [3000/4687], Loss: 0.5614, Acc: 1.00\n",
            "Epoch [15/50], Step [4000/4687], Loss: 1.0547, Acc: 0.00\n",
            "Validation Accuracy :  57.48792270531401\n",
            "Epoch [16/50], Step [1000/4687], Loss: 0.6291, Acc: 1.00\n",
            "Epoch [16/50], Step [2000/4687], Loss: 0.6653, Acc: 1.00\n",
            "Epoch [16/50], Step [3000/4687], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [16/50], Step [4000/4687], Loss: 1.1042, Acc: 0.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [17/50], Step [1000/4687], Loss: 0.6215, Acc: 1.00\n",
            "Epoch [17/50], Step [2000/4687], Loss: 0.6424, Acc: 1.00\n",
            "Epoch [17/50], Step [3000/4687], Loss: 0.5589, Acc: 1.00\n",
            "Epoch [17/50], Step [4000/4687], Loss: 1.1359, Acc: 0.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [18/50], Step [1000/4687], Loss: 0.6227, Acc: 1.00\n",
            "Epoch [18/50], Step [2000/4687], Loss: 0.6234, Acc: 1.00\n",
            "Epoch [18/50], Step [3000/4687], Loss: 0.5590, Acc: 1.00\n",
            "Epoch [18/50], Step [4000/4687], Loss: 1.1758, Acc: 0.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [19/50], Step [1000/4687], Loss: 0.6240, Acc: 1.00\n",
            "Epoch [19/50], Step [2000/4687], Loss: 0.6053, Acc: 1.00\n",
            "Epoch [19/50], Step [3000/4687], Loss: 0.5592, Acc: 1.00\n",
            "Epoch [19/50], Step [4000/4687], Loss: 1.1183, Acc: 0.00\n",
            "Validation Accuracy :  57.2463768115942\n",
            "Epoch [20/50], Step [1000/4687], Loss: 0.6277, Acc: 1.00\n",
            "Epoch [20/50], Step [2000/4687], Loss: 0.5915, Acc: 1.00\n",
            "Epoch [20/50], Step [3000/4687], Loss: 0.5595, Acc: 1.00\n",
            "Epoch [20/50], Step [4000/4687], Loss: 1.0412, Acc: 0.00\n",
            "Validation Accuracy :  56.763285024154584\n",
            "Epoch [21/50], Step [1000/4687], Loss: 0.6313, Acc: 1.00\n",
            "Epoch [21/50], Step [2000/4687], Loss: 0.5791, Acc: 1.00\n",
            "Epoch [21/50], Step [3000/4687], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [21/50], Step [4000/4687], Loss: 1.0074, Acc: 0.00\n",
            "Validation Accuracy :  56.28019323671497\n",
            "Epoch [22/50], Step [1000/4687], Loss: 0.6208, Acc: 1.00\n",
            "Epoch [22/50], Step [2000/4687], Loss: 0.5759, Acc: 1.00\n",
            "Epoch [22/50], Step [3000/4687], Loss: 0.5591, Acc: 1.00\n",
            "Epoch [22/50], Step [4000/4687], Loss: 0.9113, Acc: 1.00\n",
            "Validation Accuracy :  56.28019323671497\n",
            "Epoch [23/50], Step [1000/4687], Loss: 0.6232, Acc: 1.00\n",
            "Epoch [23/50], Step [2000/4687], Loss: 0.5697, Acc: 1.00\n",
            "Epoch [23/50], Step [3000/4687], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [23/50], Step [4000/4687], Loss: 0.8314, Acc: 1.00\n",
            "Validation Accuracy :  56.28019323671497\n",
            "Epoch [24/50], Step [1000/4687], Loss: 0.6167, Acc: 1.00\n",
            "Epoch [24/50], Step [2000/4687], Loss: 0.5666, Acc: 1.00\n",
            "Epoch [24/50], Step [3000/4687], Loss: 0.5566, Acc: 1.00\n",
            "Epoch [24/50], Step [4000/4687], Loss: 0.8400, Acc: 1.00\n",
            "Validation Accuracy :  56.52173913043478\n",
            "Epoch [25/50], Step [1000/4687], Loss: 0.6086, Acc: 1.00\n",
            "Epoch [25/50], Step [2000/4687], Loss: 0.5653, Acc: 1.00\n",
            "Epoch [25/50], Step [3000/4687], Loss: 0.5563, Acc: 1.00\n",
            "Epoch [25/50], Step [4000/4687], Loss: 0.7989, Acc: 1.00\n",
            "Validation Accuracy :  57.00483091787439\n",
            "Epoch [26/50], Step [1000/4687], Loss: 0.5983, Acc: 1.00\n",
            "Epoch [26/50], Step [2000/4687], Loss: 0.5634, Acc: 1.00\n",
            "Epoch [26/50], Step [3000/4687], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [26/50], Step [4000/4687], Loss: 0.7988, Acc: 1.00\n",
            "Validation Accuracy :  56.763285024154584\n",
            "Epoch [27/50], Step [1000/4687], Loss: 0.5872, Acc: 1.00\n",
            "Epoch [27/50], Step [2000/4687], Loss: 0.5615, Acc: 1.00\n",
            "Epoch [27/50], Step [3000/4687], Loss: 0.5547, Acc: 1.00\n",
            "Epoch [27/50], Step [4000/4687], Loss: 0.7712, Acc: 1.00\n",
            "Validation Accuracy :  57.00483091787439\n",
            "Epoch [28/50], Step [1000/4687], Loss: 0.5850, Acc: 1.00\n",
            "Epoch [28/50], Step [2000/4687], Loss: 0.5592, Acc: 1.00\n",
            "Epoch [28/50], Step [3000/4687], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [28/50], Step [4000/4687], Loss: 0.7197, Acc: 1.00\n",
            "Validation Accuracy :  57.2463768115942\n",
            "Epoch [29/50], Step [1000/4687], Loss: 0.5830, Acc: 1.00\n",
            "Epoch [29/50], Step [2000/4687], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [29/50], Step [3000/4687], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [29/50], Step [4000/4687], Loss: 0.6878, Acc: 1.00\n",
            "Validation Accuracy :  58.454106280193244\n",
            "Epoch [30/50], Step [1000/4687], Loss: 0.5772, Acc: 1.00\n",
            "Epoch [30/50], Step [2000/4687], Loss: 0.5566, Acc: 1.00\n",
            "Epoch [30/50], Step [3000/4687], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [30/50], Step [4000/4687], Loss: 0.6397, Acc: 1.00\n",
            "Validation Accuracy :  59.17874396135265\n",
            "Epoch [31/50], Step [1000/4687], Loss: 0.5746, Acc: 1.00\n",
            "Epoch [31/50], Step [2000/4687], Loss: 0.5564, Acc: 1.00\n",
            "Epoch [31/50], Step [3000/4687], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [31/50], Step [4000/4687], Loss: 0.6304, Acc: 1.00\n",
            "Validation Accuracy :  58.93719806763285\n",
            "Epoch [32/50], Step [1000/4687], Loss: 0.5713, Acc: 1.00\n",
            "Epoch [32/50], Step [2000/4687], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [32/50], Step [3000/4687], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [32/50], Step [4000/4687], Loss: 0.6119, Acc: 1.00\n",
            "Validation Accuracy :  58.69565217391305\n",
            "Epoch [33/50], Step [1000/4687], Loss: 0.5691, Acc: 1.00\n",
            "Epoch [33/50], Step [2000/4687], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [33/50], Step [3000/4687], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [33/50], Step [4000/4687], Loss: 0.6314, Acc: 1.00\n",
            "Validation Accuracy :  58.93719806763285\n",
            "Epoch [34/50], Step [1000/4687], Loss: 0.5665, Acc: 1.00\n",
            "Epoch [34/50], Step [2000/4687], Loss: 0.5555, Acc: 1.00\n",
            "Epoch [34/50], Step [3000/4687], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [34/50], Step [4000/4687], Loss: 0.5980, Acc: 1.00\n",
            "Validation Accuracy :  58.93719806763285\n",
            "Epoch [35/50], Step [1000/4687], Loss: 0.5630, Acc: 1.00\n",
            "Epoch [35/50], Step [2000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [35/50], Step [3000/4687], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [35/50], Step [4000/4687], Loss: 0.5864, Acc: 1.00\n",
            "Validation Accuracy :  59.17874396135265\n",
            "Epoch [36/50], Step [1000/4687], Loss: 0.5612, Acc: 1.00\n",
            "Epoch [36/50], Step [2000/4687], Loss: 0.5575, Acc: 1.00\n",
            "Epoch [36/50], Step [3000/4687], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [36/50], Step [4000/4687], Loss: 0.5839, Acc: 1.00\n",
            "Validation Accuracy :  58.93719806763285\n",
            "Epoch [37/50], Step [1000/4687], Loss: 0.5596, Acc: 1.00\n",
            "Epoch [37/50], Step [2000/4687], Loss: 0.5580, Acc: 1.00\n",
            "Epoch [37/50], Step [3000/4687], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [37/50], Step [4000/4687], Loss: 0.5777, Acc: 1.00\n",
            "Validation Accuracy :  59.17874396135265\n",
            "Epoch [38/50], Step [1000/4687], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [38/50], Step [2000/4687], Loss: 0.5590, Acc: 1.00\n",
            "Epoch [38/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [38/50], Step [4000/4687], Loss: 0.5774, Acc: 1.00\n",
            "Validation Accuracy :  58.93719806763285\n",
            "Epoch [39/50], Step [1000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [39/50], Step [2000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [39/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [39/50], Step [4000/4687], Loss: 0.5728, Acc: 1.00\n",
            "Validation Accuracy :  59.42028985507246\n",
            "Epoch [40/50], Step [1000/4687], Loss: 0.5561, Acc: 1.00\n",
            "Epoch [40/50], Step [2000/4687], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [40/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [40/50], Step [4000/4687], Loss: 0.5638, Acc: 1.00\n",
            "Validation Accuracy :  58.69565217391305\n",
            "Epoch [41/50], Step [1000/4687], Loss: 0.5565, Acc: 1.00\n",
            "Epoch [41/50], Step [2000/4687], Loss: 0.5649, Acc: 1.00\n",
            "Epoch [41/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [41/50], Step [4000/4687], Loss: 0.5606, Acc: 1.00\n",
            "Validation Accuracy :  58.69565217391305\n",
            "Epoch [42/50], Step [1000/4687], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [42/50], Step [2000/4687], Loss: 0.5609, Acc: 1.00\n",
            "Epoch [42/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [42/50], Step [4000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Validation Accuracy :  59.17874396135265\n",
            "Epoch [43/50], Step [1000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [43/50], Step [2000/4687], Loss: 0.5654, Acc: 1.00\n",
            "Epoch [43/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [43/50], Step [4000/4687], Loss: 0.5565, Acc: 1.00\n",
            "Validation Accuracy :  58.69565217391305\n",
            "Epoch [44/50], Step [1000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [44/50], Step [2000/4687], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [44/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [44/50], Step [4000/4687], Loss: 0.5572, Acc: 1.00\n",
            "Validation Accuracy :  58.69565217391305\n",
            "Epoch [45/50], Step [1000/4687], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [45/50], Step [2000/4687], Loss: 0.5694, Acc: 1.00\n",
            "Epoch [45/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [45/50], Step [4000/4687], Loss: 0.5569, Acc: 1.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [46/50], Step [1000/4687], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [46/50], Step [2000/4687], Loss: 0.5675, Acc: 1.00\n",
            "Epoch [46/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [46/50], Step [4000/4687], Loss: 0.5563, Acc: 1.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [47/50], Step [1000/4687], Loss: 0.5592, Acc: 1.00\n",
            "Epoch [47/50], Step [2000/4687], Loss: 0.5705, Acc: 1.00\n",
            "Epoch [47/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [47/50], Step [4000/4687], Loss: 0.5544, Acc: 1.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [48/50], Step [1000/4687], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [48/50], Step [2000/4687], Loss: 0.5695, Acc: 1.00\n",
            "Epoch [48/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [48/50], Step [4000/4687], Loss: 0.5544, Acc: 1.00\n",
            "Validation Accuracy :  57.729468599033815\n",
            "Epoch [49/50], Step [1000/4687], Loss: 0.5606, Acc: 1.00\n",
            "Epoch [49/50], Step [2000/4687], Loss: 0.5647, Acc: 1.00\n",
            "Epoch [49/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [49/50], Step [4000/4687], Loss: 0.5546, Acc: 1.00\n",
            "Validation Accuracy :  57.971014492753625\n",
            "Epoch [50/50], Step [1000/4687], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [50/50], Step [2000/4687], Loss: 0.5747, Acc: 1.00\n",
            "Epoch [50/50], Step [3000/4687], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [50/50], Step [4000/4687], Loss: 0.5534, Acc: 1.00\n",
            "Validation Accuracy :  57.729468599033815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l_KQOZFRJYwq"
      },
      "source": [
        "####Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_m-yKSKJYws",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T3rKMC_DJYwx"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fcd7db49-6b94-406a-af3a-0f7000ecb13d",
        "id": "w6C0LdbYJYw2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data) \n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda() \n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        " \n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "56.739130434782616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfFf5bXPJNsu",
        "colab_type": "text"
      },
      "source": [
        "Decreasing the learning rate helped!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SeI6Lc1KJYw8"
      },
      "source": [
        "####Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "70ad700c-508b-4025-ebbf-4931eab3b476",
        "id": "l6QqBycaJYw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[1.1206e-08, 1.0000e+00, 6.1858e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[1.0243e-10, 1.0000e+00, 2.4777e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[4.3828e-06, 1.0000e+00, 1.5409e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[9.9527e-01, 4.4906e-03, 2.4053e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[2.1905e-06, 9.9689e-01, 3.1099e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[5.5380e-05, 9.9994e-01, 1.1352e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[3.4834e-05, 9.9996e-01, 1.3263e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[1.9139e-13, 6.9042e-06, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.5756e-13, 9.5325e-01, 4.6747e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[1.0000e+00, 6.2168e-10, 7.5177e-25]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[1.6633e-09, 4.3385e-01, 5.6615e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[1.6633e-09, 4.3385e-01, 5.6615e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[1.6633e-09, 4.3385e-01, 5.6615e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[1.6633e-09, 4.3385e-01, 5.6615e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[9.6317e-06, 9.9890e-01, 1.0907e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[1.6474e-07, 1.0000e+00, 2.2658e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[4.9482e-13, 1.0000e+00, 1.3987e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[1.8385e-13, 1.0000e+00, 1.3884e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[6.6642e-11, 1.0000e+00, 7.0262e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[1.5807e-10, 9.9997e-01, 3.2231e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[3.8358e-08, 9.9004e-01, 9.9583e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[1.8405e-06, 9.9992e-01, 7.8894e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[7.9032e-09, 1.0000e+00, 2.2088e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[4.9645e-11, 1.0000e+00, 4.1737e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.5185e-01, 4.8149e-02, 3.0937e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[2.5697e-08, 1.0022e-03, 9.9900e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[7.3833e-06, 9.5377e-01, 4.6225e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[3.8150e-09, 9.9990e-01, 9.9667e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[0.0453, 0.4920, 0.4627]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 2.8787e-11, 1.7911e-26]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[6.6837e-09, 1.0000e+00, 1.9933e-15]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[9.9635e-01, 3.6521e-03, 2.0039e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[2.8188e-07, 9.9933e-01, 6.6767e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[2.2988e-12, 1.0000e+00, 2.9095e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[2.4321e-10, 1.2528e-05, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[7.7364e-17, 9.9941e-01, 5.8516e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[2.2801e-16, 9.9961e-01, 3.8923e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[4.8741e-08, 1.0000e+00, 4.3424e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[6.9510e-06, 3.2545e-01, 6.7454e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[1.2224e-08, 9.9987e-01, 1.3313e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[5.8191e-12, 1.0000e+00, 1.6818e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[2.8595e-02, 9.7141e-01, 2.2465e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[9.5848e-01, 4.1519e-02, 1.0681e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[5.1258e-06, 9.8918e-01, 1.0811e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[4.1659e-11, 1.0000e+00, 5.7462e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[3.1701e-13, 1.0000e+00, 2.7830e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[7.5234e-11, 1.0000e+00, 1.9677e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[1.2313e-04, 8.0052e-01, 1.9935e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[2.7269e-11, 1.0000e+00, 1.1626e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[4.6520e-14, 1.0000e+00, 3.9569e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmYrob49JYxH"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "95b785dc-a9b3-4634-ffa8-1fa0bc9342f7",
        "id": "g9aOmsfwJYxJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "PATH = 'organicTF50epoch'+str(model_num)+'.pt'\n",
        "PATH0 = 'organicTF50_epoch_model'+str(model_num)+'.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pCSunYhOJYxO"
      },
      "source": [
        "####Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c59c0fb8-e534-427c-e872-87c4d40790f4",
        "id": "jVjziOklJYxP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5673913043478261; \n",
            "F1-Score (Micro): 0.5673913043478261; \n",
            "F1-Score (Macro): 0.5396239625073823; \n",
            "F1-Score (Weighted): 0.5533975044542362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQOSapkPRDY0",
        "colab_type": "text"
      },
      "source": [
        "##MIL CNN with 1D MaxPool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIxR0z3YRIdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L = 500\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Conv1d(768, 20, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2, stride=2),\n",
        "            nn.Conv1d(20, 50, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(50 * 1, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 3),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = H.view(-1, 50 * 1)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "\n",
        "        A = self.attention(H)  # NxK\n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        \n",
        "        Y_hat = torch.argmax(Y_prob, dim=1).float()\n",
        "\n",
        "        return Y_prob, Y_hat, A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1777ead0-3a84-4cab-add9-51c6ead538ba",
        "id": "T5QR1UkxRI3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4136c73-1be7-4341-e0b2-930c10f25873",
        "id": "PEmg3urHRI4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Attention()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ct3hSPRoRI4Z",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FxZz7G1oRI4g",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuAwKnmpRI4l",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZZvGtkFObDE",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint saving to be able to resume training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFp9yKYuRI4r",
        "colab": {}
      },
      "source": [
        "model_chk = './checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JB4bLeNhRI42"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a666d39c-c39b-4810-f8eb-abfc6489e0e5",
        "id": "Ra6r8cr7RI43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 20, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(20, 50, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94599e1c-11a8-4c8d-f8db-cb37c191be06",
        "id": "I1WKGalURI5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emb_val, lab_val) # Validation after each Epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10000/257353], Loss: 1.5475, Acc: 0.00\n",
            "Epoch [1/20], Step [20000/257353], Loss: 0.6236, Acc: 1.00\n",
            "Epoch [1/20], Step [30000/257353], Loss: 1.4586, Acc: 0.00\n",
            "Epoch [1/20], Step [40000/257353], Loss: 1.1600, Acc: 0.00\n",
            "Epoch [1/20], Step [50000/257353], Loss: 0.5756, Acc: 1.00\n",
            "Epoch [1/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [70000/257353], Loss: 1.5480, Acc: 0.00\n",
            "Epoch [1/20], Step [80000/257353], Loss: 1.5224, Acc: 0.00\n",
            "Epoch [1/20], Step [90000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [1/20], Step [100000/257353], Loss: 1.5489, Acc: 0.00\n",
            "Epoch [1/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [130000/257353], Loss: 1.4822, Acc: 0.00\n",
            "Epoch [1/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [150000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [1/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [170000/257353], Loss: 0.5619, Acc: 1.00\n",
            "Epoch [1/20], Step [180000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [1/20], Step [190000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [1/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [1/20], Step [210000/257353], Loss: 1.0132, Acc: 0.00\n",
            "Epoch [1/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [240000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [1/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  70.666\n",
            "Epoch [2/20], Step [10000/257353], Loss: 1.5472, Acc: 0.00\n",
            "Epoch [2/20], Step [20000/257353], Loss: 0.5892, Acc: 1.00\n",
            "Epoch [2/20], Step [30000/257353], Loss: 1.0588, Acc: 0.00\n",
            "Epoch [2/20], Step [40000/257353], Loss: 1.2524, Acc: 0.00\n",
            "Epoch [2/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/20], Step [80000/257353], Loss: 1.4608, Acc: 0.00\n",
            "Epoch [2/20], Step [90000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [2/20], Step [100000/257353], Loss: 1.5458, Acc: 0.00\n",
            "Epoch [2/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [130000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [2/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [150000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [2/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [170000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [2/20], Step [180000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [2/20], Step [190000/257353], Loss: 1.5492, Acc: 0.00\n",
            "Epoch [2/20], Step [200000/257353], Loss: 0.5931, Acc: 1.00\n",
            "Epoch [2/20], Step [210000/257353], Loss: 1.4653, Acc: 0.00\n",
            "Epoch [2/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [2/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.258\n",
            "Epoch [3/20], Step [10000/257353], Loss: 1.5161, Acc: 0.00\n",
            "Epoch [3/20], Step [20000/257353], Loss: 0.6489, Acc: 1.00\n",
            "Epoch [3/20], Step [30000/257353], Loss: 1.4719, Acc: 0.00\n",
            "Epoch [3/20], Step [40000/257353], Loss: 0.7784, Acc: 1.00\n",
            "Epoch [3/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [70000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [3/20], Step [80000/257353], Loss: 0.5566, Acc: 1.00\n",
            "Epoch [3/20], Step [90000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [3/20], Step [100000/257353], Loss: 1.5232, Acc: 0.00\n",
            "Epoch [3/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [130000/257353], Loss: 0.5644, Acc: 1.00\n",
            "Epoch [3/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [150000/257353], Loss: 0.5853, Acc: 1.00\n",
            "Epoch [3/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [170000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [3/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/20], Step [210000/257353], Loss: 1.4321, Acc: 0.00\n",
            "Epoch [3/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.32\n",
            "Epoch [4/20], Step [10000/257353], Loss: 1.0599, Acc: 0.00\n",
            "Epoch [4/20], Step [20000/257353], Loss: 0.9005, Acc: 1.00\n",
            "Epoch [4/20], Step [30000/257353], Loss: 1.5407, Acc: 0.00\n",
            "Epoch [4/20], Step [40000/257353], Loss: 1.3173, Acc: 0.00\n",
            "Epoch [4/20], Step [50000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [4/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/20], Step [80000/257353], Loss: 1.3280, Acc: 0.00\n",
            "Epoch [4/20], Step [90000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [4/20], Step [100000/257353], Loss: 1.5419, Acc: 0.00\n",
            "Epoch [4/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [130000/257353], Loss: 0.5647, Acc: 1.00\n",
            "Epoch [4/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [150000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [170000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/20], Step [180000/257353], Loss: 0.5537, Acc: 1.00\n",
            "Epoch [4/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/20], Step [200000/257353], Loss: 0.5532, Acc: 1.00\n",
            "Epoch [4/20], Step [210000/257353], Loss: 0.8963, Acc: 1.00\n",
            "Epoch [4/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [240000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.794\n",
            "Epoch [5/20], Step [10000/257353], Loss: 1.4960, Acc: 0.00\n",
            "Epoch [5/20], Step [20000/257353], Loss: 0.5618, Acc: 1.00\n",
            "Epoch [5/20], Step [30000/257353], Loss: 1.4767, Acc: 0.00\n",
            "Epoch [5/20], Step [40000/257353], Loss: 0.7881, Acc: 1.00\n",
            "Epoch [5/20], Step [50000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [5/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/20], Step [80000/257353], Loss: 1.5014, Acc: 0.00\n",
            "Epoch [5/20], Step [90000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [5/20], Step [100000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [130000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [5/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [150000/257353], Loss: 1.2340, Acc: 0.00\n",
            "Epoch [5/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [170000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [5/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/20], Step [190000/257353], Loss: 1.5469, Acc: 0.00\n",
            "Epoch [5/20], Step [200000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [210000/257353], Loss: 1.5046, Acc: 0.00\n",
            "Epoch [5/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  72.75399999999999\n",
            "Epoch [6/20], Step [10000/257353], Loss: 1.4873, Acc: 0.00\n",
            "Epoch [6/20], Step [20000/257353], Loss: 0.5687, Acc: 1.00\n",
            "Epoch [6/20], Step [30000/257353], Loss: 1.4532, Acc: 0.00\n",
            "Epoch [6/20], Step [40000/257353], Loss: 0.6778, Acc: 1.00\n",
            "Epoch [6/20], Step [50000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [6/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [80000/257353], Loss: 1.5105, Acc: 0.00\n",
            "Epoch [6/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [100000/257353], Loss: 1.5507, Acc: 0.00\n",
            "Epoch [6/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [130000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [150000/257353], Loss: 0.5564, Acc: 1.00\n",
            "Epoch [6/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [170000/257353], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [6/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/20], Step [210000/257353], Loss: 1.5484, Acc: 0.00\n",
            "Epoch [6/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  69.586\n",
            "Epoch [7/20], Step [10000/257353], Loss: 1.5494, Acc: 0.00\n",
            "Epoch [7/20], Step [20000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [7/20], Step [30000/257353], Loss: 1.5164, Acc: 0.00\n",
            "Epoch [7/20], Step [40000/257353], Loss: 0.9719, Acc: 0.00\n",
            "Epoch [7/20], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [7/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/20], Step [80000/257353], Loss: 1.0284, Acc: 0.00\n",
            "Epoch [7/20], Step [90000/257353], Loss: 1.5498, Acc: 0.00\n",
            "Epoch [7/20], Step [100000/257353], Loss: 1.5434, Acc: 0.00\n",
            "Epoch [7/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [130000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [150000/257353], Loss: 0.7745, Acc: 1.00\n",
            "Epoch [7/20], Step [160000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [170000/257353], Loss: 0.5679, Acc: 1.00\n",
            "Epoch [7/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [190000/257353], Loss: 1.5507, Acc: 0.00\n",
            "Epoch [7/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [210000/257353], Loss: 1.5008, Acc: 0.00\n",
            "Epoch [7/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.432\n",
            "Epoch [8/20], Step [10000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [8/20], Step [20000/257353], Loss: 0.6108, Acc: 1.00\n",
            "Epoch [8/20], Step [30000/257353], Loss: 1.4607, Acc: 0.00\n",
            "Epoch [8/20], Step [40000/257353], Loss: 1.1885, Acc: 0.00\n",
            "Epoch [8/20], Step [50000/257353], Loss: 0.5657, Acc: 1.00\n",
            "Epoch [8/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [70000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [8/20], Step [80000/257353], Loss: 1.2501, Acc: 0.00\n",
            "Epoch [8/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/20], Step [100000/257353], Loss: 1.5472, Acc: 0.00\n",
            "Epoch [8/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [130000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [8/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [150000/257353], Loss: 0.6125, Acc: 1.00\n",
            "Epoch [8/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [170000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [8/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/20], Step [200000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [210000/257353], Loss: 1.4585, Acc: 0.00\n",
            "Epoch [8/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.886\n",
            "Epoch [9/20], Step [10000/257353], Loss: 1.1588, Acc: 0.00\n",
            "Epoch [9/20], Step [20000/257353], Loss: 0.5618, Acc: 1.00\n",
            "Epoch [9/20], Step [30000/257353], Loss: 1.5114, Acc: 0.00\n",
            "Epoch [9/20], Step [40000/257353], Loss: 1.5484, Acc: 0.00\n",
            "Epoch [9/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/20], Step [80000/257353], Loss: 0.6535, Acc: 1.00\n",
            "Epoch [9/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/20], Step [100000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [9/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [130000/257353], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [9/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [150000/257353], Loss: 1.3729, Acc: 0.00\n",
            "Epoch [9/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [170000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [9/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/20], Step [190000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [9/20], Step [200000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [9/20], Step [210000/257353], Loss: 1.5360, Acc: 0.00\n",
            "Epoch [9/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  73.476\n",
            "Epoch [10/20], Step [10000/257353], Loss: 0.9508, Acc: 1.00\n",
            "Epoch [10/20], Step [20000/257353], Loss: 0.5631, Acc: 1.00\n",
            "Epoch [10/20], Step [30000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [10/20], Step [40000/257353], Loss: 1.4901, Acc: 0.00\n",
            "Epoch [10/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [70000/257353], Loss: 1.5463, Acc: 0.00\n",
            "Epoch [10/20], Step [80000/257353], Loss: 0.7356, Acc: 1.00\n",
            "Epoch [10/20], Step [90000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [10/20], Step [100000/257353], Loss: 1.5444, Acc: 0.00\n",
            "Epoch [10/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [130000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [150000/257353], Loss: 1.0717, Acc: 0.00\n",
            "Epoch [10/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [170000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/20], Step [200000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [10/20], Step [210000/257353], Loss: 1.4672, Acc: 0.00\n",
            "Epoch [10/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  70.75\n",
            "Epoch [11/20], Step [10000/257353], Loss: 1.0915, Acc: 0.00\n",
            "Epoch [11/20], Step [20000/257353], Loss: 0.5687, Acc: 1.00\n",
            "Epoch [11/20], Step [30000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [11/20], Step [40000/257353], Loss: 1.0728, Acc: 0.00\n",
            "Epoch [11/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [70000/257353], Loss: 1.5375, Acc: 0.00\n",
            "Epoch [11/20], Step [80000/257353], Loss: 1.4416, Acc: 0.00\n",
            "Epoch [11/20], Step [90000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [11/20], Step [100000/257353], Loss: 1.5482, Acc: 0.00\n",
            "Epoch [11/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [130000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [11/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [150000/257353], Loss: 0.5958, Acc: 1.00\n",
            "Epoch [11/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [170000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [11/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/20], Step [210000/257353], Loss: 1.5488, Acc: 0.00\n",
            "Epoch [11/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  74.02\n",
            "Epoch [12/20], Step [10000/257353], Loss: 1.4876, Acc: 0.00\n",
            "Epoch [12/20], Step [20000/257353], Loss: 0.5838, Acc: 1.00\n",
            "Epoch [12/20], Step [30000/257353], Loss: 1.5457, Acc: 0.00\n",
            "Epoch [12/20], Step [40000/257353], Loss: 0.8702, Acc: 1.00\n",
            "Epoch [12/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/20], Step [80000/257353], Loss: 1.5071, Acc: 0.00\n",
            "Epoch [12/20], Step [90000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [12/20], Step [100000/257353], Loss: 1.5442, Acc: 0.00\n",
            "Epoch [12/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [130000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [12/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [150000/257353], Loss: 1.5201, Acc: 0.00\n",
            "Epoch [12/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [170000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/20], Step [200000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [210000/257353], Loss: 1.5268, Acc: 0.00\n",
            "Epoch [12/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.966\n",
            "Epoch [13/20], Step [10000/257353], Loss: 1.2344, Acc: 0.00\n",
            "Epoch [13/20], Step [20000/257353], Loss: 0.5840, Acc: 1.00\n",
            "Epoch [13/20], Step [30000/257353], Loss: 1.4285, Acc: 0.00\n",
            "Epoch [13/20], Step [40000/257353], Loss: 0.8841, Acc: 1.00\n",
            "Epoch [13/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [70000/257353], Loss: 1.5440, Acc: 0.00\n",
            "Epoch [13/20], Step [80000/257353], Loss: 1.2056, Acc: 0.00\n",
            "Epoch [13/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/20], Step [100000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [13/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [130000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [150000/257353], Loss: 1.0571, Acc: 0.00\n",
            "Epoch [13/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [170000/257353], Loss: 0.5530, Acc: 1.00\n",
            "Epoch [13/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [210000/257353], Loss: 1.5293, Acc: 0.00\n",
            "Epoch [13/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.502\n",
            "Epoch [14/20], Step [10000/257353], Loss: 1.5489, Acc: 0.00\n",
            "Epoch [14/20], Step [20000/257353], Loss: 0.5831, Acc: 1.00\n",
            "Epoch [14/20], Step [30000/257353], Loss: 1.5294, Acc: 0.00\n",
            "Epoch [14/20], Step [40000/257353], Loss: 1.1733, Acc: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mVkUxIgxRI5M"
      },
      "source": [
        "#####Resume Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD8RyMcIOsv2",
        "colab_type": "text"
      },
      "source": [
        "Load state dictionaries from checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aTvehoQURI4w",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load('checkpoint13.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_chkpoint = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5923cb1-7aed-4f90-9332-80a548d9efd9",
        "id": "StnSrt-2RI5N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epoch_chkpoint + 1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emb_val, lab_val) # Validation after each epoch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14/20], Step [10000/257353], Loss: 1.5469, Acc: 0.00\n",
            "Epoch [14/20], Step [20000/257353], Loss: 0.5558, Acc: 1.00\n",
            "Epoch [14/20], Step [30000/257353], Loss: 1.4972, Acc: 0.00\n",
            "Epoch [14/20], Step [40000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [14/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/20], Step [80000/257353], Loss: 0.9502, Acc: 1.00\n",
            "Epoch [14/20], Step [90000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [14/20], Step [100000/257353], Loss: 1.4977, Acc: 0.00\n",
            "Epoch [14/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [130000/257353], Loss: 0.5611, Acc: 1.00\n",
            "Epoch [14/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [150000/257353], Loss: 0.5586, Acc: 1.00\n",
            "Epoch [14/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [170000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/20], Step [180000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [14/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/20], Step [210000/257353], Loss: 1.3417, Acc: 0.00\n",
            "Epoch [14/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  73.622\n",
            "Epoch [15/20], Step [10000/257353], Loss: 1.5417, Acc: 0.00\n",
            "Epoch [15/20], Step [20000/257353], Loss: 0.5761, Acc: 1.00\n",
            "Epoch [15/20], Step [30000/257353], Loss: 1.5509, Acc: 0.00\n",
            "Epoch [15/20], Step [40000/257353], Loss: 0.5672, Acc: 1.00\n",
            "Epoch [15/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [70000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [15/20], Step [80000/257353], Loss: 1.3036, Acc: 0.00\n",
            "Epoch [15/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/20], Step [100000/257353], Loss: 1.5484, Acc: 0.00\n",
            "Epoch [15/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [130000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [15/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [150000/257353], Loss: 0.6168, Acc: 1.00\n",
            "Epoch [15/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [170000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [15/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [190000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [15/20], Step [200000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [15/20], Step [210000/257353], Loss: 1.2901, Acc: 0.00\n",
            "Epoch [15/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.916\n",
            "Epoch [16/20], Step [10000/257353], Loss: 1.3993, Acc: 0.00\n",
            "Epoch [16/20], Step [20000/257353], Loss: 0.5748, Acc: 1.00\n",
            "Epoch [16/20], Step [30000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/20], Step [40000/257353], Loss: 1.5429, Acc: 0.00\n",
            "Epoch [16/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/20], Step [80000/257353], Loss: 1.0094, Acc: 0.00\n",
            "Epoch [16/20], Step [90000/257353], Loss: 1.5503, Acc: 0.00\n",
            "Epoch [16/20], Step [100000/257353], Loss: 1.5299, Acc: 0.00\n",
            "Epoch [16/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [130000/257353], Loss: 1.2433, Acc: 0.00\n",
            "Epoch [16/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [150000/257353], Loss: 0.8359, Acc: 1.00\n",
            "Epoch [16/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [170000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [180000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [16/20], Step [190000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [16/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [210000/257353], Loss: 0.9581, Acc: 1.00\n",
            "Epoch [16/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [240000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  74.532\n",
            "Epoch [17/20], Step [10000/257353], Loss: 1.5158, Acc: 0.00\n",
            "Epoch [17/20], Step [20000/257353], Loss: 0.5609, Acc: 1.00\n",
            "Epoch [17/20], Step [30000/257353], Loss: 1.5271, Acc: 0.00\n",
            "Epoch [17/20], Step [40000/257353], Loss: 1.0349, Acc: 0.00\n",
            "Epoch [17/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/20], Step [80000/257353], Loss: 1.1020, Acc: 0.00\n",
            "Epoch [17/20], Step [90000/257353], Loss: 1.5487, Acc: 0.00\n",
            "Epoch [17/20], Step [100000/257353], Loss: 1.5502, Acc: 0.00\n",
            "Epoch [17/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [130000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [17/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [150000/257353], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [17/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [170000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/20], Step [200000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [17/20], Step [210000/257353], Loss: 1.4246, Acc: 0.00\n",
            "Epoch [17/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  73.952\n",
            "Epoch [18/20], Step [10000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [18/20], Step [20000/257353], Loss: 0.5631, Acc: 1.00\n",
            "Epoch [18/20], Step [30000/257353], Loss: 1.5490, Acc: 0.00\n",
            "Epoch [18/20], Step [40000/257353], Loss: 0.9131, Acc: 1.00\n",
            "Epoch [18/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [70000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [18/20], Step [80000/257353], Loss: 1.2326, Acc: 0.00\n",
            "Epoch [18/20], Step [90000/257353], Loss: 1.5510, Acc: 0.00\n",
            "Epoch [18/20], Step [100000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [18/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [130000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [18/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [150000/257353], Loss: 1.1161, Acc: 0.00\n",
            "Epoch [18/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [170000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/20], Step [200000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [210000/257353], Loss: 1.4466, Acc: 0.00\n",
            "Epoch [18/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.424\n",
            "Epoch [19/20], Step [10000/257353], Loss: 1.5500, Acc: 0.00\n",
            "Epoch [19/20], Step [20000/257353], Loss: 0.5573, Acc: 1.00\n",
            "Epoch [19/20], Step [30000/257353], Loss: 1.4535, Acc: 0.00\n",
            "Epoch [19/20], Step [40000/257353], Loss: 1.5291, Acc: 0.00\n",
            "Epoch [19/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [70000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [19/20], Step [80000/257353], Loss: 1.5279, Acc: 0.00\n",
            "Epoch [19/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/20], Step [100000/257353], Loss: 1.5486, Acc: 0.00\n",
            "Epoch [19/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [130000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [150000/257353], Loss: 1.0674, Acc: 0.00\n",
            "Epoch [19/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [170000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [180000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [190000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [19/20], Step [200000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [19/20], Step [210000/257353], Loss: 1.4241, Acc: 0.00\n",
            "Epoch [19/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.822\n",
            "Epoch [20/20], Step [10000/257353], Loss: 1.5481, Acc: 0.00\n",
            "Epoch [20/20], Step [20000/257353], Loss: 0.5752, Acc: 1.00\n",
            "Epoch [20/20], Step [30000/257353], Loss: 1.5061, Acc: 0.00\n",
            "Epoch [20/20], Step [40000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [70000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [80000/257353], Loss: 1.3435, Acc: 0.00\n",
            "Epoch [20/20], Step [90000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [100000/257353], Loss: 1.5504, Acc: 0.00\n",
            "Epoch [20/20], Step [110000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [130000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [20/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [150000/257353], Loss: 0.5740, Acc: 1.00\n",
            "Epoch [20/20], Step [160000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [170000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [180000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [190000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [210000/257353], Loss: 1.5158, Acc: 0.00\n",
            "Epoch [20/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  74.604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0nYLioJ2RI5V"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b86dbf35-9206-400a-d94a-cc337ff7dcf8",
        "id": "dgpSbbviRI5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "74.268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_CBNPS8mRI5h"
      },
      "source": [
        "####Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eOLrsRgfRI5j",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prob, hat, A = model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0-FeR57jRI5q"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "db0c2c4c-980d-4e22-b6bb-2b400845c0a7",
        "id": "tmLM7Ll2RI5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[9.9998e-01, 1.6217e-05, 2.7792e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2658, 0.7342]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[8.9767e-07, 7.7687e-01, 2.2313e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1912, 0.1900, 0.1919, 0.1856, 0.1907, 0.0506]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[1.5981e-02, 9.8402e-01, 2.8543e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4251, 0.4119, 0.1630]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[2.2411e-17, 5.3316e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0927, 0.1455, 0.0159, 0.0144, 0.1439, 0.1391, 0.1454, 0.0613, 0.1455,\n",
            "         0.0963]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[8.5825e-02, 9.1417e-01, 5.2681e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2704, 0.3332, 0.1476, 0.2488]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[2.9936e-07, 9.9995e-01, 5.3164e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3924, 0.6076]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[3.1208e-13, 1.9218e-04, 9.9981e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1931, 0.2046, 0.1993, 0.2045, 0.1986]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[1.8790e-01, 8.1210e-01, 2.2662e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1368, 0.1513, 0.1327, 0.1332, 0.1522, 0.1520, 0.1417]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.1243e-11, 3.3852e-05, 9.9997e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1096, 0.0651, 0.1145, 0.1148, 0.1139, 0.1162, 0.0886, 0.0169, 0.1163,\n",
            "         0.1156, 0.0151, 0.0134]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[9.9667e-01, 3.3344e-03, 6.8498e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0416, 0.1886, 0.1490, 0.1677, 0.2219, 0.2312]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[2.5128e-21, 7.4376e-16, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4733, 0.0499, 0.4768]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[6.9803e-01, 3.0135e-01, 6.2474e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1167, 0.1430, 0.1138, 0.1456, 0.1387, 0.0427, 0.1404, 0.0160, 0.1431]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[9.9990e-01, 9.5087e-05, 2.8402e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0541, 0.1969, 0.1260, 0.1972, 0.1880, 0.0436, 0.1942]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[6.1631e-03, 9.9384e-01, 2.7640e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1188, 0.0482, 0.1852, 0.0979, 0.1845, 0.1842, 0.1812]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[1.7546e-17, 1.7241e-17, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1269, 0.1663, 0.1663, 0.1607, 0.1531, 0.0606, 0.1662]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[2.3183e-11, 2.7214e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2205, 0.1240, 0.2196, 0.2158, 0.2201]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[6.0949e-01, 3.9051e-01, 7.1214e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3043, 0.3455, 0.3502]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[1.0676e-06, 5.5383e-03, 9.9446e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0996, 0.1028, 0.0172, 0.0400, 0.0939, 0.0763, 0.1109, 0.1084, 0.0411,\n",
            "         0.0693, 0.1115, 0.0508, 0.0782]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[0.0010, 0.0019, 0.9971]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[3.0073e-04, 9.9968e-01, 2.3471e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0700, 0.0874, 0.0518, 0.0802, 0.0674, 0.0898, 0.0884, 0.0855, 0.0869,\n",
            "         0.0249, 0.0898, 0.0898, 0.0881]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[0.0053, 0.9807, 0.0140]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3163, 0.3605, 0.3232]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.9960e-01, 4.0219e-04, 1.0773e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4543, 0.5457]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 8.3687e-09, 1.7771e-28]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2021, 0.3989, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[9.4863e-01, 5.1371e-02, 4.6102e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2659, 0.2254, 0.2404, 0.2683]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[9.8712e-01, 1.2876e-02, 1.5558e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2221, 0.3071, 0.3723, 0.0985]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[1.5698e-15, 5.6787e-12, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0278, 0.2177, 0.2174, 0.2171, 0.0384, 0.2176, 0.0640]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[2.4927e-08, 1.0000e+00, 6.0530e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0275, 0.2435, 0.2434, 0.2435, 0.2421]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[2.5061e-14, 2.3104e-10, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1528, 0.1733, 0.2045, 0.2030, 0.1991, 0.0673]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[6.3024e-21, 9.7821e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1605, 0.1160, 0.7235]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 3.9072e-08, 3.2290e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1944, 0.1942, 0.1914, 0.0309, 0.1945, 0.1946]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[1.0000e+00, 7.4467e-07, 2.0686e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0202, 0.0710, 0.0778, 0.0811, 0.0939, 0.0931, 0.0940, 0.0940, 0.0940,\n",
            "         0.0940, 0.0932, 0.0937]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[6.5947e-18, 3.8575e-13, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0814, 0.3059, 0.3063, 0.3064]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 3.8188e-10, 4.3159e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1002, 0.2145, 0.1927, 0.2178, 0.2178, 0.0570]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[2.5034e-07, 9.9983e-01, 1.6576e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3317, 0.4178, 0.2505]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[9.1066e-05, 9.1273e-01, 8.7182e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4647, 0.5353]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[3.9950e-17, 1.8301e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1693, 0.1693, 0.0168, 0.1683, 0.1668, 0.1675, 0.1420]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[3.0245e-09, 2.9179e-01, 7.0821e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2760, 0.2746, 0.1742, 0.2752]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[1.0372e-01, 8.9628e-01, 7.9348e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.9053, 0.0947]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[1.6622e-37, 3.6570e-27, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1815, 0.3851, 0.3851, 0.0484]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.8664e-01, 1.3355e-02, 2.4651e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4527, 0.5473]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[4.9053e-19, 6.0731e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1354, 0.1518, 0.1594, 0.1598, 0.1598, 0.0331, 0.1598, 0.0409]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[4.1126e-12, 9.9959e-01, 4.0941e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.9097, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[9.1835e-06, 9.9999e-01, 2.5414e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1850, 0.2881, 0.2390, 0.2879]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[5.2697e-05, 9.9986e-01, 8.4351e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0245, 0.0335, 0.2041, 0.1779, 0.2045, 0.1969, 0.1587]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[2.5277e-07, 9.9999e-01, 6.7318e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3092, 0.3391, 0.3516]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[6.1682e-10, 2.2230e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1405, 0.8595]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9806e-01, 1.9384e-03, 5.8932e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1556, 0.1520, 0.0893, 0.1515, 0.1556, 0.1425, 0.1535]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[2.4184e-01, 7.5815e-01, 7.4405e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1334, 0.6322, 0.2344]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[9.9873e-01, 1.2712e-03, 1.2170e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1049, 0.1460, 0.0637, 0.1507, 0.1414, 0.1481, 0.0951, 0.1501]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[9.0886e-06, 9.9544e-01, 4.5480e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0085, 0.0479, 0.0402, 0.0593, 0.0250, 0.0085, 0.0358, 0.0585, 0.0585,\n",
            "         0.0295, 0.0578, 0.0215, 0.0593, 0.0584, 0.0361, 0.0059, 0.0565, 0.0593,\n",
            "         0.0522, 0.0443, 0.0593, 0.0585, 0.0591]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnCOy2ezRI5z"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fYeXco1rRI51",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "34f37a53-b893-42e5-8111-46f2b0e6cc06",
        "id": "44LSvZZ_RI55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = '1dpool_model_state_CNN_20_Epochs'+str(model_num)+'.pt'\n",
        "PATH0 = '1dpool_model__CNN_20_Epochs'+str(model_num)+'.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fkdb39SlRI5_"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "315aea8b-2cb5-4bcc-f25c-4a3f0ee4b0b4",
        "id": "62IhQYJ1RI6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.74268; \n",
            "F1-Score (Micro): 0.74268; \n",
            "F1-Score (Macro): 0.7417440937348956; \n",
            "F1-Score (Weighted): 0.7430335352945195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dlix_5_mRI6H"
      },
      "source": [
        "###Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l0kE0WVvRI6I"
      },
      "source": [
        "#### Load Organic Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QDcFEQSoRI6K",
        "colab": {}
      },
      "source": [
        "train = np.load('O_train_bag.npy')\n",
        "test = np.load('O_test_bag.npy')\n",
        "val = np.load('O_val_bag.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qC7Zgr3JRI6N",
        "colab": {}
      },
      "source": [
        "emd_train = train[0]\n",
        "lab_train = train[1]\n",
        "\n",
        "\n",
        "emd_test = test[0]\n",
        "lab_test = test[1]\n",
        "\n",
        "\n",
        "emd_val = val[0]\n",
        "lab_val = val[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwqZXGoZRI6R",
        "colab": {}
      },
      "source": [
        "model = torch.load('model__CNN_50_Epochs1.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b2926a62-9fe7-4529-f5c3-47ca27abd425",
        "id": "8IXzqY5xRI6U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "model.classifier"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "  (1): Softmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wdtKi69QRI6a",
        "colab": {}
      },
      "source": [
        "organic_model = Attention()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1f73b8e8-f149-4f09-94f1-bde93a8f2523",
        "id": "BSfG4pP7RI6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "organic_model.load_state_dict(model.state_dict(), strict=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eeed25f1-4f09-44b5-9c8a-3be3474b66e6",
        "id": "XjqinAx4RI6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n",
        "\n",
        "    \n",
        "if torch.cuda.is_available():\n",
        "    organic_model.cuda()\n",
        "\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "cuda = True\n",
        "\n",
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pP5MpyDURI6p",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(organic_model.parameters(), lr=0.000005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JMIPvcfLRI6s"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fCpkBJ_VRI6t",
        "colab": {}
      },
      "source": [
        "model_chk = './organic_checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "920774ca-9628-4585-e51e-d517aef5883e",
        "id": "kqKQByqURI6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "organic_model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 20, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(20, 50, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=50, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f3af44a4-5369-41c7-b15f-9dd95fe4fc23",
        "id": "hKz3qsiwRI63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emd_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 1000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emd_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emd_val, lab_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [1000/4687], Loss: 1.5408, Acc: 0.00\n",
            "Epoch [1/50], Step [2000/4687], Loss: 1.4120, Acc: 0.00\n",
            "Epoch [1/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/50], Step [4000/4687], Loss: 1.5485, Acc: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  44.68599033816425\n",
            "Epoch [2/50], Step [1000/4687], Loss: 1.5240, Acc: 0.00\n",
            "Epoch [2/50], Step [2000/4687], Loss: 1.3986, Acc: 0.00\n",
            "Epoch [2/50], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/50], Step [4000/4687], Loss: 1.5489, Acc: 0.00\n",
            "Validation Accuracy :  46.13526570048309\n",
            "Epoch [3/50], Step [1000/4687], Loss: 1.4780, Acc: 0.00\n",
            "Epoch [3/50], Step [2000/4687], Loss: 1.3856, Acc: 0.00\n",
            "Epoch [3/50], Step [3000/4687], Loss: 1.5481, Acc: 0.00\n",
            "Epoch [3/50], Step [4000/4687], Loss: 1.5491, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [4/50], Step [1000/4687], Loss: 1.4297, Acc: 0.00\n",
            "Epoch [4/50], Step [2000/4687], Loss: 1.3703, Acc: 0.00\n",
            "Epoch [4/50], Step [3000/4687], Loss: 1.4859, Acc: 0.00\n",
            "Epoch [4/50], Step [4000/4687], Loss: 1.5486, Acc: 0.00\n",
            "Validation Accuracy :  47.34299516908212\n",
            "Epoch [5/50], Step [1000/4687], Loss: 1.3302, Acc: 0.00\n",
            "Epoch [5/50], Step [2000/4687], Loss: 1.3626, Acc: 0.00\n",
            "Epoch [5/50], Step [3000/4687], Loss: 0.9700, Acc: 0.00\n",
            "Epoch [5/50], Step [4000/4687], Loss: 1.5478, Acc: 0.00\n",
            "Validation Accuracy :  46.3768115942029\n",
            "Epoch [6/50], Step [1000/4687], Loss: 1.2003, Acc: 0.00\n",
            "Epoch [6/50], Step [2000/4687], Loss: 1.3615, Acc: 0.00\n",
            "Epoch [6/50], Step [3000/4687], Loss: 0.6251, Acc: 1.00\n",
            "Epoch [6/50], Step [4000/4687], Loss: 1.5462, Acc: 0.00\n",
            "Validation Accuracy :  48.06763285024155\n",
            "Epoch [7/50], Step [1000/4687], Loss: 1.0802, Acc: 0.00\n",
            "Epoch [7/50], Step [2000/4687], Loss: 1.3657, Acc: 0.00\n",
            "Epoch [7/50], Step [3000/4687], Loss: 0.5731, Acc: 1.00\n",
            "Epoch [7/50], Step [4000/4687], Loss: 1.5440, Acc: 0.00\n",
            "Validation Accuracy :  48.06763285024155\n",
            "Epoch [8/50], Step [1000/4687], Loss: 0.9827, Acc: 0.00\n",
            "Epoch [8/50], Step [2000/4687], Loss: 1.3727, Acc: 0.00\n",
            "Epoch [8/50], Step [3000/4687], Loss: 0.5630, Acc: 1.00\n",
            "Epoch [8/50], Step [4000/4687], Loss: 1.5407, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [9/50], Step [1000/4687], Loss: 0.9019, Acc: 1.00\n",
            "Epoch [9/50], Step [2000/4687], Loss: 1.3819, Acc: 0.00\n",
            "Epoch [9/50], Step [3000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [9/50], Step [4000/4687], Loss: 1.5360, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [10/50], Step [1000/4687], Loss: 0.8404, Acc: 1.00\n",
            "Epoch [10/50], Step [2000/4687], Loss: 1.3915, Acc: 0.00\n",
            "Epoch [10/50], Step [3000/4687], Loss: 0.5573, Acc: 1.00\n",
            "Epoch [10/50], Step [4000/4687], Loss: 1.5325, Acc: 0.00\n",
            "Validation Accuracy :  48.06763285024155\n",
            "Epoch [11/50], Step [1000/4687], Loss: 0.7840, Acc: 1.00\n",
            "Epoch [11/50], Step [2000/4687], Loss: 1.4003, Acc: 0.00\n",
            "Epoch [11/50], Step [3000/4687], Loss: 0.5554, Acc: 1.00\n",
            "Epoch [11/50], Step [4000/4687], Loss: 1.5316, Acc: 0.00\n",
            "Validation Accuracy :  48.792270531400966\n",
            "Epoch [12/50], Step [1000/4687], Loss: 0.7383, Acc: 1.00\n",
            "Epoch [12/50], Step [2000/4687], Loss: 1.4085, Acc: 0.00\n",
            "Epoch [12/50], Step [3000/4687], Loss: 0.5555, Acc: 1.00\n",
            "Epoch [12/50], Step [4000/4687], Loss: 1.5318, Acc: 0.00\n",
            "Validation Accuracy :  48.06763285024155\n",
            "Epoch [13/50], Step [1000/4687], Loss: 0.7217, Acc: 1.00\n",
            "Epoch [13/50], Step [2000/4687], Loss: 1.4159, Acc: 0.00\n",
            "Epoch [13/50], Step [3000/4687], Loss: 0.5572, Acc: 1.00\n",
            "Epoch [13/50], Step [4000/4687], Loss: 1.5317, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [14/50], Step [1000/4687], Loss: 0.7083, Acc: 1.00\n",
            "Epoch [14/50], Step [2000/4687], Loss: 1.4211, Acc: 0.00\n",
            "Epoch [14/50], Step [3000/4687], Loss: 0.5591, Acc: 1.00\n",
            "Epoch [14/50], Step [4000/4687], Loss: 1.5322, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [15/50], Step [1000/4687], Loss: 0.7001, Acc: 1.00\n",
            "Epoch [15/50], Step [2000/4687], Loss: 1.4269, Acc: 0.00\n",
            "Epoch [15/50], Step [3000/4687], Loss: 0.5603, Acc: 1.00\n",
            "Epoch [15/50], Step [4000/4687], Loss: 1.5315, Acc: 0.00\n",
            "Validation Accuracy :  48.792270531400966\n",
            "Epoch [16/50], Step [1000/4687], Loss: 0.6937, Acc: 1.00\n",
            "Epoch [16/50], Step [2000/4687], Loss: 1.4311, Acc: 0.00\n",
            "Epoch [16/50], Step [3000/4687], Loss: 0.5607, Acc: 1.00\n",
            "Epoch [16/50], Step [4000/4687], Loss: 1.5312, Acc: 0.00\n",
            "Validation Accuracy :  48.792270531400966\n",
            "Epoch [17/50], Step [1000/4687], Loss: 0.6906, Acc: 1.00\n",
            "Epoch [17/50], Step [2000/4687], Loss: 1.4330, Acc: 0.00\n",
            "Epoch [17/50], Step [3000/4687], Loss: 0.5624, Acc: 1.00\n",
            "Epoch [17/50], Step [4000/4687], Loss: 1.5316, Acc: 0.00\n",
            "Validation Accuracy :  48.792270531400966\n",
            "Epoch [18/50], Step [1000/4687], Loss: 0.6912, Acc: 1.00\n",
            "Epoch [18/50], Step [2000/4687], Loss: 1.4326, Acc: 0.00\n",
            "Epoch [18/50], Step [3000/4687], Loss: 0.5634, Acc: 1.00\n",
            "Epoch [18/50], Step [4000/4687], Loss: 1.5318, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [19/50], Step [1000/4687], Loss: 0.6938, Acc: 1.00\n",
            "Epoch [19/50], Step [2000/4687], Loss: 1.4315, Acc: 0.00\n",
            "Epoch [19/50], Step [3000/4687], Loss: 0.5635, Acc: 1.00\n",
            "Epoch [19/50], Step [4000/4687], Loss: 1.5322, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [20/50], Step [1000/4687], Loss: 0.6959, Acc: 1.00\n",
            "Epoch [20/50], Step [2000/4687], Loss: 1.4290, Acc: 0.00\n",
            "Epoch [20/50], Step [3000/4687], Loss: 0.5641, Acc: 1.00\n",
            "Epoch [20/50], Step [4000/4687], Loss: 1.5309, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [21/50], Step [1000/4687], Loss: 0.6991, Acc: 1.00\n",
            "Epoch [21/50], Step [2000/4687], Loss: 1.4259, Acc: 0.00\n",
            "Epoch [21/50], Step [3000/4687], Loss: 0.5636, Acc: 1.00\n",
            "Epoch [21/50], Step [4000/4687], Loss: 1.5303, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [22/50], Step [1000/4687], Loss: 0.7030, Acc: 1.00\n",
            "Epoch [22/50], Step [2000/4687], Loss: 1.4218, Acc: 0.00\n",
            "Epoch [22/50], Step [3000/4687], Loss: 0.5627, Acc: 1.00\n",
            "Epoch [22/50], Step [4000/4687], Loss: 1.5297, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [23/50], Step [1000/4687], Loss: 0.7072, Acc: 1.00\n",
            "Epoch [23/50], Step [2000/4687], Loss: 1.4169, Acc: 0.00\n",
            "Epoch [23/50], Step [3000/4687], Loss: 0.5613, Acc: 1.00\n",
            "Epoch [23/50], Step [4000/4687], Loss: 1.5293, Acc: 0.00\n",
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [24/50], Step [1000/4687], Loss: 0.7122, Acc: 1.00\n",
            "Epoch [24/50], Step [2000/4687], Loss: 1.4112, Acc: 0.00\n",
            "Epoch [24/50], Step [3000/4687], Loss: 0.5604, Acc: 1.00\n",
            "Epoch [24/50], Step [4000/4687], Loss: 1.5289, Acc: 0.00\n",
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [25/50], Step [1000/4687], Loss: 0.7188, Acc: 1.00\n",
            "Epoch [25/50], Step [2000/4687], Loss: 1.4049, Acc: 0.00\n",
            "Epoch [25/50], Step [3000/4687], Loss: 0.5597, Acc: 1.00\n",
            "Epoch [25/50], Step [4000/4687], Loss: 1.5282, Acc: 0.00\n",
            "Validation Accuracy :  49.275362318840585\n",
            "Epoch [26/50], Step [1000/4687], Loss: 0.7263, Acc: 1.00\n",
            "Epoch [26/50], Step [2000/4687], Loss: 1.3984, Acc: 0.00\n",
            "Epoch [26/50], Step [3000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [26/50], Step [4000/4687], Loss: 1.5269, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [27/50], Step [1000/4687], Loss: 0.7336, Acc: 1.00\n",
            "Epoch [27/50], Step [2000/4687], Loss: 1.3924, Acc: 0.00\n",
            "Epoch [27/50], Step [3000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [27/50], Step [4000/4687], Loss: 1.5260, Acc: 0.00\n",
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [28/50], Step [1000/4687], Loss: 0.7414, Acc: 1.00\n",
            "Epoch [28/50], Step [2000/4687], Loss: 1.3864, Acc: 0.00\n",
            "Epoch [28/50], Step [3000/4687], Loss: 0.5600, Acc: 1.00\n",
            "Epoch [28/50], Step [4000/4687], Loss: 1.5262, Acc: 0.00\n",
            "Validation Accuracy :  50.0\n",
            "Epoch [29/50], Step [1000/4687], Loss: 0.7506, Acc: 1.00\n",
            "Epoch [29/50], Step [2000/4687], Loss: 1.3809, Acc: 0.00\n",
            "Epoch [29/50], Step [3000/4687], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [29/50], Step [4000/4687], Loss: 1.5260, Acc: 0.00\n",
            "Validation Accuracy :  50.48309178743962\n",
            "Epoch [30/50], Step [1000/4687], Loss: 0.7578, Acc: 1.00\n",
            "Epoch [30/50], Step [2000/4687], Loss: 1.3783, Acc: 0.00\n",
            "Epoch [30/50], Step [3000/4687], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [30/50], Step [4000/4687], Loss: 1.5268, Acc: 0.00\n",
            "Validation Accuracy :  50.48309178743962\n",
            "Epoch [31/50], Step [1000/4687], Loss: 0.7636, Acc: 1.00\n",
            "Epoch [31/50], Step [2000/4687], Loss: 1.3770, Acc: 0.00\n",
            "Epoch [31/50], Step [3000/4687], Loss: 0.5602, Acc: 1.00\n",
            "Epoch [31/50], Step [4000/4687], Loss: 1.5273, Acc: 0.00\n",
            "Validation Accuracy :  51.449275362318836\n",
            "Epoch [32/50], Step [1000/4687], Loss: 0.7681, Acc: 1.00\n",
            "Epoch [32/50], Step [2000/4687], Loss: 1.3781, Acc: 0.00\n",
            "Epoch [32/50], Step [3000/4687], Loss: 0.5593, Acc: 1.00\n",
            "Epoch [32/50], Step [4000/4687], Loss: 1.5281, Acc: 0.00\n",
            "Validation Accuracy :  50.96618357487923\n",
            "Epoch [33/50], Step [1000/4687], Loss: 0.7705, Acc: 1.00\n",
            "Epoch [33/50], Step [2000/4687], Loss: 1.3801, Acc: 0.00\n",
            "Epoch [33/50], Step [3000/4687], Loss: 0.5588, Acc: 1.00\n",
            "Epoch [33/50], Step [4000/4687], Loss: 1.5295, Acc: 0.00\n",
            "Validation Accuracy :  50.96618357487923\n",
            "Epoch [34/50], Step [1000/4687], Loss: 0.7712, Acc: 1.00\n",
            "Epoch [34/50], Step [2000/4687], Loss: 1.3809, Acc: 0.00\n",
            "Epoch [34/50], Step [3000/4687], Loss: 0.5585, Acc: 1.00\n",
            "Epoch [34/50], Step [4000/4687], Loss: 1.5301, Acc: 0.00\n",
            "Validation Accuracy :  51.690821256038646\n",
            "Epoch [35/50], Step [1000/4687], Loss: 0.7702, Acc: 1.00\n",
            "Epoch [35/50], Step [2000/4687], Loss: 1.3796, Acc: 0.00\n",
            "Epoch [35/50], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [35/50], Step [4000/4687], Loss: 1.5313, Acc: 0.00\n",
            "Validation Accuracy :  51.93236714975845\n",
            "Epoch [36/50], Step [1000/4687], Loss: 0.7681, Acc: 1.00\n",
            "Epoch [36/50], Step [2000/4687], Loss: 1.3777, Acc: 0.00\n",
            "Epoch [36/50], Step [3000/4687], Loss: 0.5574, Acc: 1.00\n",
            "Epoch [36/50], Step [4000/4687], Loss: 1.5322, Acc: 0.00\n",
            "Validation Accuracy :  51.93236714975845\n",
            "Epoch [37/50], Step [1000/4687], Loss: 0.7658, Acc: 1.00\n",
            "Epoch [37/50], Step [2000/4687], Loss: 1.3760, Acc: 0.00\n",
            "Epoch [37/50], Step [3000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [37/50], Step [4000/4687], Loss: 1.5331, Acc: 0.00\n",
            "Validation Accuracy :  52.41545893719807\n",
            "Epoch [38/50], Step [1000/4687], Loss: 0.7625, Acc: 1.00\n",
            "Epoch [38/50], Step [2000/4687], Loss: 1.3735, Acc: 0.00\n",
            "Epoch [38/50], Step [3000/4687], Loss: 0.5565, Acc: 1.00\n",
            "Epoch [38/50], Step [4000/4687], Loss: 1.5342, Acc: 0.00\n",
            "Validation Accuracy :  52.65700483091788\n",
            "Epoch [39/50], Step [1000/4687], Loss: 0.7579, Acc: 1.00\n",
            "Epoch [39/50], Step [2000/4687], Loss: 1.3678, Acc: 0.00\n",
            "Epoch [39/50], Step [3000/4687], Loss: 0.5561, Acc: 1.00\n",
            "Epoch [39/50], Step [4000/4687], Loss: 1.5352, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [40/50], Step [1000/4687], Loss: 0.7520, Acc: 1.00\n",
            "Epoch [40/50], Step [2000/4687], Loss: 1.3636, Acc: 0.00\n",
            "Epoch [40/50], Step [3000/4687], Loss: 0.5559, Acc: 1.00\n",
            "Epoch [40/50], Step [4000/4687], Loss: 1.5356, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [41/50], Step [1000/4687], Loss: 0.7467, Acc: 1.00\n",
            "Epoch [41/50], Step [2000/4687], Loss: 1.3599, Acc: 0.00\n",
            "Epoch [41/50], Step [3000/4687], Loss: 0.5560, Acc: 1.00\n",
            "Epoch [41/50], Step [4000/4687], Loss: 1.5354, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [42/50], Step [1000/4687], Loss: 0.7408, Acc: 1.00\n",
            "Epoch [42/50], Step [2000/4687], Loss: 1.3562, Acc: 0.00\n",
            "Epoch [42/50], Step [3000/4687], Loss: 0.5563, Acc: 1.00\n",
            "Epoch [42/50], Step [4000/4687], Loss: 1.5350, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [43/50], Step [1000/4687], Loss: 0.7336, Acc: 1.00\n",
            "Epoch [43/50], Step [2000/4687], Loss: 1.3522, Acc: 0.00\n",
            "Epoch [43/50], Step [3000/4687], Loss: 0.5564, Acc: 1.00\n",
            "Epoch [43/50], Step [4000/4687], Loss: 1.5342, Acc: 0.00\n",
            "Validation Accuracy :  53.62318840579711\n",
            "Epoch [44/50], Step [1000/4687], Loss: 0.7269, Acc: 1.00\n",
            "Epoch [44/50], Step [2000/4687], Loss: 1.3469, Acc: 0.00\n",
            "Epoch [44/50], Step [3000/4687], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [44/50], Step [4000/4687], Loss: 1.5330, Acc: 0.00\n",
            "Validation Accuracy :  53.62318840579711\n",
            "Epoch [45/50], Step [1000/4687], Loss: 0.7203, Acc: 1.00\n",
            "Epoch [45/50], Step [2000/4687], Loss: 1.3407, Acc: 0.00\n",
            "Epoch [45/50], Step [3000/4687], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [45/50], Step [4000/4687], Loss: 1.5312, Acc: 0.00\n",
            "Validation Accuracy :  53.62318840579711\n",
            "Epoch [46/50], Step [1000/4687], Loss: 0.7134, Acc: 1.00\n",
            "Epoch [46/50], Step [2000/4687], Loss: 1.3333, Acc: 0.00\n",
            "Epoch [46/50], Step [3000/4687], Loss: 0.5575, Acc: 1.00\n",
            "Epoch [46/50], Step [4000/4687], Loss: 1.5290, Acc: 0.00\n",
            "Validation Accuracy :  53.62318840579711\n",
            "Epoch [47/50], Step [1000/4687], Loss: 0.7073, Acc: 1.00\n",
            "Epoch [47/50], Step [2000/4687], Loss: 1.3259, Acc: 0.00\n",
            "Epoch [47/50], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [47/50], Step [4000/4687], Loss: 1.5274, Acc: 0.00\n",
            "Validation Accuracy :  53.864734299516904\n",
            "Epoch [48/50], Step [1000/4687], Loss: 0.7011, Acc: 1.00\n",
            "Epoch [48/50], Step [2000/4687], Loss: 1.3179, Acc: 0.00\n",
            "Epoch [48/50], Step [3000/4687], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [48/50], Step [4000/4687], Loss: 1.5261, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [49/50], Step [1000/4687], Loss: 0.6953, Acc: 1.00\n",
            "Epoch [49/50], Step [2000/4687], Loss: 1.3079, Acc: 0.00\n",
            "Epoch [49/50], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [49/50], Step [4000/4687], Loss: 1.5252, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [50/50], Step [1000/4687], Loss: 0.6907, Acc: 1.00\n",
            "Epoch [50/50], Step [2000/4687], Loss: 1.2978, Acc: 0.00\n",
            "Epoch [50/50], Step [3000/4687], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [50/50], Step [4000/4687], Loss: 1.5250, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgDE-s9ARI67"
      },
      "source": [
        "####Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HI4LutZRI69",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "og2etYrGRI7A"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65706cdc-1939-4d09-a55f-3b5252870ee8",
        "id": "-8-qXPd-RI7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "55.00000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_J0b12RoRI7F"
      },
      "source": [
        "####Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "22694e30-e643-42f8-f2fc-babbd1f89041",
        "id": "AGZUeagjRI7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "\n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[0.2580, 0.7337, 0.0083]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[9.8988e-01, 1.0116e-02, 6.0260e-12]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[7.9797e-02, 9.2004e-01, 1.6028e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[0.0155, 0.0125, 0.9720]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[0.0441, 0.9370, 0.0189]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[0.0171, 0.8960, 0.0869]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[0.0357, 0.5203, 0.4440]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[5.5643e-10, 2.2311e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.7716e-02, 9.8228e-01, 6.4676e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[1.0000e+00, 2.1361e-06, 1.5725e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[2.3001e-08, 1.7880e-04, 9.9982e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[2.3001e-08, 1.7880e-04, 9.9982e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[2.3001e-08, 1.7880e-04, 9.9982e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[2.3001e-08, 1.7880e-04, 9.9982e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[0.0178, 0.8962, 0.0861]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[0.0162, 0.9058, 0.0780]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[6.2238e-04, 9.9877e-01, 6.0662e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[1.0000e+00, 2.3944e-08, 1.2838e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[1.4892e-05, 9.9964e-01, 3.4019e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[0.0189, 0.8935, 0.0876]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[4.0955e-01, 5.9038e-01, 7.1968e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[0.0260, 0.9704, 0.0037]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[0.7882, 0.2091, 0.0026]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[5.0250e-04, 7.6322e-01, 2.3628e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[0.8851, 0.0692, 0.0457]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[0.4259, 0.1322, 0.4419]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[0.0747, 0.6728, 0.2525]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[0.0484, 0.9237, 0.0279]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : tensor([[0.3066, 0.1368, 0.5566]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[9.9345e-01, 6.5468e-03, 8.1188e-11]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[4.6170e-04, 8.7475e-01, 1.2479e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[7.7950e-01, 2.2049e-01, 7.1879e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[7.8700e-04, 9.9921e-01, 2.3486e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[4.8166e-03, 9.9518e-01, 6.8361e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[1.8123e-12, 8.2929e-10, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[1.0000e+00, 6.4864e-10, 5.1103e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[3.0079e-04, 9.9644e-01, 3.2641e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[9.5926e-03, 9.9038e-01, 2.5792e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[0.0856, 0.6452, 0.2692]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[5.3237e-01, 4.6760e-01, 2.3669e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[1.9459e-02, 9.8054e-01, 1.6376e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[8.9524e-01, 1.0476e-01, 2.3580e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[9.5803e-01, 4.1887e-02, 8.7881e-05]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[0.0201, 0.8363, 0.1436]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[1.7634e-04, 9.9177e-01, 8.0534e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[0.1349, 0.6939, 0.1712]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[7.7833e-02, 9.2217e-01, 8.7569e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[9.9526e-01, 4.7417e-03, 3.4822e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[0.3880, 0.5600, 0.0520]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[0.0199, 0.9788, 0.0013]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2AG35HARI7N"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TD1J4BKzRI7N",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65cf2e9e-1034-4ce0-dded-3969012a67df",
        "id": "3P8k8wLTRI7S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = '_organicTF50epoch'+str(model_num)+'.pt'\n",
        "PATH0 = '_organicTF50_epoch_model'+str(model_num)+'.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ezElQsxRI7U"
      },
      "source": [
        "####Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2cfe926c-3fab-47a3-e69c-d790bd01a679",
        "id": "Y_cwb3eXRI7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.55; \n",
            "F1-Score (Micro): 0.55; \n",
            "F1-Score (Macro): 0.5316447252940143; \n",
            "F1-Score (Weighted): 0.5465790919706024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic1CCWAS-YVc",
        "colab_type": "text"
      },
      "source": [
        "### Train more epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R6UqvMxC-bYJ",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load('organic_checkpoint50.pt')\n",
        "organic_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_chkpoint = checkDatasrtpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xfLAYDV_zri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3VxzsJCq-bYm"
      },
      "source": [
        "#####Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b930029-51a6-4a23-a7fe-8eac98111fd8",
        "id": "MjLnQwIa-bYp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(epoch_chkpoint + 1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emd_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = organic_model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 1000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emd_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emd_val, lab_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [51/100], Step [1000/4687], Loss: 1.5174, Acc: 0.00\n",
            "Epoch [51/100], Step [2000/4687], Loss: 1.4024, Acc: 0.00\n",
            "Epoch [51/100], Step [3000/4687], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [51/100], Step [4000/4687], Loss: 1.5486, Acc: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  46.13526570048309\n",
            "Epoch [52/100], Step [1000/4687], Loss: 1.4780, Acc: 0.00\n",
            "Epoch [52/100], Step [2000/4687], Loss: 1.3898, Acc: 0.00\n",
            "Epoch [52/100], Step [3000/4687], Loss: 1.5482, Acc: 0.00\n",
            "Epoch [52/100], Step [4000/4687], Loss: 1.5489, Acc: 0.00\n",
            "Validation Accuracy :  47.58454106280193\n",
            "Epoch [53/100], Step [1000/4687], Loss: 1.4371, Acc: 0.00\n",
            "Epoch [53/100], Step [2000/4687], Loss: 1.3765, Acc: 0.00\n",
            "Epoch [53/100], Step [3000/4687], Loss: 1.4871, Acc: 0.00\n",
            "Epoch [53/100], Step [4000/4687], Loss: 1.5485, Acc: 0.00\n",
            "Validation Accuracy :  47.10144927536232\n",
            "Epoch [54/100], Step [1000/4687], Loss: 1.3539, Acc: 0.00\n",
            "Epoch [54/100], Step [2000/4687], Loss: 1.3650, Acc: 0.00\n",
            "Epoch [54/100], Step [3000/4687], Loss: 0.9627, Acc: 0.00\n",
            "Epoch [54/100], Step [4000/4687], Loss: 1.5479, Acc: 0.00\n",
            "Validation Accuracy :  46.3768115942029\n",
            "Epoch [55/100], Step [1000/4687], Loss: 1.2495, Acc: 0.00\n",
            "Epoch [55/100], Step [2000/4687], Loss: 1.3620, Acc: 0.00\n",
            "Epoch [55/100], Step [3000/4687], Loss: 0.6238, Acc: 1.00\n",
            "Epoch [55/100], Step [4000/4687], Loss: 1.5467, Acc: 0.00\n",
            "Validation Accuracy :  46.85990338164252\n",
            "Epoch [56/100], Step [1000/4687], Loss: 1.1511, Acc: 0.00\n",
            "Epoch [56/100], Step [2000/4687], Loss: 1.3621, Acc: 0.00\n",
            "Epoch [56/100], Step [3000/4687], Loss: 0.5685, Acc: 1.00\n",
            "Epoch [56/100], Step [4000/4687], Loss: 1.5452, Acc: 0.00\n",
            "Validation Accuracy :  47.10144927536232\n",
            "Epoch [57/100], Step [1000/4687], Loss: 1.0601, Acc: 0.00\n",
            "Epoch [57/100], Step [2000/4687], Loss: 1.3647, Acc: 0.00\n",
            "Epoch [57/100], Step [3000/4687], Loss: 0.5608, Acc: 1.00\n",
            "Epoch [57/100], Step [4000/4687], Loss: 1.5427, Acc: 0.00\n",
            "Validation Accuracy :  46.85990338164252\n",
            "Epoch [58/100], Step [1000/4687], Loss: 0.9792, Acc: 0.00\n",
            "Epoch [58/100], Step [2000/4687], Loss: 1.3699, Acc: 0.00\n",
            "Epoch [58/100], Step [3000/4687], Loss: 0.5572, Acc: 1.00\n",
            "Epoch [58/100], Step [4000/4687], Loss: 1.5388, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [59/100], Step [1000/4687], Loss: 0.8905, Acc: 1.00\n",
            "Epoch [59/100], Step [2000/4687], Loss: 1.3768, Acc: 0.00\n",
            "Epoch [59/100], Step [3000/4687], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [59/100], Step [4000/4687], Loss: 1.5365, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [60/100], Step [1000/4687], Loss: 0.8218, Acc: 1.00\n",
            "Epoch [60/100], Step [2000/4687], Loss: 1.3847, Acc: 0.00\n",
            "Epoch [60/100], Step [3000/4687], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [60/100], Step [4000/4687], Loss: 1.5365, Acc: 0.00\n",
            "Validation Accuracy :  48.792270531400966\n",
            "Epoch [61/100], Step [1000/4687], Loss: 0.7908, Acc: 1.00\n",
            "Epoch [61/100], Step [2000/4687], Loss: 1.3929, Acc: 0.00\n",
            "Epoch [61/100], Step [3000/4687], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [61/100], Step [4000/4687], Loss: 1.5359, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [62/100], Step [1000/4687], Loss: 0.7667, Acc: 1.00\n",
            "Epoch [62/100], Step [2000/4687], Loss: 1.4035, Acc: 0.00\n",
            "Epoch [62/100], Step [3000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [62/100], Step [4000/4687], Loss: 1.5352, Acc: 0.00\n",
            "Validation Accuracy :  47.82608695652174\n",
            "Epoch [63/100], Step [1000/4687], Loss: 0.7471, Acc: 1.00\n",
            "Epoch [63/100], Step [2000/4687], Loss: 1.4085, Acc: 0.00\n",
            "Epoch [63/100], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [63/100], Step [4000/4687], Loss: 1.5344, Acc: 0.00\n",
            "Validation Accuracy :  48.06763285024155\n",
            "Epoch [64/100], Step [1000/4687], Loss: 0.7316, Acc: 1.00\n",
            "Epoch [64/100], Step [2000/4687], Loss: 1.4151, Acc: 0.00\n",
            "Epoch [64/100], Step [3000/4687], Loss: 0.5594, Acc: 1.00\n",
            "Epoch [64/100], Step [4000/4687], Loss: 1.5326, Acc: 0.00\n",
            "Validation Accuracy :  48.55072463768116\n",
            "Epoch [65/100], Step [1000/4687], Loss: 0.7213, Acc: 1.00\n",
            "Epoch [65/100], Step [2000/4687], Loss: 1.4209, Acc: 0.00\n",
            "Epoch [65/100], Step [3000/4687], Loss: 0.5607, Acc: 1.00\n",
            "Epoch [65/100], Step [4000/4687], Loss: 1.5328, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [66/100], Step [1000/4687], Loss: 0.7148, Acc: 1.00\n",
            "Epoch [66/100], Step [2000/4687], Loss: 1.4249, Acc: 0.00\n",
            "Epoch [66/100], Step [3000/4687], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [66/100], Step [4000/4687], Loss: 1.5336, Acc: 0.00\n",
            "Validation Accuracy :  47.34299516908212\n",
            "Epoch [67/100], Step [1000/4687], Loss: 0.7107, Acc: 1.00\n",
            "Epoch [67/100], Step [2000/4687], Loss: 1.4252, Acc: 0.00\n",
            "Epoch [67/100], Step [3000/4687], Loss: 0.5605, Acc: 1.00\n",
            "Epoch [67/100], Step [4000/4687], Loss: 1.5340, Acc: 0.00\n",
            "Validation Accuracy :  48.55072463768116\n",
            "Epoch [68/100], Step [1000/4687], Loss: 0.7079, Acc: 1.00\n",
            "Epoch [68/100], Step [2000/4687], Loss: 1.4269, Acc: 0.00\n",
            "Epoch [68/100], Step [3000/4687], Loss: 0.5611, Acc: 1.00\n",
            "Epoch [68/100], Step [4000/4687], Loss: 1.5334, Acc: 0.00\n",
            "Validation Accuracy :  48.309178743961354\n",
            "Epoch [69/100], Step [1000/4687], Loss: 0.7064, Acc: 1.00\n",
            "Epoch [69/100], Step [2000/4687], Loss: 1.4258, Acc: 0.00\n",
            "Epoch [69/100], Step [3000/4687], Loss: 0.5612, Acc: 1.00\n",
            "Epoch [69/100], Step [4000/4687], Loss: 1.5323, Acc: 0.00\n",
            "Validation Accuracy :  49.275362318840585\n",
            "Epoch [70/100], Step [1000/4687], Loss: 0.7062, Acc: 1.00\n",
            "Epoch [70/100], Step [2000/4687], Loss: 1.4238, Acc: 0.00\n",
            "Epoch [70/100], Step [3000/4687], Loss: 0.5614, Acc: 1.00\n",
            "Epoch [70/100], Step [4000/4687], Loss: 1.5305, Acc: 0.00\n",
            "Validation Accuracy :  49.033816425120776\n",
            "Epoch [71/100], Step [1000/4687], Loss: 0.7075, Acc: 1.00\n",
            "Epoch [71/100], Step [2000/4687], Loss: 1.4198, Acc: 0.00\n",
            "Epoch [71/100], Step [3000/4687], Loss: 0.5612, Acc: 1.00\n",
            "Epoch [71/100], Step [4000/4687], Loss: 1.5289, Acc: 0.00\n",
            "Validation Accuracy :  49.275362318840585\n",
            "Epoch [72/100], Step [1000/4687], Loss: 0.7108, Acc: 1.00\n",
            "Epoch [72/100], Step [2000/4687], Loss: 1.4139, Acc: 0.00\n",
            "Epoch [72/100], Step [3000/4687], Loss: 0.5598, Acc: 1.00\n",
            "Epoch [72/100], Step [4000/4687], Loss: 1.5282, Acc: 0.00\n",
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [73/100], Step [1000/4687], Loss: 0.7157, Acc: 1.00\n",
            "Epoch [73/100], Step [2000/4687], Loss: 1.4068, Acc: 0.00\n",
            "Epoch [73/100], Step [3000/4687], Loss: 0.5585, Acc: 1.00\n",
            "Epoch [73/100], Step [4000/4687], Loss: 1.5273, Acc: 0.00\n",
            "Validation Accuracy :  49.275362318840585\n",
            "Epoch [74/100], Step [1000/4687], Loss: 0.7214, Acc: 1.00\n",
            "Epoch [74/100], Step [2000/4687], Loss: 1.3996, Acc: 0.00\n",
            "Epoch [74/100], Step [3000/4687], Loss: 0.5581, Acc: 1.00\n",
            "Epoch [74/100], Step [4000/4687], Loss: 1.5258, Acc: 0.00\n",
            "Validation Accuracy :  49.51690821256038\n",
            "Epoch [75/100], Step [1000/4687], Loss: 0.7284, Acc: 1.00\n",
            "Epoch [75/100], Step [2000/4687], Loss: 1.3913, Acc: 0.00\n",
            "Epoch [75/100], Step [3000/4687], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [75/100], Step [4000/4687], Loss: 1.5250, Acc: 0.00\n",
            "Validation Accuracy :  50.0\n",
            "Epoch [76/100], Step [1000/4687], Loss: 0.7353, Acc: 1.00\n",
            "Epoch [76/100], Step [2000/4687], Loss: 1.3839, Acc: 0.00\n",
            "Epoch [76/100], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [76/100], Step [4000/4687], Loss: 1.5232, Acc: 0.00\n",
            "Validation Accuracy :  50.0\n",
            "Epoch [77/100], Step [1000/4687], Loss: 0.7417, Acc: 1.00\n",
            "Epoch [77/100], Step [2000/4687], Loss: 1.3782, Acc: 0.00\n",
            "Epoch [77/100], Step [3000/4687], Loss: 0.5579, Acc: 1.00\n",
            "Epoch [77/100], Step [4000/4687], Loss: 1.5216, Acc: 0.00\n",
            "Validation Accuracy :  50.24154589371981\n",
            "Epoch [78/100], Step [1000/4687], Loss: 0.7486, Acc: 1.00\n",
            "Epoch [78/100], Step [2000/4687], Loss: 1.3739, Acc: 0.00\n",
            "Epoch [78/100], Step [3000/4687], Loss: 0.5578, Acc: 1.00\n",
            "Epoch [78/100], Step [4000/4687], Loss: 1.5208, Acc: 0.00\n",
            "Validation Accuracy :  50.24154589371981\n",
            "Epoch [79/100], Step [1000/4687], Loss: 0.7541, Acc: 1.00\n",
            "Epoch [79/100], Step [2000/4687], Loss: 1.3707, Acc: 0.00\n",
            "Epoch [79/100], Step [3000/4687], Loss: 0.5583, Acc: 1.00\n",
            "Epoch [79/100], Step [4000/4687], Loss: 1.5200, Acc: 0.00\n",
            "Validation Accuracy :  50.72463768115942\n",
            "Epoch [80/100], Step [1000/4687], Loss: 0.7579, Acc: 1.00\n",
            "Epoch [80/100], Step [2000/4687], Loss: 1.3692, Acc: 0.00\n",
            "Epoch [80/100], Step [3000/4687], Loss: 0.5586, Acc: 1.00\n",
            "Epoch [80/100], Step [4000/4687], Loss: 1.5201, Acc: 0.00\n",
            "Validation Accuracy :  51.449275362318836\n",
            "Epoch [81/100], Step [1000/4687], Loss: 0.7613, Acc: 1.00\n",
            "Epoch [81/100], Step [2000/4687], Loss: 1.3682, Acc: 0.00\n",
            "Epoch [81/100], Step [3000/4687], Loss: 0.5585, Acc: 1.00\n",
            "Epoch [81/100], Step [4000/4687], Loss: 1.5207, Acc: 0.00\n",
            "Validation Accuracy :  50.96618357487923\n",
            "Epoch [82/100], Step [1000/4687], Loss: 0.7642, Acc: 1.00\n",
            "Epoch [82/100], Step [2000/4687], Loss: 1.3693, Acc: 0.00\n",
            "Epoch [82/100], Step [3000/4687], Loss: 0.5588, Acc: 1.00\n",
            "Epoch [82/100], Step [4000/4687], Loss: 1.5215, Acc: 0.00\n",
            "Validation Accuracy :  50.96618357487923\n",
            "Epoch [83/100], Step [1000/4687], Loss: 0.7653, Acc: 1.00\n",
            "Epoch [83/100], Step [2000/4687], Loss: 1.3684, Acc: 0.00\n",
            "Epoch [83/100], Step [3000/4687], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [83/100], Step [4000/4687], Loss: 1.5228, Acc: 0.00\n",
            "Validation Accuracy :  51.449275362318836\n",
            "Epoch [84/100], Step [1000/4687], Loss: 0.7638, Acc: 1.00\n",
            "Epoch [84/100], Step [2000/4687], Loss: 1.3689, Acc: 0.00\n",
            "Epoch [84/100], Step [3000/4687], Loss: 0.5582, Acc: 1.00\n",
            "Epoch [84/100], Step [4000/4687], Loss: 1.5238, Acc: 0.00\n",
            "Validation Accuracy :  51.690821256038646\n",
            "Epoch [85/100], Step [1000/4687], Loss: 0.7620, Acc: 1.00\n",
            "Epoch [85/100], Step [2000/4687], Loss: 1.3681, Acc: 0.00\n",
            "Epoch [85/100], Step [3000/4687], Loss: 0.5580, Acc: 1.00\n",
            "Epoch [85/100], Step [4000/4687], Loss: 1.5246, Acc: 0.00\n",
            "Validation Accuracy :  51.690821256038646\n",
            "Epoch [86/100], Step [1000/4687], Loss: 0.7589, Acc: 1.00\n",
            "Epoch [86/100], Step [2000/4687], Loss: 1.3646, Acc: 0.00\n",
            "Epoch [86/100], Step [3000/4687], Loss: 0.5576, Acc: 1.00\n",
            "Epoch [86/100], Step [4000/4687], Loss: 1.5255, Acc: 0.00\n",
            "Validation Accuracy :  51.20772946859904\n",
            "Epoch [87/100], Step [1000/4687], Loss: 0.7552, Acc: 1.00\n",
            "Epoch [87/100], Step [2000/4687], Loss: 1.3592, Acc: 0.00\n",
            "Epoch [87/100], Step [3000/4687], Loss: 0.5573, Acc: 1.00\n",
            "Epoch [87/100], Step [4000/4687], Loss: 1.5260, Acc: 0.00\n",
            "Validation Accuracy :  51.93236714975845\n",
            "Epoch [88/100], Step [1000/4687], Loss: 0.7511, Acc: 1.00\n",
            "Epoch [88/100], Step [2000/4687], Loss: 1.3529, Acc: 0.00\n",
            "Epoch [88/100], Step [3000/4687], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [88/100], Step [4000/4687], Loss: 1.5262, Acc: 0.00\n",
            "Validation Accuracy :  52.65700483091788\n",
            "Epoch [89/100], Step [1000/4687], Loss: 0.7461, Acc: 1.00\n",
            "Epoch [89/100], Step [2000/4687], Loss: 1.3443, Acc: 0.00\n",
            "Epoch [89/100], Step [3000/4687], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [89/100], Step [4000/4687], Loss: 1.5263, Acc: 0.00\n",
            "Validation Accuracy :  52.89855072463768\n",
            "Epoch [90/100], Step [1000/4687], Loss: 0.7420, Acc: 1.00\n",
            "Epoch [90/100], Step [2000/4687], Loss: 1.3365, Acc: 0.00\n",
            "Epoch [90/100], Step [3000/4687], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [90/100], Step [4000/4687], Loss: 1.5264, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [91/100], Step [1000/4687], Loss: 0.7373, Acc: 1.00\n",
            "Epoch [91/100], Step [2000/4687], Loss: 1.3278, Acc: 0.00\n",
            "Epoch [91/100], Step [3000/4687], Loss: 0.5565, Acc: 1.00\n",
            "Epoch [91/100], Step [4000/4687], Loss: 1.5248, Acc: 0.00\n",
            "Validation Accuracy :  52.65700483091788\n",
            "Epoch [92/100], Step [1000/4687], Loss: 0.7324, Acc: 1.00\n",
            "Epoch [92/100], Step [2000/4687], Loss: 1.3175, Acc: 0.00\n",
            "Epoch [92/100], Step [3000/4687], Loss: 0.5568, Acc: 1.00\n",
            "Epoch [92/100], Step [4000/4687], Loss: 1.5232, Acc: 0.00\n",
            "Validation Accuracy :  52.41545893719807\n",
            "Epoch [93/100], Step [1000/4687], Loss: 0.7279, Acc: 1.00\n",
            "Epoch [93/100], Step [2000/4687], Loss: 1.3054, Acc: 0.00\n",
            "Epoch [93/100], Step [3000/4687], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [93/100], Step [4000/4687], Loss: 1.5210, Acc: 0.00\n",
            "Validation Accuracy :  52.65700483091788\n",
            "Epoch [94/100], Step [1000/4687], Loss: 0.7242, Acc: 1.00\n",
            "Epoch [94/100], Step [2000/4687], Loss: 1.2949, Acc: 0.00\n",
            "Epoch [94/100], Step [3000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [94/100], Step [4000/4687], Loss: 1.5193, Acc: 0.00\n",
            "Validation Accuracy :  52.65700483091788\n",
            "Epoch [95/100], Step [1000/4687], Loss: 0.7187, Acc: 1.00\n",
            "Epoch [95/100], Step [2000/4687], Loss: 1.2832, Acc: 0.00\n",
            "Epoch [95/100], Step [3000/4687], Loss: 0.5571, Acc: 1.00\n",
            "Epoch [95/100], Step [4000/4687], Loss: 1.5174, Acc: 0.00\n",
            "Validation Accuracy :  52.89855072463768\n",
            "Epoch [96/100], Step [1000/4687], Loss: 0.7142, Acc: 1.00\n",
            "Epoch [96/100], Step [2000/4687], Loss: 1.2711, Acc: 0.00\n",
            "Epoch [96/100], Step [3000/4687], Loss: 0.5570, Acc: 1.00\n",
            "Epoch [96/100], Step [4000/4687], Loss: 1.5164, Acc: 0.00\n",
            "Validation Accuracy :  53.14009661835749\n",
            "Epoch [97/100], Step [1000/4687], Loss: 0.7096, Acc: 1.00\n",
            "Epoch [97/100], Step [2000/4687], Loss: 1.2605, Acc: 0.00\n",
            "Epoch [97/100], Step [3000/4687], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [97/100], Step [4000/4687], Loss: 1.5164, Acc: 0.00\n",
            "Validation Accuracy :  53.864734299516904\n",
            "Epoch [98/100], Step [1000/4687], Loss: 0.7056, Acc: 1.00\n",
            "Epoch [98/100], Step [2000/4687], Loss: 1.2500, Acc: 0.00\n",
            "Epoch [98/100], Step [3000/4687], Loss: 0.5569, Acc: 1.00\n",
            "Epoch [98/100], Step [4000/4687], Loss: 1.5171, Acc: 0.00\n",
            "Validation Accuracy :  53.3816425120773\n",
            "Epoch [99/100], Step [1000/4687], Loss: 0.7003, Acc: 1.00\n",
            "Epoch [99/100], Step [2000/4687], Loss: 1.2410, Acc: 0.00\n",
            "Epoch [99/100], Step [3000/4687], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [99/100], Step [4000/4687], Loss: 1.5182, Acc: 0.00\n",
            "Validation Accuracy :  52.89855072463768\n",
            "Epoch [100/100], Step [1000/4687], Loss: 0.6952, Acc: 1.00\n",
            "Epoch [100/100], Step [2000/4687], Loss: 1.2292, Acc: 0.00\n",
            "Epoch [100/100], Step [3000/4687], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [100/100], Step [4000/4687], Loss: 1.5194, Acc: 0.00\n",
            "Validation Accuracy :  52.89855072463768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJmxICoVJl7g",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44kTNZUfJmZW"
      },
      "source": [
        "#####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65ec9542-269f-4112-b32f-d5bfb6c49c6b",
        "id": "IbgYnSGoJmZq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emd_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        " \n",
        "  prob, hat, A = organic_model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        "\n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "55.434782608695656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2sdwAW5DJmaC"
      },
      "source": [
        "#####Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "704e7291-d77d-4340-989e-7ab3884d17e9",
        "id": "zVLcbpHlJmaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5543478260869565; \n",
            "F1-Score (Micro): 0.5543478260869565; \n",
            "F1-Score (Macro): 0.536172742635288; \n",
            "F1-Score (Weighted): 0.5508453844164247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oGWuPTZRN9k",
        "colab_type": "text"
      },
      "source": [
        "##More number of filters\n",
        "\n",
        "We thought let us try to increase the number of filters, if it has any effect on the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1FC7PeiTMwr",
        "colab_type": "text"
      },
      "source": [
        "####Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AktuRMzJ83gr",
        "colab": {}
      },
      "source": [
        "def validate(emb_val, lab_val):\n",
        "  i = 0\n",
        "  correct = 0\n",
        "  y_target = list()\n",
        "  y_pred = list()\n",
        "  for data, label in zip(emb_val, lab_val):\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "    prEvaluateob, hat, A = model.forward(data)\n",
        "    y_target.append(int(label))\n",
        "    y_pred.append(hat.int().item())\n",
        "    if int(hat.item()) == label:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "  print('Validation Accuracy : ', (correct/len(lab_val)) * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5d3KfamTWzP",
        "colab_type": "text"
      },
      "source": [
        "#### Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_1uysKwP8v75",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.L = 500\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Conv1d(768, 512, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv1d(256, 256, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(128 * 1, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, 3),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = H.view(-1, 128 * 1)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "\n",
        "        A = self.attention(H)  # NxK\n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        M = torch.mm(A, H)  # KxL\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        \n",
        "        Y_hat = torch.argmax(Y_prob, dim=1).float()\n",
        "\n",
        "        return Y_prob, Y_hat, A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7aa995ed-2090-4b1a-b524-75ef17631ea9",
        "id": "y2ESB-Hq8v8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    print('GPU is ON!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is ON!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "37518f51-d95b-410b-a4b0-6c170a8aa1ed",
        "id": "CnvwylTE8v8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Attention()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "print('Init Model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZVlWMWGX8v8p",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GCTB-Joy8v8w",
        "colab": {}
      },
      "source": [
        "cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV0ZMnQJ_KlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_chk = './checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q0zIpwpH8v82",
        "colab": {}
      },
      "source": [
        "running_loss = 0\n",
        "total_train_loss = 0\n",
        "i = 0\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTmTK963_vCQ",
        "colab_type": "code",
        "outputId": "8ef2b743-278d-4215-cb4b-0a589f1f843a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Attention(\n",
              "  (feature_extractor_part1): Sequential(\n",
              "    (0): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_extractor_part2): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=500, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (attention): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=128, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=500, out_features=3, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CH8bTIu58v88"
      },
      "source": [
        "####Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e78704e-766e-4a16-c2b2-cf16d7fe6079",
        "id": "9pXux3C58v8-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "  i = 0\n",
        "  for data, label in zip(emb_train, lab_train):\n",
        "    i += 1\n",
        "    data = torch.from_numpy(data)\n",
        "    bag_label = label\n",
        "            \n",
        "    if cuda:\n",
        "      data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "                \n",
        "    d = data.reshape(*list(data.shape), 1)\n",
        "    one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "    data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass, backward pass, optimize\n",
        "    prob, hat, A = model.forward(data)\n",
        "            \n",
        "    loss_size = loss(prob, torch.argmax(bag_label,1))\n",
        "    loss_size.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss = loss_size.data\n",
        "    total_train_loss += loss_size.data\n",
        "    accuracy = (torch.argmax(bag_label,1).to(torch.float) == hat.squeeze()).float().sum()\n",
        "    \n",
        "    if (i+1) % 10000 == 0:\n",
        "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
        "               .format(epoch, epochs, i+1, len(emb_train), loss_size.item(), accuracy.item()))\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                        Tensorboard Logging                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "       # 1. Log scalar values (scalar summary)\n",
        "        info = { 'loss': loss_size.item(), 'accuracy': accuracy.item() }\n",
        "\n",
        "        for tag, value in info.items():\n",
        "            writer.add_scalar(tag, value, i+1)\n",
        "\n",
        "        # 2. Log values and gradients of the parameters (histogram summary)\n",
        "        for tag, value in model.named_parameters():\n",
        "            tag = tag.replace('.', '/')\n",
        "            writer.add_histogram(tag, value.data.cpu().numpy(), i+1)\n",
        "            writer.add_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
        "            \n",
        "            \n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'loss': loss,\n",
        "       }, model_chk+str(epoch)+'.pt')\n",
        "  \n",
        "  validate(emb_val, lab_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10000/257353], Loss: 1.4633, Acc: 0.00\n",
            "Epoch [1/20], Step [20000/257353], Loss: 0.8926, Acc: 1.00\n",
            "Epoch [1/20], Step [30000/257353], Loss: 1.5143, Acc: 0.00\n",
            "Epoch [1/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [50000/257353], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [1/20], Step [60000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [1/20], Step [70000/257353], Loss: 1.4245, Acc: 0.00\n",
            "Epoch [1/20], Step [80000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [1/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [100000/257353], Loss: 0.5528, Acc: 1.00\n",
            "Epoch [1/20], Step [110000/257353], Loss: 0.5761, Acc: 1.00\n",
            "Epoch [1/20], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [1/20], Step [130000/257353], Loss: 1.5428, Acc: 0.00\n",
            "Epoch [1/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [160000/257353], Loss: 0.8423, Acc: 1.00\n",
            "Epoch [1/20], Step [170000/257353], Loss: 1.5214, Acc: 0.00\n",
            "Epoch [1/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [1/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [200000/257353], Loss: 1.0841, Acc: 0.00\n",
            "Epoch [1/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [220000/257353], Loss: 0.5723, Acc: 1.00\n",
            "Epoch [1/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [1/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy :  69.44200000000001\n",
            "Epoch [2/20], Step [10000/257353], Loss: 1.5428, Acc: 0.00\n",
            "Epoch [2/20], Step [20000/257353], Loss: 1.2129, Acc: 0.00\n",
            "Epoch [2/20], Step [30000/257353], Loss: 1.5301, Acc: 0.00\n",
            "Epoch [2/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [50000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [2/20], Step [60000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [2/20], Step [70000/257353], Loss: 1.3407, Acc: 0.00\n",
            "Epoch [2/20], Step [80000/257353], Loss: 0.5698, Acc: 1.00\n",
            "Epoch [2/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [100000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [2/20], Step [110000/257353], Loss: 0.5544, Acc: 1.00\n",
            "Epoch [2/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [130000/257353], Loss: 1.3281, Acc: 0.00\n",
            "Epoch [2/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [160000/257353], Loss: 1.2681, Acc: 0.00\n",
            "Epoch [2/20], Step [170000/257353], Loss: 1.4581, Acc: 0.00\n",
            "Epoch [2/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [2/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [200000/257353], Loss: 0.6014, Acc: 1.00\n",
            "Epoch [2/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [2/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [2/20], Step [250000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Validation Accuracy :  71.78999999999999\n",
            "Epoch [3/20], Step [10000/257353], Loss: 1.5031, Acc: 0.00\n",
            "Epoch [3/20], Step [20000/257353], Loss: 1.0147, Acc: 0.00\n",
            "Epoch [3/20], Step [30000/257353], Loss: 1.3524, Acc: 0.00\n",
            "Epoch [3/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [3/20], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [3/20], Step [70000/257353], Loss: 0.7193, Acc: 1.00\n",
            "Epoch [3/20], Step [80000/257353], Loss: 1.0965, Acc: 0.00\n",
            "Epoch [3/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [100000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [3/20], Step [110000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [3/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [3/20], Step [130000/257353], Loss: 1.4802, Acc: 0.00\n",
            "Epoch [3/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [160000/257353], Loss: 0.6254, Acc: 1.00\n",
            "Epoch [3/20], Step [170000/257353], Loss: 1.5498, Acc: 0.00\n",
            "Epoch [3/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [3/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [200000/257353], Loss: 0.5534, Acc: 1.00\n",
            "Epoch [3/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [3/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [3/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.088\n",
            "Epoch [4/20], Step [10000/257353], Loss: 1.4840, Acc: 0.00\n",
            "Epoch [4/20], Step [20000/257353], Loss: 1.0591, Acc: 0.00\n",
            "Epoch [4/20], Step [30000/257353], Loss: 1.5441, Acc: 0.00\n",
            "Epoch [4/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [50000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [4/20], Step [60000/257353], Loss: 0.5542, Acc: 1.00\n",
            "Epoch [4/20], Step [70000/257353], Loss: 1.3996, Acc: 0.00\n",
            "Epoch [4/20], Step [80000/257353], Loss: 0.5847, Acc: 1.00\n",
            "Epoch [4/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [100000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [4/20], Step [110000/257353], Loss: 1.5388, Acc: 0.00\n",
            "Epoch [4/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [130000/257353], Loss: 1.4525, Acc: 0.00\n",
            "Epoch [4/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [160000/257353], Loss: 0.8860, Acc: 1.00\n",
            "Epoch [4/20], Step [170000/257353], Loss: 1.5072, Acc: 0.00\n",
            "Epoch [4/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [4/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [200000/257353], Loss: 0.5846, Acc: 1.00\n",
            "Epoch [4/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [220000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [4/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [4/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.914\n",
            "Epoch [5/20], Step [10000/257353], Loss: 1.4892, Acc: 0.00\n",
            "Epoch [5/20], Step [20000/257353], Loss: 0.8197, Acc: 1.00\n",
            "Epoch [5/20], Step [30000/257353], Loss: 1.5400, Acc: 0.00\n",
            "Epoch [5/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [50000/257353], Loss: 0.9290, Acc: 1.00\n",
            "Epoch [5/20], Step [60000/257353], Loss: 0.5728, Acc: 1.00\n",
            "Epoch [5/20], Step [70000/257353], Loss: 1.3810, Acc: 0.00\n",
            "Epoch [5/20], Step [80000/257353], Loss: 0.5544, Acc: 1.00\n",
            "Epoch [5/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/20], Step [110000/257353], Loss: 0.5567, Acc: 1.00\n",
            "Epoch [5/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/20], Step [130000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [5/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [160000/257353], Loss: 1.4294, Acc: 0.00\n",
            "Epoch [5/20], Step [170000/257353], Loss: 1.5466, Acc: 0.00\n",
            "Epoch [5/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [5/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [200000/257353], Loss: 0.6539, Acc: 1.00\n",
            "Epoch [5/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [5/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [5/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  69.99600000000001\n",
            "Epoch [6/20], Step [10000/257353], Loss: 1.4826, Acc: 0.00\n",
            "Epoch [6/20], Step [20000/257353], Loss: 0.8007, Acc: 1.00\n",
            "Epoch [6/20], Step [30000/257353], Loss: 1.4560, Acc: 0.00\n",
            "Epoch [6/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [50000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [6/20], Step [60000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [6/20], Step [70000/257353], Loss: 1.1156, Acc: 0.00\n",
            "Epoch [6/20], Step [80000/257353], Loss: 0.5705, Acc: 1.00\n",
            "Epoch [6/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [100000/257353], Loss: 0.5685, Acc: 1.00\n",
            "Epoch [6/20], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [6/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [130000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [160000/257353], Loss: 1.4899, Acc: 0.00\n",
            "Epoch [6/20], Step [170000/257353], Loss: 1.4616, Acc: 0.00\n",
            "Epoch [6/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [6/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [200000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [6/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [220000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [6/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [6/20], Step [250000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Validation Accuracy :  73.534\n",
            "Epoch [7/20], Step [10000/257353], Loss: 1.4916, Acc: 0.00\n",
            "Epoch [7/20], Step [20000/257353], Loss: 0.7659, Acc: 1.00\n",
            "Epoch [7/20], Step [30000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [60000/257353], Loss: 0.5725, Acc: 1.00\n",
            "Epoch [7/20], Step [70000/257353], Loss: 1.5461, Acc: 0.00\n",
            "Epoch [7/20], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [100000/257353], Loss: 0.5546, Acc: 1.00\n",
            "Epoch [7/20], Step [110000/257353], Loss: 0.9686, Acc: 1.00\n",
            "Epoch [7/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [130000/257353], Loss: 1.5511, Acc: 0.00\n",
            "Epoch [7/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [160000/257353], Loss: 0.5652, Acc: 1.00\n",
            "Epoch [7/20], Step [170000/257353], Loss: 1.5491, Acc: 0.00\n",
            "Epoch [7/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [7/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [200000/257353], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [7/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [7/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [7/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  73.038\n",
            "Epoch [8/20], Step [10000/257353], Loss: 1.4888, Acc: 0.00\n",
            "Epoch [8/20], Step [20000/257353], Loss: 0.6828, Acc: 1.00\n",
            "Epoch [8/20], Step [30000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [70000/257353], Loss: 1.4129, Acc: 0.00\n",
            "Epoch [8/20], Step [80000/257353], Loss: 0.5520, Acc: 1.00\n",
            "Epoch [8/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [100000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [8/20], Step [110000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [8/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [130000/257353], Loss: 1.5462, Acc: 0.00\n",
            "Epoch [8/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [160000/257353], Loss: 1.1132, Acc: 0.00\n",
            "Epoch [8/20], Step [170000/257353], Loss: 1.5408, Acc: 0.00\n",
            "Epoch [8/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [8/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [200000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [8/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [8/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.264\n",
            "Epoch [9/20], Step [10000/257353], Loss: 1.5066, Acc: 0.00\n",
            "Epoch [9/20], Step [20000/257353], Loss: 0.5808, Acc: 1.00\n",
            "Epoch [9/20], Step [30000/257353], Loss: 1.5336, Acc: 0.00\n",
            "Epoch [9/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [50000/257353], Loss: 0.5526, Acc: 1.00\n",
            "Epoch [9/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [9/20], Step [70000/257353], Loss: 0.9265, Acc: 1.00\n",
            "Epoch [9/20], Step [80000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [100000/257353], Loss: 0.5548, Acc: 1.00\n",
            "Epoch [9/20], Step [110000/257353], Loss: 0.5633, Acc: 1.00\n",
            "Epoch [9/20], Step [120000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [9/20], Step [130000/257353], Loss: 1.5494, Acc: 0.00\n",
            "Epoch [9/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [160000/257353], Loss: 0.5599, Acc: 1.00\n",
            "Epoch [9/20], Step [170000/257353], Loss: 1.5137, Acc: 0.00\n",
            "Epoch [9/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [9/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [200000/257353], Loss: 0.5928, Acc: 1.00\n",
            "Epoch [9/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [9/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.378\n",
            "Epoch [10/20], Step [10000/257353], Loss: 1.5157, Acc: 0.00\n",
            "Epoch [10/20], Step [20000/257353], Loss: 0.6375, Acc: 1.00\n",
            "Epoch [10/20], Step [30000/257353], Loss: 1.5508, Acc: 0.00\n",
            "Epoch [10/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [60000/257353], Loss: 0.5529, Acc: 1.00\n",
            "Epoch [10/20], Step [70000/257353], Loss: 1.5476, Acc: 0.00\n",
            "Epoch [10/20], Step [80000/257353], Loss: 0.5713, Acc: 1.00\n",
            "Epoch [10/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [100000/257353], Loss: 0.5519, Acc: 1.00\n",
            "Epoch [10/20], Step [110000/257353], Loss: 0.5536, Acc: 1.00\n",
            "Epoch [10/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [130000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [160000/257353], Loss: 0.6349, Acc: 1.00\n",
            "Epoch [10/20], Step [170000/257353], Loss: 1.4995, Acc: 0.00\n",
            "Epoch [10/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [10/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [200000/257353], Loss: 0.8841, Acc: 1.00\n",
            "Epoch [10/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [10/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [10/20], Step [250000/257353], Loss: 0.5558, Acc: 1.00\n",
            "Validation Accuracy :  72.08\n",
            "Epoch [11/20], Step [10000/257353], Loss: 1.5213, Acc: 0.00\n",
            "Epoch [11/20], Step [20000/257353], Loss: 0.6467, Acc: 1.00\n",
            "Epoch [11/20], Step [30000/257353], Loss: 1.4368, Acc: 0.00\n",
            "Epoch [11/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/20], Step [70000/257353], Loss: 1.1289, Acc: 0.00\n",
            "Epoch [11/20], Step [80000/257353], Loss: 0.5521, Acc: 1.00\n",
            "Epoch [11/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [100000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [11/20], Step [110000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [11/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [130000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [160000/257353], Loss: 0.8259, Acc: 1.00\n",
            "Epoch [11/20], Step [170000/257353], Loss: 1.5512, Acc: 0.00\n",
            "Epoch [11/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [11/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [200000/257353], Loss: 0.5699, Acc: 1.00\n",
            "Epoch [11/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [220000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [11/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [11/20], Step [250000/257353], Loss: 0.5554, Acc: 1.00\n",
            "Validation Accuracy :  74.188\n",
            "Epoch [12/20], Step [10000/257353], Loss: 1.5191, Acc: 0.00\n",
            "Epoch [12/20], Step [20000/257353], Loss: 1.0987, Acc: 0.00\n",
            "Epoch [12/20], Step [30000/257353], Loss: 0.9610, Acc: 0.00\n",
            "Epoch [12/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [50000/257353], Loss: 0.5533, Acc: 1.00\n",
            "Epoch [12/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/20], Step [70000/257353], Loss: 1.5476, Acc: 0.00\n",
            "Epoch [12/20], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [100000/257353], Loss: 0.5531, Acc: 1.00\n",
            "Epoch [12/20], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [12/20], Step [130000/257353], Loss: 0.9334, Acc: 1.00\n",
            "Epoch [12/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [160000/257353], Loss: 1.5487, Acc: 0.00\n",
            "Epoch [12/20], Step [170000/257353], Loss: 1.5261, Acc: 0.00\n",
            "Epoch [12/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [12/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [200000/257353], Loss: 0.5696, Acc: 1.00\n",
            "Epoch [12/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [12/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [12/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  73.60600000000001\n",
            "Epoch [13/20], Step [10000/257353], Loss: 1.4974, Acc: 0.00\n",
            "Epoch [13/20], Step [20000/257353], Loss: 0.7748, Acc: 1.00\n",
            "Epoch [13/20], Step [30000/257353], Loss: 1.3187, Acc: 0.00\n",
            "Epoch [13/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [50000/257353], Loss: 0.5659, Acc: 1.00\n",
            "Epoch [13/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [70000/257353], Loss: 1.5412, Acc: 0.00\n",
            "Epoch [13/20], Step [80000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [13/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [100000/257353], Loss: 0.5577, Acc: 1.00\n",
            "Epoch [13/20], Step [110000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [13/20], Step [130000/257353], Loss: 0.8126, Acc: 1.00\n",
            "Epoch [13/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [160000/257353], Loss: 0.5591, Acc: 1.00\n",
            "Epoch [13/20], Step [170000/257353], Loss: 1.5479, Acc: 0.00\n",
            "Epoch [13/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [13/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [200000/257353], Loss: 0.5739, Acc: 1.00\n",
            "Epoch [13/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [220000/257353], Loss: 0.5562, Acc: 1.00\n",
            "Epoch [13/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [13/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.678\n",
            "Epoch [14/20], Step [10000/257353], Loss: 1.5404, Acc: 0.00\n",
            "Epoch [14/20], Step [20000/257353], Loss: 1.4795, Acc: 0.00\n",
            "Epoch [14/20], Step [30000/257353], Loss: 1.1151, Acc: 0.00\n",
            "Epoch [14/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [14/20], Step [60000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [14/20], Step [70000/257353], Loss: 1.5513, Acc: 0.00\n",
            "Epoch [14/20], Step [80000/257353], Loss: 0.5692, Acc: 1.00\n",
            "Epoch [14/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [100000/257353], Loss: 0.5649, Acc: 1.00\n",
            "Epoch [14/20], Step [110000/257353], Loss: 0.5545, Acc: 1.00\n",
            "Epoch [14/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [130000/257353], Loss: 1.5458, Acc: 0.00\n",
            "Epoch [14/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [160000/257353], Loss: 0.5556, Acc: 1.00\n",
            "Epoch [14/20], Step [170000/257353], Loss: 1.5353, Acc: 0.00\n",
            "Epoch [14/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [14/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [200000/257353], Loss: 1.4206, Acc: 0.00\n",
            "Epoch [14/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [220000/257353], Loss: 0.5535, Acc: 1.00\n",
            "Epoch [14/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [14/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  72.18599999999999\n",
            "Epoch [15/20], Step [10000/257353], Loss: 1.4608, Acc: 0.00\n",
            "Epoch [15/20], Step [20000/257353], Loss: 0.5983, Acc: 1.00\n",
            "Epoch [15/20], Step [30000/257353], Loss: 1.1544, Acc: 0.00\n",
            "Epoch [15/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [50000/257353], Loss: 0.5522, Acc: 1.00\n",
            "Epoch [15/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/20], Step [70000/257353], Loss: 1.2891, Acc: 0.00\n",
            "Epoch [15/20], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [15/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [100000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [15/20], Step [110000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [15/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [130000/257353], Loss: 0.5897, Acc: 1.00\n",
            "Epoch [15/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [160000/257353], Loss: 0.6308, Acc: 1.00\n",
            "Epoch [15/20], Step [170000/257353], Loss: 1.5396, Acc: 0.00\n",
            "Epoch [15/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [15/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [200000/257353], Loss: 0.6920, Acc: 1.00\n",
            "Epoch [15/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [220000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [15/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [15/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  73.218\n",
            "Epoch [16/20], Step [10000/257353], Loss: 1.4585, Acc: 0.00\n",
            "Epoch [16/20], Step [20000/257353], Loss: 1.3814, Acc: 0.00\n",
            "Epoch [16/20], Step [30000/257353], Loss: 0.5655, Acc: 1.00\n",
            "Epoch [16/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [70000/257353], Loss: 0.7843, Acc: 1.00\n",
            "Epoch [16/20], Step [80000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [16/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [100000/257353], Loss: 0.5517, Acc: 1.00\n",
            "Epoch [16/20], Step [110000/257353], Loss: 0.5527, Acc: 1.00\n",
            "Epoch [16/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [16/20], Step [130000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [160000/257353], Loss: 0.6555, Acc: 1.00\n",
            "Epoch [16/20], Step [170000/257353], Loss: 1.5412, Acc: 0.00\n",
            "Epoch [16/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [16/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [200000/257353], Loss: 0.6767, Acc: 1.00\n",
            "Epoch [16/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [220000/257353], Loss: 0.5523, Acc: 1.00\n",
            "Epoch [16/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [16/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  72.696\n",
            "Epoch [17/20], Step [10000/257353], Loss: 0.6084, Acc: 1.00\n",
            "Epoch [17/20], Step [20000/257353], Loss: 1.4706, Acc: 0.00\n",
            "Epoch [17/20], Step [30000/257353], Loss: 0.9178, Acc: 1.00\n",
            "Epoch [17/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [50000/257353], Loss: 0.5550, Acc: 1.00\n",
            "Epoch [17/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/20], Step [70000/257353], Loss: 1.4623, Acc: 0.00\n",
            "Epoch [17/20], Step [80000/257353], Loss: 0.5680, Acc: 1.00\n",
            "Epoch [17/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/20], Step [110000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [17/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [130000/257353], Loss: 1.5497, Acc: 0.00\n",
            "Epoch [17/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [160000/257353], Loss: 0.9971, Acc: 0.00\n",
            "Epoch [17/20], Step [170000/257353], Loss: 1.5457, Acc: 0.00\n",
            "Epoch [17/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [17/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [200000/257353], Loss: 1.1552, Acc: 0.00\n",
            "Epoch [17/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [17/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [17/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  74.202\n",
            "Epoch [18/20], Step [10000/257353], Loss: 1.5293, Acc: 0.00\n",
            "Epoch [18/20], Step [20000/257353], Loss: 1.3588, Acc: 0.00\n",
            "Epoch [18/20], Step [30000/257353], Loss: 0.6124, Acc: 1.00\n",
            "Epoch [18/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [50000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [18/20], Step [60000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [70000/257353], Loss: 1.5236, Acc: 0.00\n",
            "Epoch [18/20], Step [80000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [18/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [100000/257353], Loss: 0.5601, Acc: 1.00\n",
            "Epoch [18/20], Step [110000/257353], Loss: 0.5518, Acc: 1.00\n",
            "Epoch [18/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [130000/257353], Loss: 1.4922, Acc: 0.00\n",
            "Epoch [18/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [160000/257353], Loss: 0.5552, Acc: 1.00\n",
            "Epoch [18/20], Step [170000/257353], Loss: 1.5305, Acc: 0.00\n",
            "Epoch [18/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [18/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [200000/257353], Loss: 0.5627, Acc: 1.00\n",
            "Epoch [18/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [220000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [18/20], Step [250000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Validation Accuracy :  74.208\n",
            "Epoch [19/20], Step [10000/257353], Loss: 1.4637, Acc: 0.00\n",
            "Epoch [19/20], Step [20000/257353], Loss: 1.3398, Acc: 0.00\n",
            "Epoch [19/20], Step [30000/257353], Loss: 1.2120, Acc: 0.00\n",
            "Epoch [19/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [50000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [70000/257353], Loss: 0.7120, Acc: 1.00\n",
            "Epoch [19/20], Step [80000/257353], Loss: 0.5664, Acc: 1.00\n",
            "Epoch [19/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [100000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [110000/257353], Loss: 0.6074, Acc: 1.00\n",
            "Epoch [19/20], Step [120000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [130000/257353], Loss: 1.1290, Acc: 0.00\n",
            "Epoch [19/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [160000/257353], Loss: 0.6475, Acc: 1.00\n",
            "Epoch [19/20], Step [170000/257353], Loss: 1.4722, Acc: 0.00\n",
            "Epoch [19/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [19/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [200000/257353], Loss: 1.3184, Acc: 0.00\n",
            "Epoch [19/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [220000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [19/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [19/20], Step [250000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Validation Accuracy :  72.944\n",
            "Epoch [20/20], Step [10000/257353], Loss: 1.5226, Acc: 0.00\n",
            "Epoch [20/20], Step [20000/257353], Loss: 1.1972, Acc: 0.00\n",
            "Epoch [20/20], Step [30000/257353], Loss: 0.5524, Acc: 1.00\n",
            "Epoch [20/20], Step [40000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [50000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [60000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [70000/257353], Loss: 1.4061, Acc: 0.00\n",
            "Epoch [20/20], Step [80000/257353], Loss: 0.5515, Acc: 1.00\n",
            "Epoch [20/20], Step [90000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [100000/257353], Loss: 0.5525, Acc: 1.00\n",
            "Epoch [20/20], Step [110000/257353], Loss: 0.5586, Acc: 1.00\n",
            "Epoch [20/20], Step [120000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [130000/257353], Loss: 0.8333, Acc: 1.00\n",
            "Epoch [20/20], Step [140000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [150000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [160000/257353], Loss: 0.5625, Acc: 1.00\n",
            "Epoch [20/20], Step [170000/257353], Loss: 1.5371, Acc: 0.00\n",
            "Epoch [20/20], Step [180000/257353], Loss: 1.5514, Acc: 0.00\n",
            "Epoch [20/20], Step [190000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [200000/257353], Loss: 0.5549, Acc: 1.00\n",
            "Epoch [20/20], Step [210000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [220000/257353], Loss: 0.5516, Acc: 1.00\n",
            "Epoch [20/20], Step [230000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [240000/257353], Loss: 0.5514, Acc: 1.00\n",
            "Epoch [20/20], Step [250000/257353], Loss: 0.5538, Acc: 1.00\n",
            "Validation Accuracy :  72.426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjlG4NcU8v9H"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7f1f1c05-737d-41e6-d9e1-a4fea9d88141",
        "id": "UaoA04HN8v9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "y_target = list()\n",
        "y_pred = list()\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        "  \n",
        "  prob, hat, A = model.forward(data)\n",
        "  y_target.append(int(label))\n",
        "  y_pred.append(hat.int().item())\n",
        " \n",
        "  if int(hat.item()) == label:\n",
        "    correct += 1\n",
        "    \n",
        "    \n",
        "print((correct/len(lab_test)) * 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "72.038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLXeycRX8v9S"
      },
      "source": [
        "#### Analyze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1cd3ae78-1eb3-49ff-b577-e2a8f878f969",
        "id": "8pJUALPC8v9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "correct = 0\n",
        "for data, label in zip(emb_test, lab_test):\n",
        "  data = torch.from_numpy(data)\n",
        "  bag_label = label\n",
        "  if cuda:\n",
        "    data, bag_label = data.cuda(), torch.tensor(bag_label).cuda()\n",
        "  d = data.reshape(*list(data.shape), 1)\n",
        "  one_hot = F.one_hot(torch.tensor(bag_label), 3)\n",
        "  data, bag_label = Variable(torch.tensor(d)), Variable(one_hot.unsqueeze(0))\n",
        " \n",
        "  prob, hat, A = model.forward(data)\n",
        "  print(i)\n",
        "  print('Prob :', prob, '\\nPredicted : ' , hat, '\\nAttention : ',  A,'\\nTarget : ', label, '\\n*************************\\n')\n",
        "  \n",
        "  i += 1\n",
        "  if i == 50:\n",
        "    break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Prob : tensor([[0.9899, 0.0054, 0.0046]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4930, 0.5070]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "1\n",
            "Prob : tensor([[1.0658e-07, 8.7821e-01, 1.2179e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1680, 0.1678, 0.1668, 0.1674, 0.1680, 0.1620]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "2\n",
            "Prob : tensor([[2.7709e-01, 7.2291e-01, 4.4433e-16]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3391, 0.3399, 0.3210]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "3\n",
            "Prob : tensor([[7.2518e-16, 4.1519e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1031, 0.1049, 0.0973, 0.0801, 0.1049, 0.1033, 0.1049, 0.0924, 0.1049,\n",
            "         0.1044]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "4\n",
            "Prob : tensor([[3.9433e-02, 9.6056e-01, 3.0713e-06]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2598, 0.2548, 0.2271, 0.2583]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "5\n",
            "Prob : tensor([[1.1392e-05, 9.7516e-01, 2.4829e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.4977, 0.5023]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "6\n",
            "Prob : tensor([[2.0676e-17, 3.5676e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2010, 0.2010, 0.1966, 0.2010, 0.2004]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "7\n",
            "Prob : tensor([[6.1935e-01, 3.8065e-01, 5.7230e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1444, 0.1435, 0.1423, 0.1410, 0.1449, 0.1449, 0.1390]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "8\n",
            "Prob : tensor([[1.0171e-10, 6.9900e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0866, 0.0869, 0.0872, 0.0874, 0.0873, 0.0872, 0.0856, 0.0675, 0.0874,\n",
            "         0.0868, 0.0720, 0.0782]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "9\n",
            "Prob : tensor([[6.6070e-01, 3.3930e-01, 2.3583e-07]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1725, 0.1596, 0.1616, 0.1713, 0.1613, 0.1737]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "10\n",
            "Prob : tensor([[5.2742e-25, 6.2609e-19, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3528, 0.2943, 0.3529]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "11\n",
            "Prob : tensor([[0.0677, 0.4224, 0.5100]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1152, 0.1139, 0.1047, 0.1152, 0.1103, 0.1092, 0.1139, 0.1022, 0.1153]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "12\n",
            "Prob : tensor([[0.8379, 0.0550, 0.1071]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1462, 0.1463, 0.1341, 0.1462, 0.1454, 0.1356, 0.1461]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "13\n",
            "Prob : tensor([[3.8070e-02, 9.6193e-01, 9.4682e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1434, 0.1456, 0.1485, 0.1190, 0.1484, 0.1470, 0.1481]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "14\n",
            "Prob : tensor([[1.5737e-19, 1.7838e-17, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1437, 0.1439, 0.1439, 0.1432, 0.1438, 0.1377, 0.1439]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "15\n",
            "Prob : tensor([[1.0736e-09, 6.4210e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2016, 0.1937, 0.2016, 0.2015, 0.2015]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "16\n",
            "Prob : tensor([[9.9287e-01, 7.1271e-03, 4.6576e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3392, 0.3198, 0.3410]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "17\n",
            "Prob : tensor([[1.6795e-07, 8.0095e-05, 9.9992e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.0783, 0.0776, 0.0665, 0.0745, 0.0791, 0.0791, 0.0795, 0.0794, 0.0794,\n",
            "         0.0694, 0.0795, 0.0782, 0.0795]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "18\n",
            "Prob : tensor([[6.7117e-14, 4.1458e-11, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "19\n",
            "Prob : tensor([[5.3087e-04, 9.9887e-01, 6.0267e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0740, 0.0792, 0.0788, 0.0790, 0.0744, 0.0795, 0.0794, 0.0794, 0.0794,\n",
            "         0.0586, 0.0795, 0.0794, 0.0794]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "20\n",
            "Prob : tensor([[1.3368e-11, 3.5353e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3085, 0.3654, 0.3261]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "21\n",
            "Prob : tensor([[9.9906e-01, 9.4247e-04, 2.6912e-10]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.4937, 0.5063]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "22\n",
            "Prob : tensor([[1.0000e+00, 1.1348e-07, 3.0391e-31]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.3152, 0.3424, 0.3424]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "23\n",
            "Prob : tensor([[6.8690e-01, 3.1284e-01, 2.6257e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2477, 0.2439, 0.2597, 0.2488]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "24\n",
            "Prob : tensor([[7.9606e-01, 2.0383e-01, 1.1365e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.2565, 0.2459, 0.2682, 0.2293]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "25\n",
            "Prob : tensor([[1.7279e-13, 1.4281e-09, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1228, 0.1558, 0.1558, 0.1425, 0.1298, 0.1558, 0.1374]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "26\n",
            "Prob : tensor([[1.2967e-09, 9.9633e-01, 3.6707e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1933, 0.2030, 0.2029, 0.2017, 0.1992]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "27\n",
            "Prob : tensor([[5.7549e-19, 7.5311e-14, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1288, 0.1762, 0.1762, 0.1762, 0.1762, 0.1665]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "28\n",
            "Prob : "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.7197e-12, 5.2282e-06, 9.9999e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.3119, 0.3424, 0.3456]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "29\n",
            "Prob : tensor([[1.0000e+00, 9.8395e-09, 1.3061e-21]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1721, 0.1721, 0.1715, 0.1408, 0.1713, 0.1722]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "30\n",
            "Prob : tensor([[9.9994e-01, 5.9221e-05, 3.0602e-13]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.0679, 0.0846, 0.0846, 0.0804, 0.0856, 0.0834, 0.0856, 0.0856, 0.0856,\n",
            "         0.0856, 0.0855, 0.0855]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "31\n",
            "Prob : tensor([[7.7147e-25, 9.1365e-19, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2345, 0.2552, 0.2552, 0.2552]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "32\n",
            "Prob : tensor([[1.0000e+00, 1.0557e-09, 3.2418e-18]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1711, 0.1739, 0.1736, 0.1739, 0.1738, 0.1338]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "33\n",
            "Prob : tensor([[1.1421e-08, 5.9956e-01, 4.0044e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2863, 0.3586, 0.3551]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "34\n",
            "Prob : tensor([[9.5037e-08, 1.1078e-02, 9.8892e-01]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4936, 0.5064]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "35\n",
            "Prob : tensor([[2.2507e-15, 1.4919e-08, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1478, 0.1478, 0.1222, 0.1469, 0.1474, 0.1454, 0.1424]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "36\n",
            "Prob : tensor([[2.3988e-15, 1.1058e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2501, 0.2501, 0.2498, 0.2501]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "37\n",
            "Prob : tensor([[2.2477e-02, 9.7752e-01, 6.6666e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.5161, 0.4839]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "38\n",
            "Prob : tensor([[1.9223e-38, 9.7775e-28, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.2501, 0.2702, 0.2702, 0.2096]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "39\n",
            "Prob : tensor([[9.9998e-01, 1.7776e-05, 3.4147e-17]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.5017, 0.4983]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "40\n",
            "Prob : tensor([[3.2500e-19, 5.8779e-13, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.1310, 0.1231, 0.1321, 0.1322, 0.1322, 0.1077, 0.1322, 0.1095]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "41\n",
            "Prob : tensor([[7.1339e-12, 2.7919e-07, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.5748, 0.4252]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "42\n",
            "Prob : tensor([[1.9798e-04, 9.9980e-01, 1.3937e-08]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2353, 0.2626, 0.2393, 0.2627]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n",
            "43\n",
            "Prob : tensor([[1.5489e-05, 9.9784e-01, 2.1495e-03]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.1190, 0.1314, 0.1510, 0.1509, 0.1512, 0.1491, 0.1474]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "44\n",
            "Prob : tensor([[2.7398e-02, 9.7260e-01, 1.0403e-09]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.3335, 0.3268, 0.3398]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "45\n",
            "Prob : tensor([[3.2039e-15, 4.0795e-10, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([2.], device='cuda:0') \n",
            "Attention :  tensor([[0.4261, 0.5739]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "46\n",
            "Prob : tensor([[9.9947e-01, 5.2771e-04, 5.9226e-14]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1456, 0.1443, 0.1362, 0.1430, 0.1456, 0.1424, 0.1429]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "47\n",
            "Prob : tensor([[3.3750e-05, 9.9940e-01, 5.6449e-04]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.2843, 0.3563, 0.3594]], device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  2 \n",
            "*************************\n",
            "\n",
            "48\n",
            "Prob : tensor([[1.0000e+00, 5.7042e-08, 1.6129e-19]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([0.], device='cuda:0') \n",
            "Attention :  tensor([[0.1250, 0.1258, 0.1236, 0.1259, 0.1256, 0.1250, 0.1231, 0.1259]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>) \n",
            "Target :  0 \n",
            "*************************\n",
            "\n",
            "49\n",
            "Prob : tensor([[9.6604e-05, 9.4470e-01, 5.5199e-02]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Predicted :  tensor([1.], device='cuda:0') \n",
            "Attention :  tensor([[0.0331, 0.0449, 0.0440, 0.0451, 0.0447, 0.0424, 0.0411, 0.0451, 0.0447,\n",
            "         0.0448, 0.0435, 0.0438, 0.0451, 0.0451, 0.0436, 0.0451, 0.0418, 0.0451,\n",
            "         0.0444, 0.0382, 0.0450, 0.0447, 0.0446]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>) \n",
            "Target :  1 \n",
            "*************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8VyXk5Zn8v9g"
      },
      "source": [
        "####Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9r1_JNkC8v9i",
        "colab": {}
      },
      "source": [
        "model_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9eed58cb-c13f-4ac2-a337-b4ba54af3648",
        "id": "GwO6bW9c8v9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "PATH = 'model_state_CNN512'+str(model_num)\n",
        "PATH0 = 'model_CNN512.pt'\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "torch.save(model, PATH0)\n",
        "\n",
        "model_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WKiRNomo8v9y"
      },
      "source": [
        "####Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3cc3d57e-0e59-49d5-9f99-db1c43d616f0",
        "id": "_nRFlbJy8v90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "evaluate_model(y_target, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.72038; \n",
            "F1-Score (Micro): 0.72038; \n",
            "F1-Score (Macro): 0.7145053553637407; \n",
            "F1-Score (Weighted): 0.71586851750882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSZiuHKuUQyh",
        "colab_type": "text"
      },
      "source": [
        "We could also try to change the size the size of filters, and other hyperparameters. Since all the variations we tried resulted into similar performances we skipped it for now. But it is definetaly something for us to try out in *Future Scopes*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y6xSqtmU5Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}